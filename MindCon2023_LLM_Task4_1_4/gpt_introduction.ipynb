{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"./assets/cover.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"./assets/table_of_contents_1.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. 问题一：参与本次课程需要什么基础？\n",
    "\n",
    "- 了解深度学习基础知识，如损失函数、反向传播等；\n",
    "- 了解NLP相关基础知识，如word embedding，RNN层，seq2seq模型等；\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/d2l.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. 问题二：可否提供一些课后练习？\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/course-review.png\"></div>\n",
    "\n",
    "- Transformer:\n",
    "    - 尝试使用混合精度，提升模型训练及推理速度（包括BLEU Score计算）；\n",
    "    - 尝试更换数据集进行另两种语言的机器翻译；\n",
    "- BERT\n",
    "    - 参考课程中介绍的下游任务，自己选择一种进行实践；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. 问题三：学习中遇到问题怎么办？\n",
    "\n",
    "- 课上：在直播评论区随时提问，如自己的问题未得到答复，可将问题写入共享文档中，课后/下节课正式授课前进行统一答复；\n",
    "- 课后：在QQ群进行提问，或在代码仓中提issue；\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/q_and_a.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"./assets/table_of_contents_2.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "\n",
    "未标注的文本数据远多于已标注的文本数据，并且对于不同的下游任务会存在不同的标注方式\n",
    "\n",
    "# Method\n",
    "\n",
    "## semi-supervised learning\n",
    "\n",
    "- 基于大量未标注的文本数据，训练预训练语言模型\n",
    "- 使用已标注文本数据，对模型针对某一特定下游任务进行finetune，只更改output layer（线性层）\n",
    "\n",
    "# Problem Statement\n",
    "\n",
    "- 自然语言处理的下游任务非常多元，难以有统一的优化目标\n",
    "- 难以将预训练模型的信息完全传递到finetune的下游任务中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## objective\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/objective.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## model architecture\n",
    "\n",
    "由于训练objective的选择，gpt在模型选择上不应该看见当前token后的信息，故模型应设计为单向网络，即transformer中的decoder结构。\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/model.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import nn\n",
    "from mindspore import ops\n",
    "from mindspore import Tensor\n",
    "from mindspore.common.initializer import initializer, Normal\n",
    "from mindnlp.transformers import GPTConfig, PreTrainedModel\n",
    "from mindnlp._legacy.nn import Dropout \n",
    "from mindnlp.transformers.ms_utils import Conv1D, prune_conv1d_layer, find_pruneable_heads_and_indices\n",
    "from mindnlp.transformers.modeling_utils import SequenceSummary\n",
    "from mindnlp.transformers.activations import ACT2FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#定义了一个多层感知机（MLP）的神经网络模型\n",
    "class MLP(nn.Cell):\n",
    "    r\"\"\"\n",
    "    GPT MLP\n",
    "\t\"\"\"\n",
    "\n",
    "    def __init__(self, n_state, config):\n",
    "        super().__init__()\n",
    "        n_embd = config.n_embd\n",
    "        # 定义第一层卷积层\n",
    "        self.c_fc = Conv1D(n_state, n_embd)\n",
    "        # 定义第二层卷积层\n",
    "        self.c_proj = Conv1D(n_embd, n_state)\n",
    "        # 激活函数\n",
    "        self.act = ACT2FN[config.afn]\n",
    "        # Dropout 层，用于防止过拟合\n",
    "        self.dropout = Dropout(p=config.resid_pdrop)\n",
    "\n",
    "    def construct(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        return self.dropout(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Multi-head attention 实现\n",
    "#Attention 类实现了自注意力机制，并在 GPT 模型中用于对输入序列中的每个位置进行信息提取和变换。\n",
    "\n",
    "class Attention(nn.Cell):\n",
    "    r\"\"\"\n",
    "    GPT Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nx, n_positions, config, scale=False):\n",
    "        super().__init__()\n",
    "        n_state = nx\n",
    "        if n_state % config.n_head != 0:\n",
    "            raise ValueError(f\"Attention n_state shape: {n_state} must be divisible by config.n_head {config.n_head}\")\n",
    "\n",
    "        ## 初始化参数\n",
    "        self.bias = Tensor(np.tril(np.ones((n_positions, n_positions))), mindspore.float32).view(1, 1, n_positions, n_positions)\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "\n",
    "        self.c_attn = Conv1D(n_state * 3, n_state)\n",
    "        ## 定义卷积层\n",
    "        self.c_attn = Conv1D(n_state * 3, n_state)\n",
    "        self.c_proj = Conv1D(n_state, n_state)\n",
    "        self.attn_dropout = Dropout(p=config.attn_pdrop)\n",
    "        self.resid_dropout = Dropout(p=config.resid_pdrop)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model.\n",
    "        \"\"\"\n",
    "        ## 剪枝操作，删除指定的注意力头\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        head_size = self.split_size//self.n_head\n",
    "        heads, index = find_pruneable_heads_and_indices(heads, self.n_head, head_size, self.pruned_heads)\n",
    "        index_attn = ops.cat([index, index + self.split_size, index + (2 * self.split_size)])\n",
    "        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, axis=1)\n",
    "        self.c_proj = prune_conv1d_layer(self.c_proj, index, axis=0)\n",
    "        self.split_size = (self.split_size // self.n_head) * (self.n_head - len(heads))\n",
    "        self.n_head = self.n_head - len(heads)\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def _attn(self, q, k, v, attention_mask=None, head_mask=None):\n",
    "        \n",
    "        w = ops.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / ops.sqrt(ops.scalar_to_tensor(v.shape[-1]))\n",
    "        b = self.bias[:, :, : w.shape[-2], : w.shape[-1]]\n",
    "        ## 计算注意力权重\n",
    "        w = w * b + -1e9 * (1 - b)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            w = w + attention_mask\n",
    "\n",
    "        #实现自注意力机制\n",
    "        w = ops.softmax(w)\n",
    "        w = self.attn_dropout(w)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            w = w * head_mask\n",
    "\n",
    "        outputs = (ops.matmul(w, v),)\n",
    "        if self.output_attentions:\n",
    "            outputs += (w,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        \"\"\"merge heads\"\"\"\n",
    "        #将多头注意力的结果合并\n",
    "        x = x.transpose(0, 2, 1, 3)\n",
    "        new_x_shape = x.shape[:-2] + (x.shape[-2] * x.shape[-1],)\n",
    "        return x.view(new_x_shape)\n",
    "\n",
    "    def split_heads(self, x, k=False):\n",
    "        \"\"\"split heads\"\"\"\n",
    "        #将输入拆分为多个注意力头\n",
    "        new_x_shape = x.shape[:-1] + (self.n_head, x.shape[-1] // self.n_head)\n",
    "        x = x.view(new_x_shape)\n",
    "        if k:\n",
    "            return x.transpose(0, 2, 3, 1)\n",
    "        return x.transpose(0, 2, 1, 3)\n",
    "\n",
    "    def construct(self, x, attention_mask=None, head_mask=None):\n",
    "        #Attention 层的前向传播\n",
    "        ## 应用卷积层\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = ops.split(x, self.split_size, axis=2)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "\n",
    "        attn_outputs = self._attn(query, key, value, attention_mask, head_mask)\n",
    "        a = attn_outputs[0]\n",
    "\n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        a = self.resid_dropout(a)\n",
    "        outputs = (a,) + attn_outputs[1:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#GPT 模型由多个这样的块组成，每个块包含了自注意力机制和一个多层感知机（MLP）。\n",
    "class Block(nn.Cell):\n",
    "    r\"\"\"\n",
    "    GPT Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_positions, config, scale=False):\n",
    "        super().__init__()\n",
    "        nx = config.n_embd\n",
    "        ## 注意力层（自注意力机制）\n",
    "        self.attn = Attention(nx, n_positions, config, scale)\n",
    "        # Layer Normalization 层 1\n",
    "        self.ln_1 = nn.LayerNorm((nx,), epsilon=config.layer_norm_epsilon)\n",
    "        # 多层感知机层\n",
    "        self.mlp = MLP(4 * nx, config)\n",
    "        # Layer Normalization 层 2\n",
    "        self.ln_2 = nn.LayerNorm((nx,), epsilon=config.layer_norm_epsilon)\n",
    "\n",
    "    def construct(self, x, attention_mask=None, head_mask=None):\n",
    "        output_attn = self.attn(\n",
    "            x,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "\n",
    "        a = output_attn[0]\n",
    "        n = self.ln_1(x + a)\n",
    "        m = self.mlp(n)\n",
    "        h = self.ln_2(n + m)\n",
    "\n",
    "        outputs = (h,) + output_attn[1:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#该基类提供了权重初始化和加载预训练模型的通用接口\n",
    "class GPTPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    config_class = GPTConfig\n",
    "    base_model_prefix = \"transformer\"\n",
    "    _keys_to_ignore_on_load_unexpected = [r'attn.bias']\n",
    "\n",
    "    def _init_weights(self, cell):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(cell, (nn.Dense, Conv1D)):\n",
    "            cell.weight.set_data(initializer(Normal(self.config.initializer_range),\n",
    "                                                    cell.weight.shape, cell.weight.dtype))\n",
    "            if cell.bias is not None:\n",
    "                cell.bias.set_data(initializer('zeros', cell.bias.shape, cell.bias.dtype))\n",
    "        # 初始化 Embedding 层的权重        \n",
    "        elif isinstance(cell, nn.Embedding):\n",
    "            weight = initializer(Normal(self.config.initializer_range),\n",
    "                                                 cell.weight.shape,\n",
    "                                                 cell.weight.dtype)\n",
    "            # 如果有 padding_idx，将对应位置的权重置为零\n",
    "            if cell.padding_idx is not None:\n",
    "                weight[cell.padding_idx] = 0\n",
    "            cell.weight.set_data(weight)\n",
    "        elif isinstance(cell, nn.LayerNorm):\n",
    "            cell.weight.set_data(initializer('ones', cell.weight.shape, cell.weight.dtype))\n",
    "            cell.bias.set_data(initializer('zeros', cell.bias.shape, cell.bias.dtype))\n",
    "\n",
    "\n",
    "class GPTModel(GPTPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The bare GPT transformer model outputting raw hidden-states without any specific head on top\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        # Embedding 层\n",
    "        self.tokens_embed = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.positions_embed = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        # Dropout 层\n",
    "        self.drop = Dropout(p=config.embd_pdrop)\n",
    "        # 堆叠多个块（Block）\n",
    "        self.h = nn.CellList([Block(config.n_positions, config, scale=True) for _ in range(config.n_layer)])\n",
    "        # 位置编码\n",
    "        self.position_ids = ops.arange(config.n_positions)\n",
    "\n",
    "        self.n_layer = self.config.n_layer\n",
    "        self.output_attentions = self.config.output_attentions\n",
    "        self.output_hidden_states = self.config.output_hidden_states\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        \"\"\"\n",
    "        return the input embeddings layer\n",
    "        \"\"\"\n",
    "        return self.tokens_embed\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        \"\"\"\n",
    "        set the input embeddings layer\n",
    "        \"\"\"\n",
    "        self.tokens_embed = new_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.h[layer].attn.prune_heads(heads)\n",
    "\n",
    "    def construct(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "    ):\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.shape\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.shape[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        # 处理位置编码\n",
    "        if position_ids is None:\n",
    "            # Code is different from when we had a single embedding matrix  from position and token embeddings\n",
    "            position_ids = self.position_ids[None, : input_shape[-1]]\n",
    "\n",
    "        # 处理注意力掩码\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "            attention_mask = (1.0 - attention_mask) * Tensor(np.finfo(mindspore.dtype_to_nptype(self.dtype)).min,\n",
    "                                                             self.dtype)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        head_mask = self.get_head_mask(head_mask, self.n_layer)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.tokens_embed(input_ids)\n",
    "        position_embeds = self.positions_embed(position_ids)\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n",
    "            token_type_embeds = self.tokens_embed(token_type_ids)\n",
    "        else:\n",
    "            token_type_embeds = 0\n",
    "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        output_shape = input_shape + (hidden_states.shape[-1],)\n",
    "\n",
    "        all_attentions = ()\n",
    "        all_hidden_states = ()\n",
    "        for i, block in enumerate(self.h):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            outputs = block(hidden_states, attention_mask, head_mask[i])\n",
    "            hidden_states = outputs[0]\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (outputs[1],)\n",
    "\n",
    "        hidden_states = hidden_states.view(*output_shape)\n",
    "\n",
    "        # Add last layer\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "        return (hidden_states, all_hidden_states, all_attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"./assets/table_of_contents_3.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Fine-Tuning\n",
    "\n",
    "在已经预训练好的GPT上额外加一层线性层\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/finetune_output.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "并通过缩小目标与计算结果的误差进行模型优化\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/finetune_objective.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "最终为加速模型收敛及提高模型的泛化性，融入pretrain时language modelling的优化目标\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/finetune_overall_objective.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 文本序列分类任务\n",
    "\n",
    "#定义了一个带有序列分类头部（线性层）的 GPT 模型\n",
    "class GPTForSequenceClassification(GPTPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The Original GPT Model transformer with a sequence classification head on top (linear layer).\n",
    "    GPTForSequenceClassification uses the last token in order to do the classification, as other causal\n",
    "    models (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the\n",
    "    last token. If a `pad_token_id` is defined in the configuration, it finds the last token that is not a padding\n",
    "    token in each row. If no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since\n",
    "    it cannot guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take\n",
    "    the last value in each row of the batch).\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = config.num_labels\n",
    "        self.transformer = GPTModel(config)\n",
    "        self.score = nn.Dense(config.n_embd, self.num_labels, has_bias=False)\n",
    "\n",
    "        self.pad_token_id = self.config.pad_token_id\n",
    "        problem_type = config.problem_type\n",
    "        if problem_type is None:\n",
    "            self.loss = None\n",
    "        else:\n",
    "            if self.num_labels == 1:\n",
    "                self.problem_type = \"regression\"\n",
    "                self.loss = nn.MSELoss()\n",
    "            elif self.num_labels > 1:\n",
    "                self.problem_type = \"single_label_classification\"\n",
    "                self.loss = nn.CrossEntropyLoss()\n",
    "            else:\n",
    "                self.problem_type = \"multi_label_classification\"\n",
    "                self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def construct(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        token_type_ids = None,\n",
    "        position_ids = None,\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in\n",
    "            `[0, ...,config.num_labels - 1]`.\n",
    "            If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        # GPT 模型的前向传播\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        # 分类头部的线性层\n",
    "        logits = self.score(hidden_states)\n",
    "\n",
    "        # 处理输入\n",
    "        if input_ids is not None:\n",
    "            batch_size, _ = input_ids.shape[:2]\n",
    "        else:\n",
    "            batch_size, _ = inputs_embeds.shape[:2]\n",
    "\n",
    "        # 处理 padding token\n",
    "        if self.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "\n",
    "        if self.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                sequence_lengths = ops.ne(input_ids, self.pad_token_id).sum(-1) - 1\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "\n",
    "        # 获取最后一个 token 对应的 logits\n",
    "        pooled_logits = logits[:, sequence_lengths]\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        output = (pooled_logits,) + transformer_outputs[1:]\n",
    "\n",
    "        # 计算损失\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss = self.loss(pooled_logits.squeeze(), labels.squeeze())\n",
    "            elif self.num_labels > 1:\n",
    "                loss = self.loss(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            else:\n",
    "                loss = self.loss(pooled_logits, labels)\n",
    "\n",
    "        if loss is not None:\n",
    "            output = (loss,) + output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "rise": {
   "enable_chalkboard": true,
   "height": "110%",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
