/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \L
  if not dirpath.find("AppData\Local\Temp"):
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \B
  """
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \c
  """
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _nlv = LooseVersion(_np_version)
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(__version__) >= LooseVersion("1.17.0"):
/home/ma-user/work/mindformers/scripts/mf_standalone/mindformers/wrapper/wrapper.py:52: DeprecationWarning: invalid escape sequence \l
  """TrainOneStep For MindFormer.
/home/ma-user/work/mindformers/scripts/mf_standalone/mindformers/wrapper/wrapper.py:164: DeprecationWarning: invalid escape sequence \*
  """
2024-01-23 22:55:29,115 - mindformers[mindformers/tools/utils.py:153] - INFO - set output path to '/home/ma-user/work/mindformers/output'
2024-01-23 22:55:29,117 - mindformers[mindformers/scripts/mf_standalone/run_mindformer.py:109] - INFO - .........Build context config..........
2024-01-23 22:55:29,117 - mindformers[mindformers/core/parallel_config.py:45] - INFO - initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
2024-01-23 22:55:29,117 - mindformers[mindformers/core/parallel_config.py:51] - INFO - initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'use_seq_parallel': False, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}
2024-01-23 22:55:29,118 - mindformers[mindformers/scripts/mf_standalone/run_mindformer.py:111] - INFO - context config is: [ParallelConfig]
_recompute:[ParallelConfig]
_recompute:True
_select_recompute:False
_parallel_optimizer_comm_recompute:False
_mp_comm_recompute:True
_recompute_slice_activation:True

select_recompute:False
use_seq_parallel:False
_gradient_aggregation_group:4
_embed_dp_mp_config:[ParallelConfig]
_dp_mp_config:[ParallelConfig]
_data_parallel:8
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_vocab_emb_dp:True
use_seq_parallel:False
select_recompute:False

_pp_config:[ParallelConfig]
_pipeline_stage:1
_micro_batch_num:1

_moe_config:[ParallelConfig]
_dpmp:[ParallelConfig]
_data_parallel:8
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_expert_parallel:1
use_seq_parallel:False
select_recompute:False


2024-01-23 22:55:29,118 - mindformers[mindformers/scripts/mf_standalone/run_mindformer.py:112] - INFO - moe config is: <mindformers.modules.transformer.moe.MoEConfig object at 0xffff15ac57c0>
2024-01-23 22:55:29,119 - mindformers[mindformers/trainer/base_trainer.py:85] - INFO - Now Running Task is: text_generation, Model is: llama_7b_lora
2024-01-23 22:55:29,119 - mindformers[mindformers/trainer/base_trainer.py:228] - INFO - The current parallel mode is stand_alone, batch size per card will not be changed: batch_size_per_card = 2
2024-01-23 22:55:29,119 - mindformers[mindformers/trainer/base_trainer.py:232] - INFO - global_batch_size = batch_size_per_card * device_num * gradient_accumulation_steps = 2 = 2 * 1 * 1
2024-01-23 22:55:29,120 - mindformers[mindformers/trainer/base_trainer.py:241] - INFO - parallel_config will be change to default config: [ParallelConfig]
_recompute:[ParallelConfig]
_recompute:True
_select_recompute:False
_parallel_optimizer_comm_recompute:False
_mp_comm_recompute:True
_recompute_slice_activation:True

select_recompute:False
use_seq_parallel:False
_gradient_aggregation_group:4
_embed_dp_mp_config:[ParallelConfig]
_dp_mp_config:[ParallelConfig]
_data_parallel:1
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_vocab_emb_dp:True
use_seq_parallel:False
select_recompute:False

_pp_config:[ParallelConfig]
_pipeline_stage:1
_micro_batch_num:1

_moe_config:[ParallelConfig]
_dpmp:[ParallelConfig]
_data_parallel:1
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_expert_parallel:1
use_seq_parallel:False
select_recompute:False

.
2024-01-23 22:55:29,120 - mindformers[mindformers/trainer/base_trainer.py:621] - INFO - .........Build Dataset For Train..........
2024-01-23 22:55:29,120 - mindformers[mindformers/trainer/base_trainer.py:348] - INFO - .........Build Dataset From Config..........
2024-01-23 22:55:29,120 - mindformers[mindformers/dataset/causal_language_model_dataset.py:166] - INFO - Now Create Causal Language Model Dataset.
2024-01-23 22:55:29,127 - mindformers[mindformers/trainer/utils.py:149] - INFO - Will be Training epochs:1, sink_size:2
2024-01-23 22:55:29,128 - mindformers[mindformers/trainer/utils.py:151] - INFO - Create training dataset finish, dataset size:26001
2024-01-23 22:55:29,128 - mindformers[mindformers/trainer/base_trainer.py:634] - INFO - .........Build Net For Train..........
2024-01-23 22:55:29,128 - mindformers[mindformers/trainer/base_trainer.py:383] - INFO - .........Build Network From Config..........
2024-01-23 22:55:29,129 - mindformers[mindformers/models/llama/llama_config.py:174] - WARNING - Argument `pretrain_seqlen` is deprecated. Use `scaling_factor` instead.
2024-01-23 22:55:29,129 - mindformers[mindformers/models/llama/llama_config.py:177] - WARNING - Argument `compute_in_2d` is deprecated.
2024-01-23 22:55:29,129 - mindformers[mindformers/models/llama/llama_config.py:180] - WARNING - Argument `use_past_shard` is deprecated.
2024-01-23 22:55:29,129 - mindformers[mindformers/version_control.py:60] - INFO - The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1
2024-01-23 22:55:29,130 - mindformers[mindformers/version_control.py:64] - INFO - 
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: 
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
2024-01-23 22:55:29,130 - mindformers[mindformers/version_control.py:70] - INFO - The Cell Reuse compilation acceleration feature does not support single-card mode.This feature is disabled by default. ENABLE_CELL_REUSE=1 does not take effect.
2024-01-23 22:55:29,130 - mindformers[mindformers/version_control.py:73] - INFO - The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:55:29.694.513 [mindspore/ops/primitive.py:228] The in_strategy of the operator in your network will not take effect in stand_alone mode. This means the the shard function called in the network is ignored. 
If you want to enable it, please use semi auto or auto parallel mode by context.set_auto_parallel_context(parallel_mode=ParallelMode.SEMI_AUTO_PARALLEL or context.set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL)
2024-01-23 22:55:31,541 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:31,544 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:33,132 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:34,525 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:55:34.528.091 [mindspore/common/parameter.py:786] This interface may be deleted in the future.
2024-01-23 22:55:34,532 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:34,535 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:36,120 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:37,409 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:55:37,414 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:37,417 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:38,957 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:40,205 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:55:40,210 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:40,212 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:41,757 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:42,935 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:55:42,941 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:42,943 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:44,570 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:45,814 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:55:45,819 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:45,821 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:47,424 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:48,675 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:55:48,680 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:48,683 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:50,205 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:51,440 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:55:51,445 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:51,447 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:52,984 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:54,174 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:55:54,179 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:54,181 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:55,670 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:57,040 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:55:57,046 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:57,048 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:55:58,637 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:00,021 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:00,028 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:00,031 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:01,655 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:02,923 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:02,930 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:02,932 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:04,531 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:05,754 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:05,759 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:05,761 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:07,264 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:08,505 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:08,510 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:08,512 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:10,031 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:11,420 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:11,425 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:11,427 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:12,969 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:14,265 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:14,271 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:14,273 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:15,826 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:17,073 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:17,078 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:17,081 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:18,590 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:19,895 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:19,903 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:19,905 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:21,524 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:22,921 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:22,926 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:22,928 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:24,478 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:25,652 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:25,657 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:25,660 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:27,192 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:28,418 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:28,423 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:28,426 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:29,972 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:31,196 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:31,201 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:31,204 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:32,703 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:33,878 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:33,884 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:33,886 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:35,354 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:36,719 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:36,724 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:36,726 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:38,323 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:39,745 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:39,750 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:39,753 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:41,341 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:42,568 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:42,573 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:42,575 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:44,085 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:45,268 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:45,273 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:45,275 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:46,850 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:48,156 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:48,161 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:48,164 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:49,749 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:51,172 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:51,178 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:51,181 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:52,781 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:54,140 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:54,146 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:54,148 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:55,689 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:56,956 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:56,962 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:56,964 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:58,629 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:59,908 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:56:59,918 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:56:59,921 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:57:01,551 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:57:02,762 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-01-23 22:57:02,766 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.
2024-01-23 22:57:04,723 - mindformers[mindformers/models/base_model.py:117] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:29.659.052 [mindspore/train/serialization.py:172] The type of model.layers.0.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:29.660.487 [mindspore/train/serialization.py:172] The type of model.layers.0.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:30.125.717 [mindspore/train/serialization.py:172] The type of model.layers.1.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:30.126.806 [mindspore/train/serialization.py:172] The type of model.layers.1.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:30.656.429 [mindspore/train/serialization.py:172] The type of model.layers.2.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:30.657.525 [mindspore/train/serialization.py:172] The type of model.layers.2.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:31.334.617 [mindspore/train/serialization.py:172] The type of model.layers.3.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:31.335.707 [mindspore/train/serialization.py:172] The type of model.layers.3.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:32.144.64 [mindspore/train/serialization.py:172] The type of model.layers.4.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:32.155.17 [mindspore/train/serialization.py:172] The type of model.layers.4.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:32.742.510 [mindspore/train/serialization.py:172] The type of model.layers.5.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:32.743.577 [mindspore/train/serialization.py:172] The type of model.layers.5.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:33.396.680 [mindspore/train/serialization.py:172] The type of model.layers.6.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:33.397.730 [mindspore/train/serialization.py:172] The type of model.layers.6.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:34.535.90 [mindspore/train/serialization.py:172] The type of model.layers.7.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:34.546.41 [mindspore/train/serialization.py:172] The type of model.layers.7.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:34.722.747 [mindspore/train/serialization.py:172] The type of model.layers.8.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:34.723.859 [mindspore/train/serialization.py:172] The type of model.layers.8.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:35.396.360 [mindspore/train/serialization.py:172] The type of model.layers.9.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:35.397.518 [mindspore/train/serialization.py:172] The type of model.layers.9.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:36.602.85 [mindspore/train/serialization.py:172] The type of model.layers.10.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:36.612.98 [mindspore/train/serialization.py:172] The type of model.layers.10.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:36.733.890 [mindspore/train/serialization.py:172] The type of model.layers.11.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:36.734.913 [mindspore/train/serialization.py:172] The type of model.layers.11.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:37.403.855 [mindspore/train/serialization.py:172] The type of model.layers.12.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:37.404.918 [mindspore/train/serialization.py:172] The type of model.layers.12.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:38.773.40 [mindspore/train/serialization.py:172] The type of model.layers.13.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:38.783.69 [mindspore/train/serialization.py:172] The type of model.layers.13.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:38.750.268 [mindspore/train/serialization.py:172] The type of model.layers.14.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:38.751.308 [mindspore/train/serialization.py:172] The type of model.layers.14.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:39.428.743 [mindspore/train/serialization.py:172] The type of model.layers.15.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:39.429.782 [mindspore/train/serialization.py:172] The type of model.layers.15.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:40.930.99 [mindspore/train/serialization.py:172] The type of model.layers.16.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:40.941.05 [mindspore/train/serialization.py:172] The type of model.layers.16.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:40.762.549 [mindspore/train/serialization.py:172] The type of model.layers.17.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:40.763.585 [mindspore/train/serialization.py:172] The type of model.layers.17.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:41.431.052 [mindspore/train/serialization.py:172] The type of model.layers.18.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:41.432.094 [mindspore/train/serialization.py:172] The type of model.layers.18.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:42.742.6 [mindspore/train/serialization.py:172] The type of model.layers.19.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:42.853.6 [mindspore/train/serialization.py:172] The type of model.layers.19.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:42.491.440 [mindspore/train/serialization.py:172] The type of model.layers.20.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:42.492.498 [mindspore/train/serialization.py:172] The type of model.layers.20.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:42.954.591 [mindspore/train/serialization.py:172] The type of model.layers.21.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:42.960.478 [mindspore/train/serialization.py:172] The type of model.layers.21.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:43.422.553 [mindspore/train/serialization.py:172] The type of model.layers.22.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:43.423.623 [mindspore/train/serialization.py:172] The type of model.layers.22.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:43.891.329 [mindspore/train/serialization.py:172] The type of model.layers.23.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:43.892.368 [mindspore/train/serialization.py:172] The type of model.layers.23.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:44.358.311 [mindspore/train/serialization.py:172] The type of model.layers.24.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:44.359.309 [mindspore/train/serialization.py:172] The type of model.layers.24.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:44.833.356 [mindspore/train/serialization.py:172] The type of model.layers.25.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:44.834.771 [mindspore/train/serialization.py:172] The type of model.layers.25.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:45.392.477 [mindspore/train/serialization.py:172] The type of model.layers.26.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:45.393.725 [mindspore/train/serialization.py:172] The type of model.layers.26.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:46.725.80 [mindspore/train/serialization.py:172] The type of model.layers.27.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:46.738.52 [mindspore/train/serialization.py:172] The type of model.layers.27.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:46.751.396 [mindspore/train/serialization.py:172] The type of model.layers.28.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:46.752.601 [mindspore/train/serialization.py:172] The type of model.layers.28.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:47.429.512 [mindspore/train/serialization.py:172] The type of model.layers.29.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:47.430.674 [mindspore/train/serialization.py:172] The type of model.layers.29.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:48.983.79 [mindspore/train/serialization.py:172] The type of model.layers.30.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:48.995.63 [mindspore/train/serialization.py:172] The type of model.layers.30.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:48.766.514 [mindspore/train/serialization.py:172] The type of model.layers.31.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:48.767.677 [mindspore/train/serialization.py:172] The type of model.layers.31.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.435.225 [mindspore/train/serialization.py:172] The type of model.norm_out.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.892.243 [mindspore/train/serialization.py:1317] For 'load_param_into_net', 256 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.892.583 [mindspore/train/serialization.py:1322] model.layers.0.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.892.681 [mindspore/train/serialization.py:1322] model.layers.0.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.892.761 [mindspore/train/serialization.py:1322] model.layers.0.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.892.835 [mindspore/train/serialization.py:1322] model.layers.0.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.892.906 [mindspore/train/serialization.py:1322] model.layers.0.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.892.976 [mindspore/train/serialization.py:1322] model.layers.0.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.045 [mindspore/train/serialization.py:1322] model.layers.0.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.114 [mindspore/train/serialization.py:1322] model.layers.0.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.203 [mindspore/train/serialization.py:1322] model.layers.1.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.274 [mindspore/train/serialization.py:1322] model.layers.1.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.341 [mindspore/train/serialization.py:1322] model.layers.1.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.409 [mindspore/train/serialization.py:1322] model.layers.1.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.475 [mindspore/train/serialization.py:1322] model.layers.1.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.540 [mindspore/train/serialization.py:1322] model.layers.1.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.606 [mindspore/train/serialization.py:1322] model.layers.1.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.669 [mindspore/train/serialization.py:1322] model.layers.1.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.733 [mindspore/train/serialization.py:1322] model.layers.2.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.796 [mindspore/train/serialization.py:1322] model.layers.2.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.859 [mindspore/train/serialization.py:1322] model.layers.2.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.922 [mindspore/train/serialization.py:1322] model.layers.2.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.893.989 [mindspore/train/serialization.py:1322] model.layers.2.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.052 [mindspore/train/serialization.py:1322] model.layers.2.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.116 [mindspore/train/serialization.py:1322] model.layers.2.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.179 [mindspore/train/serialization.py:1322] model.layers.2.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.241 [mindspore/train/serialization.py:1322] model.layers.3.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.304 [mindspore/train/serialization.py:1322] model.layers.3.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.369 [mindspore/train/serialization.py:1322] model.layers.3.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.433 [mindspore/train/serialization.py:1322] model.layers.3.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.497 [mindspore/train/serialization.py:1322] model.layers.3.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.560 [mindspore/train/serialization.py:1322] model.layers.3.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.622 [mindspore/train/serialization.py:1322] model.layers.3.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.693 [mindspore/train/serialization.py:1322] model.layers.3.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.759 [mindspore/train/serialization.py:1322] model.layers.4.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.821 [mindspore/train/serialization.py:1322] model.layers.4.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.884 [mindspore/train/serialization.py:1322] model.layers.4.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.894.947 [mindspore/train/serialization.py:1322] model.layers.4.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.010 [mindspore/train/serialization.py:1322] model.layers.4.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.072 [mindspore/train/serialization.py:1322] model.layers.4.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.134 [mindspore/train/serialization.py:1322] model.layers.4.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.193 [mindspore/train/serialization.py:1322] model.layers.4.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.256 [mindspore/train/serialization.py:1322] model.layers.5.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.318 [mindspore/train/serialization.py:1322] model.layers.5.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.381 [mindspore/train/serialization.py:1322] model.layers.5.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.445 [mindspore/train/serialization.py:1322] model.layers.5.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.507 [mindspore/train/serialization.py:1322] model.layers.5.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.591 [mindspore/train/serialization.py:1322] model.layers.5.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.659 [mindspore/train/serialization.py:1322] model.layers.5.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.724 [mindspore/train/serialization.py:1322] model.layers.5.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.789 [mindspore/train/serialization.py:1322] model.layers.6.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.853 [mindspore/train/serialization.py:1322] model.layers.6.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.917 [mindspore/train/serialization.py:1322] model.layers.6.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.895.978 [mindspore/train/serialization.py:1322] model.layers.6.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.040 [mindspore/train/serialization.py:1322] model.layers.6.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.112 [mindspore/train/serialization.py:1322] model.layers.6.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.179 [mindspore/train/serialization.py:1322] model.layers.6.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.245 [mindspore/train/serialization.py:1322] model.layers.6.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.310 [mindspore/train/serialization.py:1322] model.layers.7.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.374 [mindspore/train/serialization.py:1322] model.layers.7.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.436 [mindspore/train/serialization.py:1322] model.layers.7.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.499 [mindspore/train/serialization.py:1322] model.layers.7.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.564 [mindspore/train/serialization.py:1322] model.layers.7.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.627 [mindspore/train/serialization.py:1322] model.layers.7.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.690 [mindspore/train/serialization.py:1322] model.layers.7.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.756 [mindspore/train/serialization.py:1322] model.layers.7.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.818 [mindspore/train/serialization.py:1322] model.layers.8.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.881 [mindspore/train/serialization.py:1322] model.layers.8.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.896.945 [mindspore/train/serialization.py:1322] model.layers.8.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.010 [mindspore/train/serialization.py:1322] model.layers.8.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.072 [mindspore/train/serialization.py:1322] model.layers.8.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.134 [mindspore/train/serialization.py:1322] model.layers.8.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.195 [mindspore/train/serialization.py:1322] model.layers.8.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.257 [mindspore/train/serialization.py:1322] model.layers.8.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.319 [mindspore/train/serialization.py:1322] model.layers.9.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.381 [mindspore/train/serialization.py:1322] model.layers.9.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.443 [mindspore/train/serialization.py:1322] model.layers.9.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.502 [mindspore/train/serialization.py:1322] model.layers.9.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.571 [mindspore/train/serialization.py:1322] model.layers.9.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.636 [mindspore/train/serialization.py:1322] model.layers.9.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.699 [mindspore/train/serialization.py:1322] model.layers.9.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.761 [mindspore/train/serialization.py:1322] model.layers.9.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.824 [mindspore/train/serialization.py:1322] model.layers.10.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.884 [mindspore/train/serialization.py:1322] model.layers.10.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.897.947 [mindspore/train/serialization.py:1322] model.layers.10.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.009 [mindspore/train/serialization.py:1322] model.layers.10.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.072 [mindspore/train/serialization.py:1322] model.layers.10.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.135 [mindspore/train/serialization.py:1322] model.layers.10.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.196 [mindspore/train/serialization.py:1322] model.layers.10.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.257 [mindspore/train/serialization.py:1322] model.layers.10.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.319 [mindspore/train/serialization.py:1322] model.layers.11.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.382 [mindspore/train/serialization.py:1322] model.layers.11.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.443 [mindspore/train/serialization.py:1322] model.layers.11.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.507 [mindspore/train/serialization.py:1322] model.layers.11.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.567 [mindspore/train/serialization.py:1322] model.layers.11.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.628 [mindspore/train/serialization.py:1322] model.layers.11.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.689 [mindspore/train/serialization.py:1322] model.layers.11.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.751 [mindspore/train/serialization.py:1322] model.layers.11.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.814 [mindspore/train/serialization.py:1322] model.layers.12.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.874 [mindspore/train/serialization.py:1322] model.layers.12.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.898.935 [mindspore/train/serialization.py:1322] model.layers.12.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.004 [mindspore/train/serialization.py:1322] model.layers.12.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.068 [mindspore/train/serialization.py:1322] model.layers.12.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.131 [mindspore/train/serialization.py:1322] model.layers.12.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.194 [mindspore/train/serialization.py:1322] model.layers.12.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.255 [mindspore/train/serialization.py:1322] model.layers.12.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.318 [mindspore/train/serialization.py:1322] model.layers.13.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.379 [mindspore/train/serialization.py:1322] model.layers.13.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.442 [mindspore/train/serialization.py:1322] model.layers.13.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.504 [mindspore/train/serialization.py:1322] model.layers.13.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.577 [mindspore/train/serialization.py:1322] model.layers.13.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.642 [mindspore/train/serialization.py:1322] model.layers.13.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.706 [mindspore/train/serialization.py:1322] model.layers.13.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.769 [mindspore/train/serialization.py:1322] model.layers.13.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.832 [mindspore/train/serialization.py:1322] model.layers.14.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.894 [mindspore/train/serialization.py:1322] model.layers.14.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.899.954 [mindspore/train/serialization.py:1322] model.layers.14.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.016 [mindspore/train/serialization.py:1322] model.layers.14.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.078 [mindspore/train/serialization.py:1322] model.layers.14.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.140 [mindspore/train/serialization.py:1322] model.layers.14.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.200 [mindspore/train/serialization.py:1322] model.layers.14.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.259 [mindspore/train/serialization.py:1322] model.layers.14.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.320 [mindspore/train/serialization.py:1322] model.layers.15.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.389 [mindspore/train/serialization.py:1322] model.layers.15.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.453 [mindspore/train/serialization.py:1322] model.layers.15.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.515 [mindspore/train/serialization.py:1322] model.layers.15.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.579 [mindspore/train/serialization.py:1322] model.layers.15.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.638 [mindspore/train/serialization.py:1322] model.layers.15.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.700 [mindspore/train/serialization.py:1322] model.layers.15.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.761 [mindspore/train/serialization.py:1322] model.layers.15.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.822 [mindspore/train/serialization.py:1322] model.layers.16.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.883 [mindspore/train/serialization.py:1322] model.layers.16.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.900.943 [mindspore/train/serialization.py:1322] model.layers.16.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.003 [mindspore/train/serialization.py:1322] model.layers.16.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.064 [mindspore/train/serialization.py:1322] model.layers.16.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.126 [mindspore/train/serialization.py:1322] model.layers.16.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.188 [mindspore/train/serialization.py:1322] model.layers.16.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.248 [mindspore/train/serialization.py:1322] model.layers.16.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.311 [mindspore/train/serialization.py:1322] model.layers.17.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.372 [mindspore/train/serialization.py:1322] model.layers.17.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.434 [mindspore/train/serialization.py:1322] model.layers.17.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.495 [mindspore/train/serialization.py:1322] model.layers.17.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.556 [mindspore/train/serialization.py:1322] model.layers.17.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.618 [mindspore/train/serialization.py:1322] model.layers.17.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.682 [mindspore/train/serialization.py:1322] model.layers.17.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.745 [mindspore/train/serialization.py:1322] model.layers.17.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.816 [mindspore/train/serialization.py:1322] model.layers.18.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.880 [mindspore/train/serialization.py:1322] model.layers.18.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.901.940 [mindspore/train/serialization.py:1322] model.layers.18.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.002 [mindspore/train/serialization.py:1322] model.layers.18.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.064 [mindspore/train/serialization.py:1322] model.layers.18.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.126 [mindspore/train/serialization.py:1322] model.layers.18.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.189 [mindspore/train/serialization.py:1322] model.layers.18.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.249 [mindspore/train/serialization.py:1322] model.layers.18.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.311 [mindspore/train/serialization.py:1322] model.layers.19.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.373 [mindspore/train/serialization.py:1322] model.layers.19.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.436 [mindspore/train/serialization.py:1322] model.layers.19.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.498 [mindspore/train/serialization.py:1322] model.layers.19.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.559 [mindspore/train/serialization.py:1322] model.layers.19.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.620 [mindspore/train/serialization.py:1322] model.layers.19.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.682 [mindspore/train/serialization.py:1322] model.layers.19.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.744 [mindspore/train/serialization.py:1322] model.layers.19.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.805 [mindspore/train/serialization.py:1322] model.layers.20.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.869 [mindspore/train/serialization.py:1322] model.layers.20.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.931 [mindspore/train/serialization.py:1322] model.layers.20.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.902.992 [mindspore/train/serialization.py:1322] model.layers.20.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.053 [mindspore/train/serialization.py:1322] model.layers.20.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.115 [mindspore/train/serialization.py:1322] model.layers.20.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.186 [mindspore/train/serialization.py:1322] model.layers.20.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.251 [mindspore/train/serialization.py:1322] model.layers.20.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.312 [mindspore/train/serialization.py:1322] model.layers.21.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.374 [mindspore/train/serialization.py:1322] model.layers.21.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.436 [mindspore/train/serialization.py:1322] model.layers.21.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.498 [mindspore/train/serialization.py:1322] model.layers.21.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.574 [mindspore/train/serialization.py:1322] model.layers.21.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.637 [mindspore/train/serialization.py:1322] model.layers.21.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.699 [mindspore/train/serialization.py:1322] model.layers.21.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.761 [mindspore/train/serialization.py:1322] model.layers.21.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.824 [mindspore/train/serialization.py:1322] model.layers.22.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.886 [mindspore/train/serialization.py:1322] model.layers.22.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.903.950 [mindspore/train/serialization.py:1322] model.layers.22.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.012 [mindspore/train/serialization.py:1322] model.layers.22.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.073 [mindspore/train/serialization.py:1322] model.layers.22.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.135 [mindspore/train/serialization.py:1322] model.layers.22.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.196 [mindspore/train/serialization.py:1322] model.layers.22.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.257 [mindspore/train/serialization.py:1322] model.layers.22.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.317 [mindspore/train/serialization.py:1322] model.layers.23.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.377 [mindspore/train/serialization.py:1322] model.layers.23.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.438 [mindspore/train/serialization.py:1322] model.layers.23.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.500 [mindspore/train/serialization.py:1322] model.layers.23.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.562 [mindspore/train/serialization.py:1322] model.layers.23.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.628 [mindspore/train/serialization.py:1322] model.layers.23.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.692 [mindspore/train/serialization.py:1322] model.layers.23.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.757 [mindspore/train/serialization.py:1322] model.layers.23.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.819 [mindspore/train/serialization.py:1322] model.layers.24.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.881 [mindspore/train/serialization.py:1322] model.layers.24.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.904.943 [mindspore/train/serialization.py:1322] model.layers.24.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.004 [mindspore/train/serialization.py:1322] model.layers.24.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.065 [mindspore/train/serialization.py:1322] model.layers.24.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.126 [mindspore/train/serialization.py:1322] model.layers.24.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.188 [mindspore/train/serialization.py:1322] model.layers.24.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.250 [mindspore/train/serialization.py:1322] model.layers.24.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.309 [mindspore/train/serialization.py:1322] model.layers.25.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.371 [mindspore/train/serialization.py:1322] model.layers.25.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.432 [mindspore/train/serialization.py:1322] model.layers.25.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.494 [mindspore/train/serialization.py:1322] model.layers.25.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.554 [mindspore/train/serialization.py:1322] model.layers.25.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.614 [mindspore/train/serialization.py:1322] model.layers.25.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.676 [mindspore/train/serialization.py:1322] model.layers.25.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.736 [mindspore/train/serialization.py:1322] model.layers.25.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.797 [mindspore/train/serialization.py:1322] model.layers.26.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.859 [mindspore/train/serialization.py:1322] model.layers.26.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.917 [mindspore/train/serialization.py:1322] model.layers.26.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.905.984 [mindspore/train/serialization.py:1322] model.layers.26.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.049 [mindspore/train/serialization.py:1322] model.layers.26.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.114 [mindspore/train/serialization.py:1322] model.layers.26.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.177 [mindspore/train/serialization.py:1322] model.layers.26.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.240 [mindspore/train/serialization.py:1322] model.layers.26.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.299 [mindspore/train/serialization.py:1322] model.layers.27.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.360 [mindspore/train/serialization.py:1322] model.layers.27.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.421 [mindspore/train/serialization.py:1322] model.layers.27.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.483 [mindspore/train/serialization.py:1322] model.layers.27.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.545 [mindspore/train/serialization.py:1322] model.layers.27.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.605 [mindspore/train/serialization.py:1322] model.layers.27.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.667 [mindspore/train/serialization.py:1322] model.layers.27.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.729 [mindspore/train/serialization.py:1322] model.layers.27.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.790 [mindspore/train/serialization.py:1322] model.layers.28.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.850 [mindspore/train/serialization.py:1322] model.layers.28.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.910 [mindspore/train/serialization.py:1322] model.layers.28.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.906.971 [mindspore/train/serialization.py:1322] model.layers.28.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.033 [mindspore/train/serialization.py:1322] model.layers.28.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.095 [mindspore/train/serialization.py:1322] model.layers.28.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.156 [mindspore/train/serialization.py:1322] model.layers.28.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.214 [mindspore/train/serialization.py:1322] model.layers.28.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.275 [mindspore/train/serialization.py:1322] model.layers.29.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.336 [mindspore/train/serialization.py:1322] model.layers.29.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.404 [mindspore/train/serialization.py:1322] model.layers.29.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.468 [mindspore/train/serialization.py:1322] model.layers.29.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.537 [mindspore/train/serialization.py:1322] model.layers.29.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.604 [mindspore/train/serialization.py:1322] model.layers.29.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.667 [mindspore/train/serialization.py:1322] model.layers.29.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.730 [mindspore/train/serialization.py:1322] model.layers.29.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.792 [mindspore/train/serialization.py:1322] model.layers.30.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.854 [mindspore/train/serialization.py:1322] model.layers.30.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.914 [mindspore/train/serialization.py:1322] model.layers.30.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.907.975 [mindspore/train/serialization.py:1322] model.layers.30.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.036 [mindspore/train/serialization.py:1322] model.layers.30.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.099 [mindspore/train/serialization.py:1322] model.layers.30.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.161 [mindspore/train/serialization.py:1322] model.layers.30.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.221 [mindspore/train/serialization.py:1322] model.layers.30.attention.wo.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.282 [mindspore/train/serialization.py:1322] model.layers.31.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.343 [mindspore/train/serialization.py:1322] model.layers.31.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.404 [mindspore/train/serialization.py:1322] model.layers.31.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.466 [mindspore/train/serialization.py:1322] model.layers.31.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.528 [mindspore/train/serialization.py:1322] model.layers.31.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.589 [mindspore/train/serialization.py:1322] model.layers.31.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.652 [mindspore/train/serialization.py:1322] model.layers.31.attention.wo.mindpet_delta_lora_a is not loaded.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:49.908.715 [mindspore/train/serialization.py:1322] model.layers.31.attention.wo.mindpet_delta_lora_b is not loaded.
2024-01-23 22:58:49,908 - mindformers[mindformers/models/base_model.py:115] - INFO - weights in /home/ma-user/work/lama_7b.ckpt are loaded
[INFO] 2024-01-23 22:58:49,911 [678030] [SDK] : Start to freeze model for delta, mode: lora, include list: None, exclude list: None
[INFO] 2024-01-23 22:58:49,913 [678030] [SDK] : Start to freeze model, include list: ['*'], exclude list: ['*mindpet_delta_lora*']
[INFO] 2024-01-23 22:58:49,924 [678030] [SDK] : End to freeze model.
[INFO] 2024-01-23 22:58:49,924 [678030] [SDK] : End to freeze model for delta.
2024-01-23 22:58:49,942 - mindformers[mindformers/trainer/base_trainer.py:527] - INFO - Network Parameters: 16 M.
2024-01-23 22:58:49,942 - mindformers[mindformers/trainer/base_trainer.py:658] - INFO - .........Build Optimizer For Train..........
2024-01-23 22:58:49,943 - mindformers[mindformers/trainer/base_trainer.py:423] - INFO - .........Build Optimizer From Config..........
2024-01-23 22:58:49,943 - mindformers[mindformers/trainer/base_trainer.py:456] - INFO - .........Build LR Schedule From Config..........
2024-01-23 22:58:49,950 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:74] - WARNING - dynamic_lr_schedule will be reset and invalid when layer_scale is False.
2024-01-23 22:58:49,956 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:113] - INFO - Param groups = {
  "decay": {
    "weight_decay": 0.0,
    "params": [
      "model.layers.0.attention.wq.mindpet_delta_lora_a",
      "model.layers.0.attention.wq.mindpet_delta_lora_b",
      "model.layers.0.attention.wk.mindpet_delta_lora_a",
      "model.layers.0.attention.wk.mindpet_delta_lora_b",
      "model.layers.0.attention.wv.mindpet_delta_lora_a",
      "model.layers.0.attention.wv.mindpet_delta_lora_b",
      "model.layers.0.attention.wo.mindpet_delta_lora_a",
      "model.layers.0.attention.wo.mindpet_delta_lora_b",
      "model.layers.1.attention.wq.mindpet_delta_lora_a",
      "model.layers.1.attention.wq.mindpet_delta_lora_b",
      "model.layers.1.attention.wk.mindpet_delta_lora_a",
      "model.layers.1.attention.wk.mindpet_delta_lora_b",
      "model.layers.1.attention.wv.mindpet_delta_lora_a",
      "model.layers.1.attention.wv.mindpet_delta_lora_b",
      "model.layers.1.attention.wo.mindpet_delta_lora_a",
      "model.layers.1.attention.wo.mindpet_delta_lora_b",
      "model.layers.2.attention.wq.mindpet_delta_lora_a",
      "model.layers.2.attention.wq.mindpet_delta_lora_b",
      "model.layers.2.attention.wk.mindpet_delta_lora_a",
      "model.layers.2.attention.wk.mindpet_delta_lora_b",
      "model.layers.2.attention.wv.mindpet_delta_lora_a",
      "model.layers.2.attention.wv.mindpet_delta_lora_b",
      "model.layers.2.attention.wo.mindpet_delta_lora_a",
      "model.layers.2.attention.wo.mindpet_delta_lora_b",
      "model.layers.3.attention.wq.mindpet_delta_lora_a",
      "model.layers.3.attention.wq.mindpet_delta_lora_b",
      "model.layers.3.attention.wk.mindpet_delta_lora_a",
      "model.layers.3.attention.wk.mindpet_delta_lora_b",
      "model.layers.3.attention.wv.mindpet_delta_lora_a",
      "model.layers.3.attention.wv.mindpet_delta_lora_b",
      "model.layers.3.attention.wo.mindpet_delta_lora_a",
      "model.layers.3.attention.wo.mindpet_delta_lora_b",
      "model.layers.4.attention.wq.mindpet_delta_lora_a",
      "model.layers.4.attention.wq.mindpet_delta_lora_b",
      "model.layers.4.attention.wk.mindpet_delta_lora_a",
      "model.layers.4.attention.wk.mindpet_delta_lora_b",
      "model.layers.4.attention.wv.mindpet_delta_lora_a",
      "model.layers.4.attention.wv.mindpet_delta_lora_b",
      "model.layers.4.attention.wo.mindpet_delta_lora_a",
      "model.layers.4.attention.wo.mindpet_delta_lora_b",
      "model.layers.5.attention.wq.mindpet_delta_lora_a",
      "model.layers.5.attention.wq.mindpet_delta_lora_b",
      "model.layers.5.attention.wk.mindpet_delta_lora_a",
      "model.layers.5.attention.wk.mindpet_delta_lora_b",
      "model.layers.5.attention.wv.mindpet_delta_lora_a",
      "model.layers.5.attention.wv.mindpet_delta_lora_b",
      "model.layers.5.attention.wo.mindpet_delta_lora_a",
      "model.layers.5.attention.wo.mindpet_delta_lora_b",
      "model.layers.6.attention.wq.mindpet_delta_lora_a",
      "model.layers.6.attention.wq.mindpet_delta_lora_b",
      "model.layers.6.attention.wk.mindpet_delta_lora_a",
      "model.layers.6.attention.wk.mindpet_delta_lora_b",
      "model.layers.6.attention.wv.mindpet_delta_lora_a",
      "model.layers.6.attention.wv.mindpet_delta_lora_b",
      "model.layers.6.attention.wo.mindpet_delta_lora_a",
      "model.layers.6.attention.wo.mindpet_delta_lora_b",
      "model.layers.7.attention.wq.mindpet_delta_lora_a",
      "model.layers.7.attention.wq.mindpet_delta_lora_b",
      "model.layers.7.attention.wk.mindpet_delta_lora_a",
      "model.layers.7.attention.wk.mindpet_delta_lora_b",
      "model.layers.7.attention.wv.mindpet_delta_lora_a",
      "model.layers.7.attention.wv.mindpet_delta_lora_b",
      "model.layers.7.attention.wo.mindpet_delta_lora_a",
      "model.layers.7.attention.wo.mindpet_delta_lora_b",
      "model.layers.8.attention.wq.mindpet_delta_lora_a",
      "model.layers.8.attention.wq.mindpet_delta_lora_b",
      "model.layers.8.attention.wk.mindpet_delta_lora_a",
      "model.layers.8.attention.wk.mindpet_delta_lora_b",
      "model.layers.8.attention.wv.mindpet_delta_lora_a",
      "model.layers.8.attention.wv.mindpet_delta_lora_b",
      "model.layers.8.attention.wo.mindpet_delta_lora_a",
      "model.layers.8.attention.wo.mindpet_delta_lora_b",
      "model.layers.9.attention.wq.mindpet_delta_lora_a",
      "model.layers.9.attention.wq.mindpet_delta_lora_b",
      "model.layers.9.attention.wk.mindpet_delta_lora_a",
      "model.layers.9.attention.wk.mindpet_delta_lora_b",
      "model.layers.9.attention.wv.mindpet_delta_lora_a",
      "model.layers.9.attention.wv.mindpet_delta_lora_b",
      "model.layers.9.attention.wo.mindpet_delta_lora_a",
      "model.layers.9.attention.wo.mindpet_delta_lora_b",
      "model.layers.10.attention.wq.mindpet_delta_lora_a",
      "model.layers.10.attention.wq.mindpet_delta_lora_b",
      "model.layers.10.attention.wk.mindpet_delta_lora_a",
      "model.layers.10.attention.wk.mindpet_delta_lora_b",
      "model.layers.10.attention.wv.mindpet_delta_lora_a",
      "model.layers.10.attention.wv.mindpet_delta_lora_b",
      "model.layers.10.attention.wo.mindpet_delta_lora_a",
      "model.layers.10.attention.wo.mindpet_delta_lora_b",
      "model.layers.11.attention.wq.mindpet_delta_lora_a",
      "model.layers.11.attention.wq.mindpet_delta_lora_b",
      "model.layers.11.attention.wk.mindpet_delta_lora_a",
      "model.layers.11.attention.wk.mindpet_delta_lora_b",
      "model.layers.11.attention.wv.mindpet_delta_lora_a",
      "model.layers.11.attention.wv.mindpet_delta_lora_b",
      "model.layers.11.attention.wo.mindpet_delta_lora_a",
      "model.layers.11.attention.wo.mindpet_delta_lora_b",
      "model.layers.12.attention.wq.mindpet_delta_lora_a",
      "model.layers.12.attention.wq.mindpet_delta_lora_b",
      "model.layers.12.attention.wk.mindpet_delta_lora_a",
      "model.layers.12.attention.wk.mindpet_delta_lora_b",
      "model.layers.12.attention.wv.mindpet_delta_lora_a",
      "model.layers.12.attention.wv.mindpet_delta_lora_b",
      "model.layers.12.attention.wo.mindpet_delta_lora_a",
      "model.layers.12.attention.wo.mindpet_delta_lora_b",
      "model.layers.13.attention.wq.mindpet_delta_lora_a",
      "model.layers.13.attention.wq.mindpet_delta_lora_b",
      "model.layers.13.attention.wk.mindpet_delta_lora_a",
      "model.layers.13.attention.wk.mindpet_delta_lora_b",
      "model.layers.13.attention.wv.mindpet_delta_lora_a",
      "model.layers.13.attention.wv.mindpet_delta_lora_b",
      "model.layers.13.attention.wo.mindpet_delta_lora_a",
      "model.layers.13.attention.wo.mindpet_delta_lora_b",
      "model.layers.14.attention.wq.mindpet_delta_lora_a",
      "model.layers.14.attention.wq.mindpet_delta_lora_b",
      "model.layers.14.attention.wk.mindpet_delta_lora_a",
      "model.layers.14.attention.wk.mindpet_delta_lora_b",
      "model.layers.14.attention.wv.mindpet_delta_lora_a",
      "model.layers.14.attention.wv.mindpet_delta_lora_b",
      "model.layers.14.attention.wo.mindpet_delta_lora_a",
      "model.layers.14.attention.wo.mindpet_delta_lora_b",
      "model.layers.15.attention.wq.mindpet_delta_lora_a",
      "model.layers.15.attention.wq.mindpet_delta_lora_b",
      "model.layers.15.attention.wk.mindpet_delta_lora_a",
      "model.layers.15.attention.wk.mindpet_delta_lora_b",
      "model.layers.15.attention.wv.mindpet_delta_lora_a",
      "model.layers.15.attention.wv.mindpet_delta_lora_b",
      "model.layers.15.attention.wo.mindpet_delta_lora_a",
      "model.layers.15.attention.wo.mindpet_delta_lora_b",
      "model.layers.16.attention.wq.mindpet_delta_lora_a",
      "model.layers.16.attention.wq.mindpet_delta_lora_b",
      "model.layers.16.attention.wk.mindpet_delta_lora_a",
      "model.layers.16.attention.wk.mindpet_delta_lora_b",
      "model.layers.16.attention.wv.mindpet_delta_lora_a",
      "model.layers.16.attention.wv.mindpet_delta_lora_b",
      "model.layers.16.attention.wo.mindpet_delta_lora_a",
      "model.layers.16.attention.wo.mindpet_delta_lora_b",
      "model.layers.17.attention.wq.mindpet_delta_lora_a",
      "model.layers.17.attention.wq.mindpet_delta_lora_b",
      "model.layers.17.attention.wk.mindpet_delta_lora_a",
      "model.layers.17.attention.wk.mindpet_delta_lora_b",
      "model.layers.17.attention.wv.mindpet_delta_lora_a",
      "model.layers.17.attention.wv.mindpet_delta_lora_b",
      "model.layers.17.attention.wo.mindpet_delta_lora_a",
      "model.layers.17.attention.wo.mindpet_delta_lora_b",
      "model.layers.18.attention.wq.mindpet_delta_lora_a",
      "model.layers.18.attention.wq.mindpet_delta_lora_b",
      "model.layers.18.attention.wk.mindpet_delta_lora_a",
      "model.layers.18.attention.wk.mindpet_delta_lora_b",
      "model.layers.18.attention.wv.mindpet_delta_lora_a",
      "model.layers.18.attention.wv.mindpet_delta_lora_b",
      "model.layers.18.attention.wo.mindpet_delta_lora_a",
      "model.layers.18.attention.wo.mindpet_delta_lora_b",
      "model.layers.19.attention.wq.mindpet_delta_lora_a",
      "model.layers.19.attention.wq.mindpet_delta_lora_b",
      "model.layers.19.attention.wk.mindpet_delta_lora_a",
      "model.layers.19.attention.wk.mindpet_delta_lora_b",
      "model.layers.19.attention.wv.mindpet_delta_lora_a",
      "model.layers.19.attention.wv.mindpet_delta_lora_b",
      "model.layers.19.attention.wo.mindpet_delta_lora_a",
      "model.layers.19.attention.wo.mindpet_delta_lora_b",
      "model.layers.20.attention.wq.mindpet_delta_lora_a",
      "model.layers.20.attention.wq.mindpet_delta_lora_b",
      "model.layers.20.attention.wk.mindpet_delta_lora_a",
      "model.layers.20.attention.wk.mindpet_delta_lora_b",
      "model.layers.20.attention.wv.mindpet_delta_lora_a",
      "model.layers.20.attention.wv.mindpet_delta_lora_b",
      "model.layers.20.attention.wo.mindpet_delta_lora_a",
      "model.layers.20.attention.wo.mindpet_delta_lora_b",
      "model.layers.21.attention.wq.mindpet_delta_lora_a",
      "model.layers.21.attention.wq.mindpet_delta_lora_b",
      "model.layers.21.attention.wk.mindpet_delta_lora_a",
      "model.layers.21.attention.wk.mindpet_delta_lora_b",
      "model.layers.21.attention.wv.mindpet_delta_lora_a",
      "model.layers.21.attention.wv.mindpet_delta_lora_b",
      "model.layers.21.attention.wo.mindpet_delta_lora_a",
      "model.layers.21.attention.wo.mindpet_delta_lora_b",
      "model.layers.22.attention.wq.mindpet_delta_lora_a",
      "model.layers.22.attention.wq.mindpet_delta_lora_b",
      "model.layers.22.attention.wk.mindpet_delta_lora_a",
      "model.layers.22.attention.wk.mindpet_delta_lora_b",
      "model.layers.22.attention.wv.mindpet_delta_lora_a",
      "model.layers.22.attention.wv.mindpet_delta_lora_b",
      "model.layers.22.attention.wo.mindpet_delta_lora_a",
      "model.layers.22.attention.wo.mindpet_delta_lora_b",
      "model.layers.23.attention.wq.mindpet_delta_lora_a",
      "model.layers.23.attention.wq.mindpet_delta_lora_b",
      "model.layers.23.attention.wk.mindpet_delta_lora_a",
      "model.layers.23.attention.wk.mindpet_delta_lora_b",
      "model.layers.23.attention.wv.mindpet_delta_lora_a",
      "model.layers.23.attention.wv.mindpet_delta_lora_b",
      "model.layers.23.attention.wo.mindpet_delta_lora_a",
      "model.layers.23.attention.wo.mindpet_delta_lora_b",
      "model.layers.24.attention.wq.mindpet_delta_lora_a",
      "model.layers.24.attention.wq.mindpet_delta_lora_b",
      "model.layers.24.attention.wk.mindpet_delta_lora_a",
      "model.layers.24.attention.wk.mindpet_delta_lora_b",
      "model.layers.24.attention.wv.mindpet_delta_lora_a",
      "model.layers.24.attention.wv.mindpet_delta_lora_b",
      "model.layers.24.attention.wo.mindpet_delta_lora_a",
      "model.layers.24.attention.wo.mindpet_delta_lora_b",
      "model.layers.25.attention.wq.mindpet_delta_lora_a",
      "model.layers.25.attention.wq.mindpet_delta_lora_b",
      "model.layers.25.attention.wk.mindpet_delta_lora_a",
      "model.layers.25.attention.wk.mindpet_delta_lora_b",
      "model.layers.25.attention.wv.mindpet_delta_lora_a",
      "model.layers.25.attention.wv.mindpet_delta_lora_b",
      "model.layers.25.attention.wo.mindpet_delta_lora_a",
      "model.layers.25.attention.wo.mindpet_delta_lora_b",
      "model.layers.26.attention.wq.mindpet_delta_lora_a",
      "model.layers.26.attention.wq.mindpet_delta_lora_b",
      "model.layers.26.attention.wk.mindpet_delta_lora_a",
      "model.layers.26.attention.wk.mindpet_delta_lora_b",
      "model.layers.26.attention.wv.mindpet_delta_lora_a",
      "model.layers.26.attention.wv.mindpet_delta_lora_b",
      "model.layers.26.attention.wo.mindpet_delta_lora_a",
      "model.layers.26.attention.wo.mindpet_delta_lora_b",
      "model.layers.27.attention.wq.mindpet_delta_lora_a",
      "model.layers.27.attention.wq.mindpet_delta_lora_b",
      "model.layers.27.attention.wk.mindpet_delta_lora_a",
      "model.layers.27.attention.wk.mindpet_delta_lora_b",
      "model.layers.27.attention.wv.mindpet_delta_lora_a",
      "model.layers.27.attention.wv.mindpet_delta_lora_b",
      "model.layers.27.attention.wo.mindpet_delta_lora_a",
      "model.layers.27.attention.wo.mindpet_delta_lora_b",
      "model.layers.28.attention.wq.mindpet_delta_lora_a",
      "model.layers.28.attention.wq.mindpet_delta_lora_b",
      "model.layers.28.attention.wk.mindpet_delta_lora_a",
      "model.layers.28.attention.wk.mindpet_delta_lora_b",
      "model.layers.28.attention.wv.mindpet_delta_lora_a",
      "model.layers.28.attention.wv.mindpet_delta_lora_b",
      "model.layers.28.attention.wo.mindpet_delta_lora_a",
      "model.layers.28.attention.wo.mindpet_delta_lora_b",
      "model.layers.29.attention.wq.mindpet_delta_lora_a",
      "model.layers.29.attention.wq.mindpet_delta_lora_b",
      "model.layers.29.attention.wk.mindpet_delta_lora_a",
      "model.layers.29.attention.wk.mindpet_delta_lora_b",
      "model.layers.29.attention.wv.mindpet_delta_lora_a",
      "model.layers.29.attention.wv.mindpet_delta_lora_b",
      "model.layers.29.attention.wo.mindpet_delta_lora_a",
      "model.layers.29.attention.wo.mindpet_delta_lora_b",
      "model.layers.30.attention.wq.mindpet_delta_lora_a",
      "model.layers.30.attention.wq.mindpet_delta_lora_b",
      "model.layers.30.attention.wk.mindpet_delta_lora_a",
      "model.layers.30.attention.wk.mindpet_delta_lora_b",
      "model.layers.30.attention.wv.mindpet_delta_lora_a",
      "model.layers.30.attention.wv.mindpet_delta_lora_b",
      "model.layers.30.attention.wo.mindpet_delta_lora_a",
      "model.layers.30.attention.wo.mindpet_delta_lora_b",
      "model.layers.31.attention.wq.mindpet_delta_lora_a",
      "model.layers.31.attention.wq.mindpet_delta_lora_b",
      "model.layers.31.attention.wk.mindpet_delta_lora_a",
      "model.layers.31.attention.wk.mindpet_delta_lora_b",
      "model.layers.31.attention.wv.mindpet_delta_lora_a",
      "model.layers.31.attention.wv.mindpet_delta_lora_b",
      "model.layers.31.attention.wo.mindpet_delta_lora_a",
      "model.layers.31.attention.wo.mindpet_delta_lora_b"
    ]
  }
}
2024-01-23 22:58:51,936 - mindformers[mindformers/trainer/base_trainer.py:664] - INFO - .........Build Running Wrapper From Config For Train..........
2024-01-23 22:58:51,937 - mindformers[mindformers/trainer/base_trainer.py:493] - INFO - .........Build Model Wrapper for Train From Config..........
2024-01-23 22:58:51,949 - mindformers[mindformers/trainer/base_trainer.py:671] - INFO - .........Build Callbacks For Train..........
2024-01-23 22:58:51,949 - mindformers[mindformers/trainer/base_trainer.py:502] - INFO - .........Build Callbacks for Train From Config..........
2024-01-23 22:58:51,952 - mindformers[mindformers/trainer/base_trainer.py:695] - INFO - .........Starting Init Train Model..........
2024-01-23 22:58:51,952 - mindformers[mindformers/trainer/base_trainer.py:730] - INFO - .........Starting Training Model..........
{'auto_trans_ckpt': False,
 'auto_tune': False,
 'autotune_per_step': 10,
 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),
               OrderedDict([('type', 'CheckpointMointor'),
                            ('prefix', 'llama_7b_lora'),
                            ('save_checkpoint_steps', 100),
                            ('integrated_save', False),
                            ('async_save', False)]),
               OrderedDict([('type', 'ObsMonitor')])],
 'context': {'device_id': 0,
             'device_target': 'Ascend',
             'enable_graph_kernel': False,
             'graph_kernel_flags': '--disable_expand_ops=Softmax,Dropout '
                                   '--enable_parallel_fusion=true '
                                   '--reduce_fuse_depth=8 '
                                   '--enable_auto_tensor_inplace=true',
             'max_call_depth': 10000,
             'save_graphs': False,
             'save_graphs_path': './graph'},
 'data_size': 26001,
 'device_num': 1,
 'do_eval': False,
 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor')])],
 'eval_dataset': {'auto_tune': False,
                  'autotune_per_step': 10,
                  'batch_size': 2,
                  'data_loader': {'dataset_dir': '',
                                  'shuffle': False,
                                  'type': 'MindDataset'},
                  'do_eval': True,
                  'drop_remainder': False,
                  'filepath_prefix': './autotune',
                  'input_columns': ['input_ids', 'labels'],
                  'num_parallel_workers': 8,
                  'numa_enable': False,
                  'prefetch_size': 1,
                  'profile': False,
                  'python_multiprocessing': False,
                  'repeat': 1,
                  'seed': 0},
 'eval_dataset_task': {'dataset_config': {'auto_tune': False,
                                          'autotune_per_step': 10,
                                          'batch_size': 2,
                                          'data_loader': {'dataset_dir': '',
                                                          'shuffle': False,
                                                          'type': 'MindDataset'},
                                          'do_eval': True,
                                          'drop_remainder': False,
                                          'filepath_prefix': './autotune',
                                          'input_columns': ['input_ids',
                                                            'labels'],
                                          'num_parallel_workers': 8,
                                          'numa_enable': False,
                                          'prefetch_size': 1,
                                          'profile': False,
                                          'python_multiprocessing': False,
                                          'repeat': 1,
                                          'seed': 0},
                       'type': 'CausalLanguageModelDataset'},
 'filepath_prefix': './autotune',
 'init_start_profile': False,
 'layer_decay': 0.65,
 'layer_scale': False,
 'load_checkpoint': None,
 'local_rank': 0,
 'lr_scale_factor': 256,
 'lr_schedule': {'learning_rate': 0.0001,
                 'total_steps': 26000,
                 'type': 'CosineWithWarmUpLR',
                 'warmup_steps': 780},
 'metric': {'type': 'PerplexityMetric'},
 'micro_batch_interleave_num': 1,
 'model': {'arch': {'type': 'LlamaForCausalLM'},
           'model_config': {'batch_size': 1,
                            'bos_token_id': 1,
                            'checkpoint_name_or_path': None,
                            'compute_dtype': 'float16',
                            'compute_in_2d': False,
                            'do_sample': False,
                            'eos_token_id': 2,
                            'extend_method': 'None',
                            'hidden_size': 4096,
                            'ignore_token_id': -100,
                            'layernorm_compute_dtype': 'float32',
                            'max_decode_length': 512,
                            'max_position_embedding': 2048,
                            'multiple_of': 256,
                            'num_heads': 32,
                            'num_layers': 32,
                            'offset': 0,
                            'pad_token_id': 0,
                            'param_init_type': 'float16',
                            'pet_config': {'lora_alpha': 16,
                                           'lora_dropout': 0.05,
                                           'lora_rank': 16,
                                           'pet_type': 'lora',
                                           'target_modules': '.*wq|.*wk|.*wv|.*wo'},
                            'pretrain_seqlen': 2048,
                            'repetition_penalty': 1,
                            'rms_norm_eps': 1e-06,
                            'rotary_dtype': 'float16',
                            'seq_length': 2048,
                            'softmax_compute_dtype': 'float16',
                            'top_k': 3,
                            'top_p': 1,
                            'type': 'LlamaConfig',
                            'use_flash_attention': False,
                            'use_past': False,
                            'use_past_shard': False,
                            'vocab_size': 32000}},
 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0xffff15ac57c0>,
 'only_save_strategy': False,
 'optimizer': {'beta1': 0.9,
               'beta2': 0.95,
               'eps': 1e-08,
               'learning_rate': 0.0001,
               'type': 'FP32StateAdamWeightDecay'},
 'output_dir': '../.././output',
 'parallel': {'enable_alltoall': False,
              'enable_parallel_optimizer': False,
              'full_batch': True,
              'gradients_mean': False,
              'parallel_mode': 1,
              'parallel_optimizer_config': {'gradient_accumulation_shard': False,
                                            'parallel_optimizer_threshold': 64},
              'search_mode': 'sharding_propagation',
              'strategy_ckpt_config': {'only_trainable_params': False,
                                       'save_file': './ckpt_strategy.ckpt'}},
 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0xfffe6c7633d0>,
 'processor': {'return_tensors': 'ms',
               'tokenizer': {'bos_token': '<s>',
                             'eos_token': '</s>',
                             'pad_token': '<unk>',
                             'type': 'LlamaTokenizer',
                             'unk_token': '<unk>'}},
 'profile': False,
 'profile_communication': False,
 'profile_memory': True,
 'profile_start_step': 1,
 'profile_stop_step': 10,
 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0xffff89111ca0>,
 'remote_save_url': 'Please input obs url on AICC platform.',
 'resume_training': False,
 'run_mode': 'finetune',
 'runner_config': {'batch_size': 2,
                   'epochs': 13000,
                   'gradient_accumulation_steps': 1,
                   'initial_epoch': 0,
                   'initial_step': 0,
                   'origin_epochs': 1,
                   'sink_mode': True,
                   'sink_size': 2},
 'runner_wrapper': {'scale_sense': DynamicLossScaleUpdateCell<>,
                    'type': 'MFTrainOneStepCell',
                    'use_clip_grad': True},
 'seed': 0,
 'src_strategy_path_or_dir': '',
 'train_dataset': {'auto_tune': False,
                   'autotune_per_step': 10,
                   'batch_size': 2,
                   'data_loader': {'dataset_dir': '/home/ma-user/work/alpaca-fastchat2048.mindrecord',
                                   'shuffle': True,
                                   'type': 'MindDataset'},
                   'do_eval': False,
                   'drop_remainder': True,
                   'filepath_prefix': './autotune',
                   'input_columns': ['input_ids', 'labels'],
                   'num_parallel_workers': 8,
                   'numa_enable': False,
                   'prefetch_size': 1,
                   'profile': False,
                   'python_multiprocessing': False,
                   'repeat': 1,
                   'seed': 0},
 'train_dataset_task': {'dataset_config': {'auto_tune': False,
                                           'autotune_per_step': 10,
                                           'batch_size': 2,
                                           'data_loader': {'dataset_dir': '/home/ma-user/work/alpaca-fastchat2048.mindrecord',
                                                           'shuffle': True,
                                                           'type': 'MindDataset'},
                                           'do_eval': False,
                                           'drop_remainder': True,
                                           'filepath_prefix': './autotune',
                                           'input_columns': ['input_ids',
                                                             'labels'],
                                           'num_parallel_workers': 8,
                                           'numa_enable': False,
                                           'prefetch_size': 1,
                                           'profile': False,
                                           'python_multiprocessing': False,
                                           'repeat': 1,
                                           'seed': 0},
                        'type': 'CausalLanguageModelDataset'},
 'trainer': {'model_name': 'llama_7b_lora',
             'type': 'CausalLanguageModelingTrainer'},
 'use_parallel': False}
2024-01-23 22:58:51,958 - mindformers[mindformers/trainer/base_trainer.py:733] - INFO - .........Model Compiling, Please Wait a Moment...........
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:51.959.243 [mindspore/train/model.py:1106] For MFLossMonitor callback, {'step_begin', 'step_end', 'epoch_begin', 'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:51.959.396 [mindspore/train/model.py:1106] For Local2ObsMonitor callback, {'step_end', 'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.
[WARNING] ME(678030:281473200668848,MainProcess):2024-01-23-22:58:51.959.774 [mindspore/train/model.py:651] In dataset_sink mode (dataset_size % sink_size) should equal to 0, it is suggested to pad/drop data or adjust sink_size. But got 'dataset_size': 26001, 'sink_size': 2.
[WARNING] DEVICE(678030,ffff9623c0b0,python):2024-01-23-22:58:52.887.984 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_memory_adapter.cc:103] Initialize] Reserved memory size for other components(1073741824) is less than recommend size(2145789184), It may lead to Out Of Memory in HCCL or other components, Please double check context key 'variable_memory_max_size'/'max_device_memory'
/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \L
  if not dirpath.find("AppData\Local\Temp"):
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \B
  """
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \c
  """
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \L
  if not dirpath.find("AppData\Local\Temp"):
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \B
  """
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \c
  """
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _nlv = LooseVersion(_np_version)
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(__version__) >= LooseVersion("1.17.0"):
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if _ is not 1:
2024-01-23 23:08:29,509 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[    2/26001], loss: 1.740, per_step_time: 270163ms, lr: 0.0, overflow cond: True, loss_scale: 1073741800.0
2024-01-23 23:08:29,533 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.0% |                                                  | 0.01 samples/s/p  81 days, 7:06:24 }
2024-01-23 23:08:35,376 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[    4/26001], loss: 1.570, per_step_time: 2453ms, lr: 0.0, overflow cond: True, loss_scale: 268435460.0
2024-01-23 23:08:35,377 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.0% |                                                  | 0.82 samples/s/p  17:42:59 }
2024-01-23 23:08:40,250 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[    6/26001], loss: 1.075, per_step_time: 2432ms, lr: 0.0, overflow cond: True, loss_scale: 67108864.0
2024-01-23 23:08:40,251 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.0% |                                                  | 0.82 samples/s/p  17:33:52 }
2024-01-23 23:08:45,137 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[    8/26001], loss: 3.309, per_step_time: 2434ms, lr: 0.0, overflow cond: True, loss_scale: 16777216.0
2024-01-23 23:08:45,138 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.0% |                                                  | 0.82 samples/s/p  17:34:27 }
2024-01-23 23:08:50,009 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   10/26001], loss: 1.060, per_step_time: 2432ms, lr: 0.0, overflow cond: True, loss_scale: 4194304.0
2024-01-23 23:08:50,010 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.0% |                                                  | 0.82 samples/s/p  17:33:34 }
2024-01-23 23:08:54,886 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   12/26001], loss: 1.735, per_step_time: 2434ms, lr: 0.0, overflow cond: True, loss_scale: 1048576.0
2024-01-23 23:08:54,887 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.0% |                                                  | 0.82 samples/s/p  17:34:25 }
2024-01-23 23:08:59,761 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   14/26001], loss: 1.415, per_step_time: 2433ms, lr: 0.0, overflow cond: True, loss_scale: 262144.0
2024-01-23 23:08:59,762 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:34:09 }
2024-01-23 23:09:04,636 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   16/26001], loss: 1.459, per_step_time: 2433ms, lr: 0.0, overflow cond: True, loss_scale: 65536.0
2024-01-23 23:09:04,636 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:33:53 }
2024-01-23 23:09:09,519 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   18/26001], loss: 1.356, per_step_time: 2438ms, lr: 0.0, overflow cond: False, loss_scale: 32768.0
2024-01-23 23:09:09,520 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:35:47 }
2024-01-23 23:09:14,399 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   20/26001], loss: 1.550, per_step_time: 2436ms, lr: 2.5641026e-07, overflow cond: True, loss_scale: 16384.0
2024-01-23 23:09:14,400 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:35:04 }
2024-01-23 23:09:19,294 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   22/26001], loss: 1.655, per_step_time: 2443ms, lr: 3.8461536e-07, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:19,294 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:37:55 }
2024-01-23 23:09:24,191 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   24/26001], loss: 1.643, per_step_time: 2445ms, lr: 6.410256e-07, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:24,192 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:38:43 }
2024-01-23 23:09:29,086 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   26/26001], loss: 2.102, per_step_time: 2443ms, lr: 8.9743594e-07, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:29,087 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:37:57 }
2024-01-23 23:09:33,978 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   28/26001], loss: 1.752, per_step_time: 2442ms, lr: 1.1538461e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:33,979 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:37:22 }
2024-01-23 23:09:38,869 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   30/26001], loss: 1.852, per_step_time: 2441ms, lr: 1.4102563e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:38,870 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:36:52 }
2024-01-23 23:09:43,766 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   32/26001], loss: 1.553, per_step_time: 2444ms, lr: 1.6666667e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:43,767 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:37:56 }
2024-01-23 23:09:48,662 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   34/26001], loss: 1.536, per_step_time: 2443ms, lr: 1.923077e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:48,662 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:37:39 }
2024-01-23 23:09:53,565 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   36/26001], loss: 1.397, per_step_time: 2447ms, lr: 2.1794872e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:53,566 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:39:11 }
2024-01-23 23:09:58,467 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   38/26001], loss: 1.612, per_step_time: 2446ms, lr: 2.4358974e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:09:58,468 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.1% |                                                  | 0.82 samples/s/p  17:38:32 }
2024-01-23 23:10:03,360 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   40/26001], loss: 1.644, per_step_time: 2442ms, lr: 2.6923076e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:10:03,361 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:36:46 }
2024-01-23 23:10:08,258 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   42/26001], loss: 1.240, per_step_time: 2445ms, lr: 2.9487178e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:10:08,259 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:37:53 }
2024-01-23 23:10:13,154 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   44/26001], loss: 1.563, per_step_time: 2443ms, lr: 3.2051282e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:10:13,154 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:37:04 }
2024-01-23 23:10:18,046 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   46/26001], loss: 1.610, per_step_time: 2442ms, lr: 3.4615387e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:10:18,047 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:36:27 }
2024-01-23 23:10:22,942 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   48/26001], loss: 1.805, per_step_time: 2443ms, lr: 3.7179489e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:10:22,942 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:36:58 }
2024-01-23 23:10:27,831 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   50/26001], loss: 1.420, per_step_time: 2440ms, lr: 3.974359e-06, overflow cond: False, loss_scale: 16384.0
2024-01-23 23:10:27,840 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:35:30 }
2024-01-23 23:10:32,721 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   52/26001], loss: 2.604, per_step_time: 2436ms, lr: 4.102564e-06, overflow cond: False, loss_scale: 8192.0
2024-01-23 23:10:32,721 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:33:51 }
2024-01-23 23:10:37,613 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   54/26001], loss: 1.233, per_step_time: 2441ms, lr: 4.3589744e-06, overflow cond: False, loss_scale: 8192.0
2024-01-23 23:10:37,613 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:36:00 }
2024-01-23 23:10:42,494 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   56/26001], loss: 1.162, per_step_time: 2436ms, lr: 4.6153846e-06, overflow cond: True, loss_scale: 4096.0
2024-01-23 23:10:42,495 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:33:36 }
2024-01-23 23:10:47,388 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   58/26001], loss: 1.661, per_step_time: 2443ms, lr: 4.7435897e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:10:47,389 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:36:19 }
2024-01-23 23:10:52,285 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   60/26001], loss: 1.400, per_step_time: 2444ms, lr: 5e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:10:52,285 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:36:40 }
2024-01-23 23:10:57,176 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   62/26001], loss: 0.939, per_step_time: 2441ms, lr: 5.25641e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:10:57,177 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:35:31 }
2024-01-23 23:11:02,069 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   64/26001], loss: 1.353, per_step_time: 2442ms, lr: 5.5128203e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:02,069 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.2% |                                                  | 0.82 samples/s/p  17:35:45 }
2024-01-23 23:11:06,961 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   66/26001], loss: 1.865, per_step_time: 2442ms, lr: 5.7692305e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:06,962 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:43 }
2024-01-23 23:11:11,853 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   68/26001], loss: 1.544, per_step_time: 2441ms, lr: 6.0256407e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:11,854 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:22 }
2024-01-23 23:11:16,745 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   70/26001], loss: 1.043, per_step_time: 2442ms, lr: 6.2820513e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:16,746 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:23 }
2024-01-23 23:11:21,637 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   72/26001], loss: 1.552, per_step_time: 2441ms, lr: 6.5384615e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:21,638 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:12 }
2024-01-23 23:11:26,528 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   74/26001], loss: 2.067, per_step_time: 2441ms, lr: 6.7948718e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:26,529 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:34:59 }
2024-01-23 23:11:31,420 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   76/26001], loss: 0.862, per_step_time: 2442ms, lr: 7.0512824e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:31,421 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:12 }
2024-01-23 23:11:36,375 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   78/26001], loss: 1.300, per_step_time: 2472ms, lr: 7.3076926e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:36,375 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.81 samples/s/p  17:48:25 }
2024-01-23 23:11:41,276 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   80/26001], loss: 1.372, per_step_time: 2446ms, lr: 7.564103e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:41,277 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:36:58 }
2024-01-23 23:11:46,174 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   82/26001], loss: 1.406, per_step_time: 2445ms, lr: 7.820513e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:46,175 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:36:13 }
2024-01-23 23:11:51,070 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   84/26001], loss: 1.271, per_step_time: 2443ms, lr: 8.076923e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:51,071 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:36 }
2024-01-23 23:11:55,964 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   86/26001], loss: 1.977, per_step_time: 2442ms, lr: 8.333333e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:11:55,964 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:04 }
2024-01-23 23:12:00,858 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   88/26001], loss: 0.647, per_step_time: 2443ms, lr: 8.589744e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:12:00,858 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:08 }
2024-01-23 23:12:05,752 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   90/26001], loss: 1.715, per_step_time: 2443ms, lr: 8.846154e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:12:05,753 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.3% |                                                  | 0.82 samples/s/p  17:35:02 }
2024-01-23 23:12:10,647 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   92/26001], loss: 1.064, per_step_time: 2443ms, lr: 9.102564e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:12:10,648 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:35:09 }
2024-01-23 23:12:15,547 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   94/26001], loss: 1.729, per_step_time: 2445ms, lr: 9.358974e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:12:15,547 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:36:02 }
2024-01-23 23:12:20,439 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   96/26001], loss: 1.430, per_step_time: 2442ms, lr: 9.615384e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:12:20,440 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:34:29 }
2024-01-23 23:12:25,334 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[   98/26001], loss: 1.500, per_step_time: 2443ms, lr: 9.871795e-06, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:12:25,337 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:34:56 }
2024-01-23 23:12:30,233 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  100/26001], loss: 1.758, per_step_time: 2443ms, lr: 1.0128205e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:12:30,233 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:34:54 }
2024-01-23 23:12:30,234 - mindformers[mindformers/core/callback/callback.py:559] - INFO - ......Saving ckpt......
2024-01-23 23:17:49,382 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  102/26001], loss: 1.356, per_step_time: 2451ms, lr: 1.0384615e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:17:49,383 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:38:06 }
2024-01-23 23:17:54,277 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  104/26001], loss: 1.582, per_step_time: 2442ms, lr: 1.0641025e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:17:54,277 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:34:24 }
2024-01-23 23:17:59,174 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  106/26001], loss: 1.801, per_step_time: 2444ms, lr: 1.08974355e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:17:59,174 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:35:08 }
2024-01-23 23:18:04,068 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  108/26001], loss: 1.782, per_step_time: 2443ms, lr: 1.1153846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:04,069 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:34:23 }
2024-01-23 23:18:08,964 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  110/26001], loss: 1.471, per_step_time: 2443ms, lr: 1.1410256e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:08,964 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:34:32 }
2024-01-23 23:18:13,859 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  112/26001], loss: 0.804, per_step_time: 2443ms, lr: 1.1666666e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:13,859 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:34:27 }
2024-01-23 23:18:18,753 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  114/26001], loss: 1.224, per_step_time: 2443ms, lr: 1.1923076e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:18,753 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:34:06 }
2024-01-23 23:18:23,647 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  116/26001], loss: 1.084, per_step_time: 2443ms, lr: 1.21794865e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:23,648 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.4% |                                                  | 0.82 samples/s/p  17:33:59 }
2024-01-23 23:18:28,549 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  118/26001], loss: 0.936, per_step_time: 2447ms, lr: 1.2435897e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:28,550 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:35:42 }
2024-01-23 23:18:33,445 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  120/26001], loss: 1.404, per_step_time: 2444ms, lr: 1.2692308e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:33,446 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:34:16 }
2024-01-23 23:18:38,395 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  122/26001], loss: 1.257, per_step_time: 2470ms, lr: 1.2948718e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:38,396 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.81 samples/s/p  17:45:40 }
2024-01-23 23:18:43,289 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  124/26001], loss: 1.722, per_step_time: 2443ms, lr: 1.3205128e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:43,290 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:33:45 }
2024-01-23 23:18:48,177 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  126/26001], loss: 1.376, per_step_time: 2440ms, lr: 1.3461538e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:48,178 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:32:16 }
2024-01-23 23:18:53,065 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  128/26001], loss: 1.043, per_step_time: 2439ms, lr: 1.3717949e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:53,066 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:32:04 }
2024-01-23 23:18:57,954 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  130/26001], loss: 1.758, per_step_time: 2440ms, lr: 1.397436e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:18:57,955 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:32:18 }
2024-01-23 23:19:02,844 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  132/26001], loss: 1.186, per_step_time: 2441ms, lr: 1.423077e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:02,845 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:32:32 }
2024-01-23 23:19:07,732 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  134/26001], loss: 0.781, per_step_time: 2440ms, lr: 1.448718e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:07,733 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:32:00 }
2024-01-23 23:19:12,620 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  136/26001], loss: 1.132, per_step_time: 2440ms, lr: 1.474359e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:12,621 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:31:52 }
2024-01-23 23:19:17,508 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  138/26001], loss: 1.103, per_step_time: 2439ms, lr: 1.50000005e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:17,508 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:31:42 }
2024-01-23 23:19:22,394 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  140/26001], loss: 1.520, per_step_time: 2439ms, lr: 1.5256411e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:22,395 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:31:22 }
2024-01-23 23:19:27,286 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  142/26001], loss: 1.215, per_step_time: 2442ms, lr: 1.551282e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:27,287 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.5% |                                                  | 0.82 samples/s/p  17:32:31 }
2024-01-23 23:19:32,181 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  144/26001], loss: 1.293, per_step_time: 2443ms, lr: 1.576923e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:32,182 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:33:01 }
2024-01-23 23:19:37,074 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  146/26001], loss: 1.778, per_step_time: 2442ms, lr: 1.602564e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:37,075 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:32:31 }
2024-01-23 23:19:41,970 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  148/26001], loss: 1.155, per_step_time: 2443ms, lr: 1.628205e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:41,970 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:32:58 }
2024-01-23 23:19:46,870 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  150/26001], loss: 1.340, per_step_time: 2442ms, lr: 1.653846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:46,870 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:32:22 }
2024-01-23 23:19:51,765 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  152/26001], loss: 1.474, per_step_time: 2443ms, lr: 1.6794871e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:51,766 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:32:40 }
2024-01-23 23:19:56,660 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  154/26001], loss: 1.096, per_step_time: 2443ms, lr: 1.7051281e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:19:56,661 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:32:27 }
2024-01-23 23:20:01,554 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  156/26001], loss: 1.351, per_step_time: 2442ms, lr: 1.7307691e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:01,555 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:32:16 }
2024-01-23 23:20:06,447 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  158/26001], loss: 1.181, per_step_time: 2442ms, lr: 1.7564103e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:06,448 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:31:57 }
2024-01-23 23:20:11,358 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  160/26001], loss: 1.599, per_step_time: 2451ms, lr: 1.7820514e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:11,359 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:35:44 }
2024-01-23 23:20:16,276 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  162/26001], loss: 1.228, per_step_time: 2454ms, lr: 1.8076924e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:16,276 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.81 samples/s/p  17:37:10 }
2024-01-23 23:20:21,191 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  164/26001], loss: 1.238, per_step_time: 2453ms, lr: 1.8333334e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:21,192 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:36:32 }
2024-01-23 23:20:26,083 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  166/26001], loss: 1.444, per_step_time: 2442ms, lr: 1.8589744e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:26,084 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:31:34 }
2024-01-23 23:20:30,977 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  168/26001], loss: 1.994, per_step_time: 2442ms, lr: 1.8846154e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:30,978 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.6% |                                                  | 0.82 samples/s/p  17:31:44 }
2024-01-23 23:20:35,867 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  170/26001], loss: 1.589, per_step_time: 2441ms, lr: 1.9102565e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:35,868 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:30:56 }
2024-01-23 23:20:40,758 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  172/26001], loss: 1.327, per_step_time: 2441ms, lr: 1.9358975e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:40,759 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:31:02 }
2024-01-23 23:20:45,649 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  174/26001], loss: 1.033, per_step_time: 2441ms, lr: 1.9615385e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:45,649 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:30:45 }
2024-01-23 23:20:50,541 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  176/26001], loss: 1.156, per_step_time: 2442ms, lr: 1.9871795e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:50,542 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:31:10 }
2024-01-23 23:20:55,436 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  178/26001], loss: 1.475, per_step_time: 2443ms, lr: 2.0128206e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:20:55,437 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:31:35 }
2024-01-23 23:21:00,327 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  180/26001], loss: 1.174, per_step_time: 2441ms, lr: 2.0384616e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:00,328 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:30:42 }
2024-01-23 23:21:05,219 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  182/26001], loss: 1.247, per_step_time: 2442ms, lr: 2.0641026e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:05,220 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:30:53 }
2024-01-23 23:21:10,109 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  184/26001], loss: 1.735, per_step_time: 2441ms, lr: 2.0897436e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:10,110 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:30:20 }
2024-01-23 23:21:14,999 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  186/26001], loss: 1.063, per_step_time: 2440ms, lr: 2.1153846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:15,000 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:30:08 }
2024-01-23 23:21:19,893 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  188/26001], loss: 0.696, per_step_time: 2442ms, lr: 2.1410257e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:19,893 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:31:00 }
2024-01-23 23:21:24,788 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  190/26001], loss: 1.199, per_step_time: 2443ms, lr: 2.1666667e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:24,789 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:31:19 }
2024-01-23 23:21:29,683 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  192/26001], loss: 1.763, per_step_time: 2443ms, lr: 2.1923077e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:29,683 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:30:58 }
2024-01-23 23:21:34,574 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  194/26001], loss: 0.926, per_step_time: 2441ms, lr: 2.2179487e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:34,574 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.7% |                                                  | 0.82 samples/s/p  17:30:11 }
2024-01-23 23:21:39,466 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  196/26001], loss: 1.208, per_step_time: 2442ms, lr: 2.2435897e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:39,466 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:30:21 }
2024-01-23 23:21:44,357 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  198/26001], loss: 0.931, per_step_time: 2442ms, lr: 2.2692308e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:44,358 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:30:12 }
2024-01-23 23:21:49,247 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  200/26001], loss: 0.789, per_step_time: 2440ms, lr: 2.2948718e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:21:49,247 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:29:34 }
2024-01-23 23:21:49,248 - mindformers[mindformers/core/callback/callback.py:559] - INFO - ......Saving ckpt......
2024-01-23 23:27:12,917 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  202/26001], loss: 0.567, per_step_time: 2445ms, lr: 2.3205128e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:12,917 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:31:30 }
2024-01-23 23:27:17,803 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  204/26001], loss: 1.505, per_step_time: 2439ms, lr: 2.3461538e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:17,804 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:28:45 }
2024-01-23 23:27:22,693 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  206/26001], loss: 1.377, per_step_time: 2441ms, lr: 2.3717948e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:22,694 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:29:36 }
2024-01-23 23:27:27,578 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  208/26001], loss: 0.879, per_step_time: 2438ms, lr: 2.3974359e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:27,579 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:28:14 }
2024-01-23 23:27:32,465 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  210/26001], loss: 1.156, per_step_time: 2439ms, lr: 2.4230769e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:32,466 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:28:43 }
2024-01-23 23:27:37,351 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  212/26001], loss: 1.699, per_step_time: 2439ms, lr: 2.4487179e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:37,352 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:28:25 }
2024-01-23 23:27:42,238 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  214/26001], loss: 0.963, per_step_time: 2439ms, lr: 2.474359e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:42,238 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:28:18 }
2024-01-23 23:27:47,123 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  216/26001], loss: 0.522, per_step_time: 2439ms, lr: 2.5e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:47,124 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:28:10 }
2024-01-23 23:27:52,012 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  218/26001], loss: 1.287, per_step_time: 2440ms, lr: 2.525641e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:52,012 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:28:36 }
2024-01-23 23:27:56,894 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  220/26001], loss: 1.100, per_step_time: 2437ms, lr: 2.551282e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:27:56,894 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.8% |                                                  | 0.82 samples/s/p  17:27:13 }
2024-01-23 23:28:01,778 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  222/26001], loss: 1.327, per_step_time: 2438ms, lr: 2.576923e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:01,778 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:34 }
2024-01-23 23:28:06,661 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  224/26001], loss: 1.321, per_step_time: 2437ms, lr: 2.602564e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:06,661 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:13 }
2024-01-23 23:28:11,550 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  226/26001], loss: 1.601, per_step_time: 2440ms, lr: 2.628205e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:11,551 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:28:35 }
2024-01-23 23:28:16,438 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  228/26001], loss: 1.404, per_step_time: 2440ms, lr: 2.653846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:16,439 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:28:08 }
2024-01-23 23:28:21,325 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  230/26001], loss: 1.662, per_step_time: 2439ms, lr: 2.679487e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:21,326 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:48 }
2024-01-23 23:28:26,212 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  232/26001], loss: 1.376, per_step_time: 2439ms, lr: 2.7051281e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:26,213 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:45 }
2024-01-23 23:28:31,098 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  234/26001], loss: 0.829, per_step_time: 2439ms, lr: 2.7307691e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:31,098 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:26 }
2024-01-23 23:28:35,984 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  236/26001], loss: 0.553, per_step_time: 2439ms, lr: 2.7564101e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:35,985 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:31 }
2024-01-23 23:28:40,876 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  238/26001], loss: 1.883, per_step_time: 2441ms, lr: 2.7820512e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:40,876 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:28:26 }
2024-01-23 23:28:45,764 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  240/26001], loss: 1.463, per_step_time: 2440ms, lr: 2.8076922e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:45,764 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:42 }
2024-01-23 23:28:50,651 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  242/26001], loss: 1.068, per_step_time: 2440ms, lr: 2.8333332e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:50,652 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:32 }
2024-01-23 23:28:55,540 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  244/26001], loss: 1.496, per_step_time: 2440ms, lr: 2.8589742e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:28:55,540 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:27:37 }
2024-01-23 23:29:00,430 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  246/26001], loss: 1.532, per_step_time: 2441ms, lr: 2.8846152e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:00,431 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    0.9% |                                                  | 0.82 samples/s/p  17:28:01 }
2024-01-23 23:29:05,318 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  248/26001], loss: 1.545, per_step_time: 2440ms, lr: 2.9102563e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:05,319 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:27:20 }
2024-01-23 23:29:10,205 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  250/26001], loss: 0.873, per_step_time: 2439ms, lr: 2.9358973e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:10,205 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:50 }
2024-01-23 23:29:15,091 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  252/26001], loss: 0.934, per_step_time: 2439ms, lr: 2.9615383e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:15,092 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:54 }
2024-01-23 23:29:19,979 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  254/26001], loss: 1.357, per_step_time: 2439ms, lr: 2.9871793e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:19,980 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:58 }
2024-01-23 23:29:24,866 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  256/26001], loss: 1.134, per_step_time: 2439ms, lr: 3.0128203e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:24,867 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:49 }
2024-01-23 23:29:29,755 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  258/26001], loss: 1.469, per_step_time: 2440ms, lr: 3.0384614e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:29,755 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:27:00 }
2024-01-23 23:29:34,641 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  260/26001], loss: 1.384, per_step_time: 2439ms, lr: 3.0641026e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:34,642 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:34 }
2024-01-23 23:29:39,532 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  262/26001], loss: 1.269, per_step_time: 2440ms, lr: 3.0897434e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:39,532 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:27:08 }
2024-01-23 23:29:44,420 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  264/26001], loss: 1.089, per_step_time: 2440ms, lr: 3.1153846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:44,420 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:39 }
2024-01-23 23:29:49,307 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  266/26001], loss: 1.096, per_step_time: 2439ms, lr: 3.1410254e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:49,308 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:28 }
2024-01-23 23:29:54,193 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  268/26001], loss: 1.500, per_step_time: 2439ms, lr: 3.1666666e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:54,194 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:12 }
2024-01-23 23:29:59,081 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  270/26001], loss: 1.049, per_step_time: 2439ms, lr: 3.1923075e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:29:59,081 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:11 }
2024-01-23 23:30:03,968 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  272/26001], loss: 1.210, per_step_time: 2439ms, lr: 3.2179487e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:03,968 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.0% |                                                  | 0.82 samples/s/p  17:26:06 }
2024-01-23 23:30:08,858 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  274/26001], loss: 1.679, per_step_time: 2440ms, lr: 3.2435895e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:08,860 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:26:24 }
2024-01-23 23:30:13,756 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  276/26001], loss: 0.757, per_step_time: 2439ms, lr: 3.2692307e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:13,756 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:25:58 }
2024-01-23 23:30:18,646 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  278/26001], loss: 1.187, per_step_time: 2441ms, lr: 3.2948716e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:18,647 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:26:48 }
2024-01-23 23:30:23,545 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  280/26001], loss: 1.799, per_step_time: 2445ms, lr: 3.3205128e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:23,546 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:28:17 }
2024-01-23 23:30:28,446 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  282/26001], loss: 1.359, per_step_time: 2445ms, lr: 3.3461536e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:28,446 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:28:24 }
2024-01-23 23:30:33,349 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  284/26001], loss: 1.893, per_step_time: 2447ms, lr: 3.3717948e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:33,350 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:29:11 }
2024-01-23 23:30:38,249 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  286/26001], loss: 1.172, per_step_time: 2445ms, lr: 3.3974356e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:38,249 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:28:11 }
2024-01-23 23:30:43,146 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  288/26001], loss: 0.876, per_step_time: 2444ms, lr: 3.423077e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:43,146 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:27:34 }
2024-01-23 23:30:48,041 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  290/26001], loss: 0.894, per_step_time: 2443ms, lr: 3.4487177e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:48,042 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:27:11 }
2024-01-23 23:30:52,938 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  292/26001], loss: 1.244, per_step_time: 2444ms, lr: 3.474359e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:52,939 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:27:24 }
2024-01-23 23:30:57,833 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  294/26001], loss: 1.427, per_step_time: 2443ms, lr: 3.4999997e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:30:57,834 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:26:58 }
2024-01-23 23:31:02,728 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  296/26001], loss: 1.033, per_step_time: 2443ms, lr: 3.525641e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:31:02,729 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:26:50 }
2024-01-23 23:31:07,626 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  298/26001], loss: 1.472, per_step_time: 2444ms, lr: 3.5512818e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:31:07,626 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.1% |                                                  | 0.82 samples/s/p  17:27:17 }
2024-01-23 23:31:12,521 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  300/26001], loss: 1.087, per_step_time: 2443ms, lr: 3.576923e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:31:12,522 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:26:42 }
2024-01-23 23:31:12,522 - mindformers[mindformers/core/callback/callback.py:559] - INFO - ......Saving ckpt......
2024-01-23 23:36:24,488 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  302/26001], loss: 1.479, per_step_time: 2477ms, lr: 3.6025638e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:36:24,489 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.81 samples/s/p  17:41:13 }
2024-01-23 23:36:29,378 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  304/26001], loss: 0.885, per_step_time: 2441ms, lr: 3.628205e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:36:29,379 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:25:27 }
2024-01-23 23:36:34,273 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  306/26001], loss: 0.956, per_step_time: 2443ms, lr: 3.653846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:36:34,273 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:26:21 }
2024-01-23 23:36:39,169 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  308/26001], loss: 0.568, per_step_time: 2444ms, lr: 3.679487e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:36:39,169 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:26:37 }
2024-01-23 23:36:44,064 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  310/26001], loss: 1.786, per_step_time: 2443ms, lr: 3.705128e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:36:44,065 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:26:27 }
2024-01-23 23:36:48,956 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  312/26001], loss: 1.512, per_step_time: 2441ms, lr: 3.730769e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:36:48,957 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:25:31 }
2024-01-23 23:36:53,846 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  314/26001], loss: 1.674, per_step_time: 2441ms, lr: 3.75641e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:36:53,847 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:25:07 }
2024-01-23 23:36:58,738 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  316/26001], loss: 1.570, per_step_time: 2441ms, lr: 3.782051e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:36:58,739 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:25:22 }
2024-01-23 23:37:03,630 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  318/26001], loss: 0.377, per_step_time: 2441ms, lr: 3.807692e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:03,630 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:25:16 }
2024-01-23 23:37:08,523 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  320/26001], loss: 1.514, per_step_time: 2442ms, lr: 3.833333e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:08,524 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:25:33 }
2024-01-23 23:37:13,416 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  322/26001], loss: 1.162, per_step_time: 2442ms, lr: 3.858974e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:13,417 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:25:24 }
2024-01-23 23:37:18,306 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  324/26001], loss: 0.792, per_step_time: 2440ms, lr: 3.8846152e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:18,306 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.2% |                                                  | 0.82 samples/s/p  17:24:32 }
2024-01-23 23:37:23,196 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  326/26001], loss: 1.054, per_step_time: 2441ms, lr: 3.910256e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:23,196 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:36 }
2024-01-23 23:37:28,087 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  328/26001], loss: 0.906, per_step_time: 2441ms, lr: 3.9358973e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:28,088 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:48 }
2024-01-23 23:37:32,978 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  330/26001], loss: 1.424, per_step_time: 2441ms, lr: 3.961538e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:32,978 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:29 }
2024-01-23 23:37:37,870 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  332/26001], loss: 1.549, per_step_time: 2442ms, lr: 3.9871793e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:37,870 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:44 }
2024-01-23 23:37:42,760 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  334/26001], loss: 1.197, per_step_time: 2441ms, lr: 4.0128205e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:42,760 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:17 }
2024-01-23 23:37:47,652 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  336/26001], loss: 1.289, per_step_time: 2442ms, lr: 4.0384613e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:47,652 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:37 }
2024-01-23 23:37:52,542 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  338/26001], loss: 0.999, per_step_time: 2441ms, lr: 4.0641025e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:52,542 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:04 }
2024-01-23 23:37:57,434 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  340/26001], loss: 1.394, per_step_time: 2442ms, lr: 4.0897434e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:37:57,435 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:31 }
2024-01-23 23:38:02,325 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  342/26001], loss: 1.119, per_step_time: 2441ms, lr: 4.1153846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:02,326 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:13 }
2024-01-23 23:38:07,217 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  344/26001], loss: 1.163, per_step_time: 2441ms, lr: 4.1410254e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:07,218 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:11 }
2024-01-23 23:38:12,116 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  346/26001], loss: 0.797, per_step_time: 2440ms, lr: 4.1666666e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:12,117 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:23:37 }
2024-01-23 23:38:17,010 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  348/26001], loss: 1.504, per_step_time: 2442ms, lr: 4.1923075e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:17,010 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:24:21 }
2024-01-23 23:38:21,901 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  350/26001], loss: 1.351, per_step_time: 2441ms, lr: 4.2179487e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:21,902 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.3% |                                                  | 0.82 samples/s/p  17:23:34 }
2024-01-23 23:38:26,790 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  352/26001], loss: 0.681, per_step_time: 2440ms, lr: 4.2435895e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:26,791 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:23:28 }
2024-01-23 23:38:31,691 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  354/26001], loss: 1.292, per_step_time: 2446ms, lr: 4.2692307e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:31,692 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:25:53 }
2024-01-23 23:38:36,592 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  356/26001], loss: 1.210, per_step_time: 2444ms, lr: 4.2948715e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:36,593 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:24:44 }
2024-01-23 23:38:41,500 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  358/26001], loss: 1.558, per_step_time: 2450ms, lr: 4.3205127e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:41,500 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:27:07 }
2024-01-23 23:38:46,398 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  360/26001], loss: 1.702, per_step_time: 2445ms, lr: 4.3461536e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:46,399 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:25:17 }
2024-01-23 23:38:51,305 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  362/26001], loss: 1.436, per_step_time: 2450ms, lr: 4.3717948e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:51,305 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:26:55 }
2024-01-23 23:38:56,199 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  364/26001], loss: 1.472, per_step_time: 2443ms, lr: 4.3974356e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:38:56,199 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:24:07 }
2024-01-23 23:39:01,088 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  366/26001], loss: 1.048, per_step_time: 2440ms, lr: 4.423077e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:01,088 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:22:51 }
2024-01-23 23:39:05,975 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  368/26001], loss: 1.052, per_step_time: 2440ms, lr: 4.4487177e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:05,976 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:22:37 }
2024-01-23 23:39:10,861 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  370/26001], loss: 1.126, per_step_time: 2439ms, lr: 4.474359e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:10,862 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:22:06 }
2024-01-23 23:39:15,748 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  372/26001], loss: 0.791, per_step_time: 2440ms, lr: 4.4999997e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:15,749 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:22:24 }
2024-01-23 23:39:20,633 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  374/26001], loss: 1.630, per_step_time: 2438ms, lr: 4.525641e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:20,633 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:21:43 }
2024-01-23 23:39:25,525 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  376/26001], loss: 2.041, per_step_time: 2442ms, lr: 4.5512817e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:25,525 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.4% |                                                  | 0.82 samples/s/p  17:23:20 }
2024-01-23 23:39:30,411 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  378/26001], loss: 1.758, per_step_time: 2440ms, lr: 4.576923e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:30,412 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:22:02 }
2024-01-23 23:39:35,298 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  380/26001], loss: 1.337, per_step_time: 2440ms, lr: 4.6025638e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:35,298 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:57 }
2024-01-23 23:39:40,185 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  382/26001], loss: 0.699, per_step_time: 2440ms, lr: 4.628205e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:40,185 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:55 }
2024-01-23 23:39:45,075 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  384/26001], loss: 1.713, per_step_time: 2441ms, lr: 4.653846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:45,075 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:22:33 }
2024-01-23 23:39:49,962 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  386/26001], loss: 1.363, per_step_time: 2440ms, lr: 4.679487e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:49,963 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:49 }
2024-01-23 23:39:54,851 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  388/26001], loss: 1.688, per_step_time: 2440ms, lr: 4.705128e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:54,852 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:57 }
2024-01-23 23:39:59,737 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  390/26001], loss: 1.022, per_step_time: 2439ms, lr: 4.730769e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:39:59,738 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:21 }
2024-01-23 23:40:04,624 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  392/26001], loss: 1.788, per_step_time: 2440ms, lr: 4.75641e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:40:04,624 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:31 }
2024-01-23 23:40:09,511 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  394/26001], loss: 1.585, per_step_time: 2440ms, lr: 4.782051e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:40:09,511 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:28 }
2024-01-23 23:40:14,463 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  396/26001], loss: 1.235, per_step_time: 2441ms, lr: 4.807692e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:40:14,463 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:59 }
2024-01-23 23:40:19,347 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  398/26001], loss: 0.787, per_step_time: 2439ms, lr: 4.833333e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:40:19,348 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:20:45 }
2024-01-23 23:40:24,235 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  400/26001], loss: 1.089, per_step_time: 2441ms, lr: 4.858974e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:40:24,236 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:21:32 }
2024-01-23 23:40:24,236 - mindformers[mindformers/core/callback/callback.py:559] - INFO - ......Saving ckpt......
2024-01-23 23:45:54,723 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  402/26001], loss: 0.648, per_step_time: 2448ms, lr: 4.8846152e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:45:55,629 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.5% |                                                  | 0.82 samples/s/p  17:24:30 }
2024-01-23 23:46:00,521 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  404/26001], loss: 1.917, per_step_time: 2441ms, lr: 4.910256e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:00,522 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:21:45 }
2024-01-23 23:46:05,409 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  406/26001], loss: 1.023, per_step_time: 2440ms, lr: 4.9358972e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:05,410 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:20:55 }
2024-01-23 23:46:10,303 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  408/26001], loss: 0.764, per_step_time: 2442ms, lr: 4.961538e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:10,303 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:21:57 }
2024-01-23 23:46:15,191 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  410/26001], loss: 1.455, per_step_time: 2440ms, lr: 4.9871793e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:15,191 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:20:45 }
2024-01-23 23:46:20,087 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  412/26001], loss: 1.801, per_step_time: 2443ms, lr: 5.01282e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:20,088 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:22:15 }
2024-01-23 23:46:25,014 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  414/26001], loss: 0.777, per_step_time: 2456ms, lr: 5.0384617e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:25,014 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.81 samples/s/p  17:27:44 }
2024-01-23 23:46:29,917 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  416/26001], loss: 0.538, per_step_time: 2447ms, lr: 5.064102e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:29,917 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:23:37 }
2024-01-23 23:46:34,891 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  418/26001], loss: 1.437, per_step_time: 2483ms, lr: 5.0897437e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:34,891 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.81 samples/s/p  17:38:44 }
2024-01-23 23:46:39,785 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  420/26001], loss: 1.608, per_step_time: 2443ms, lr: 5.1153842e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:39,786 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:21:44 }
2024-01-23 23:46:44,676 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  422/26001], loss: 1.254, per_step_time: 2440ms, lr: 5.1410258e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:44,677 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:20:23 }
2024-01-23 23:46:49,564 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  424/26001], loss: 0.560, per_step_time: 2439ms, lr: 5.1666662e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:49,565 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:20:05 }
2024-01-23 23:46:54,446 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  426/26001], loss: 1.575, per_step_time: 2437ms, lr: 5.1923078e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:54,446 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:18:48 }
2024-01-23 23:46:59,328 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  428/26001], loss: 0.968, per_step_time: 2437ms, lr: 5.2179483e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:46:59,329 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.6% |                                                  | 0.82 samples/s/p  17:18:52 }
2024-01-23 23:47:04,215 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  430/26001], loss: 0.901, per_step_time: 2439ms, lr: 5.24359e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:04,216 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:19:43 }
2024-01-23 23:47:09,104 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  432/26001], loss: 1.507, per_step_time: 2440ms, lr: 5.2692303e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:09,105 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:19:59 }
2024-01-23 23:47:13,988 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  434/26001], loss: 1.423, per_step_time: 2438ms, lr: 5.294872e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:13,989 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:18:53 }
2024-01-23 23:47:18,873 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  436/26001], loss: 1.080, per_step_time: 2438ms, lr: 5.3205124e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:18,874 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:19:03 }
2024-01-23 23:47:23,822 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  438/26001], loss: 1.218, per_step_time: 2439ms, lr: 5.346154e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:23,823 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:19:27 }
2024-01-23 23:47:28,707 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  440/26001], loss: 1.873, per_step_time: 2438ms, lr: 5.3717944e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:28,707 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:18:49 }
2024-01-23 23:47:33,594 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  442/26001], loss: 1.818, per_step_time: 2439ms, lr: 5.397436e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:33,595 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:19:19 }
2024-01-23 23:47:38,485 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  444/26001], loss: 1.350, per_step_time: 2441ms, lr: 5.4230764e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:38,486 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:19:51 }
2024-01-23 23:47:43,370 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  446/26001], loss: 1.342, per_step_time: 2438ms, lr: 5.448718e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:43,370 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:18:31 }
2024-01-23 23:47:48,251 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  448/26001], loss: 1.457, per_step_time: 2436ms, lr: 5.4743585e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:48,252 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:17:47 }
2024-01-23 23:47:53,136 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  450/26001], loss: 1.303, per_step_time: 2438ms, lr: 5.5e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:53,136 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:18:27 }
2024-01-23 23:47:58,021 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  452/26001], loss: 1.250, per_step_time: 2438ms, lr: 5.525641e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:47:58,022 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:18:31 }
2024-01-23 23:48:02,907 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  454/26001], loss: 0.949, per_step_time: 2438ms, lr: 5.551282e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:02,908 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.7% |                                                  | 0.82 samples/s/p  17:18:28 }
2024-01-23 23:48:07,797 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  456/26001], loss: 1.395, per_step_time: 2440ms, lr: 5.576923e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:07,797 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:19:08 }
2024-01-23 23:48:12,683 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  458/26001], loss: 1.412, per_step_time: 2439ms, lr: 5.602564e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:12,683 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:18:23 }
2024-01-23 23:48:17,570 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  460/26001], loss: 0.642, per_step_time: 2439ms, lr: 5.628205e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:17,570 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:18:30 }
2024-01-23 23:48:22,457 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  462/26001], loss: 0.549, per_step_time: 2439ms, lr: 5.653846e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:22,457 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:18:22 }
2024-01-23 23:48:27,341 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  464/26001], loss: 1.102, per_step_time: 2438ms, lr: 5.679487e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:27,342 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:17:47 }
2024-01-23 23:48:32,224 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  466/26001], loss: 1.454, per_step_time: 2437ms, lr: 5.7051282e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:32,224 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:17:23 }
2024-01-23 23:48:37,109 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  468/26001], loss: 0.345, per_step_time: 2438ms, lr: 5.730769e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:37,109 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:17:45 }
2024-01-23 23:48:41,997 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  470/26001], loss: 1.644, per_step_time: 2439ms, lr: 5.7564102e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:41,997 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:18:14 }
2024-01-23 23:48:46,881 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  472/26001], loss: 1.362, per_step_time: 2438ms, lr: 5.782051e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:46,881 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:17:24 }
2024-01-23 23:48:51,767 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  474/26001], loss: 1.709, per_step_time: 2439ms, lr: 5.8076923e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:51,768 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:17:52 }
2024-01-23 23:48:56,658 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  476/26001], loss: 1.549, per_step_time: 2441ms, lr: 5.833333e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:48:56,659 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:18:27 }
2024-01-23 23:49:01,553 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  478/26001], loss: 1.297, per_step_time: 2438ms, lr: 5.8589743e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:01,554 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:17:29 }
2024-01-23 23:49:06,443 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  480/26001], loss: 1.292, per_step_time: 2440ms, lr: 5.884615e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:06,443 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.8% |                                                  | 0.82 samples/s/p  17:18:05 }
2024-01-23 23:49:11,332 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  482/26001], loss: 1.497, per_step_time: 2440ms, lr: 5.9102564e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:11,333 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:17:50 }
2024-01-23 23:49:16,222 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  484/26001], loss: 1.526, per_step_time: 2440ms, lr: 5.9358972e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:16,223 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:17:53 }
2024-01-23 23:49:21,115 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  486/26001], loss: 1.991, per_step_time: 2442ms, lr: 5.9615384e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:21,115 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:18:31 }
2024-01-23 23:49:26,012 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  488/26001], loss: 1.290, per_step_time: 2444ms, lr: 5.9871792e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:26,012 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:19:18 }
2024-01-23 23:49:30,908 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  490/26001], loss: 0.963, per_step_time: 2444ms, lr: 6.0128204e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:30,909 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:19:15 }
2024-01-23 23:49:35,802 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  492/26001], loss: 0.644, per_step_time: 2442ms, lr: 6.0384613e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:35,803 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:18:28 }
2024-01-23 23:49:40,700 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  494/26001], loss: 1.590, per_step_time: 2444ms, lr: 6.0641025e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:40,700 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:19:17 }
2024-01-23 23:49:45,591 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  496/26001], loss: 1.448, per_step_time: 2441ms, lr: 6.0897433e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:45,591 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:17:49 }
2024-01-23 23:49:50,478 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  498/26001], loss: 1.156, per_step_time: 2439ms, lr: 6.115385e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:50,479 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:17:03 }
2024-01-23 23:49:55,368 - mindformers[mindformers/core/callback/callback.py:313] - INFO - { Epoch:[  1/  1], step:[  500/26001], loss: 0.969, per_step_time: 2440ms, lr: 6.1410254e-05, overflow cond: False, loss_scale: 4096.0
2024-01-23 23:49:55,369 - mindformers[mindformers/core/callback/callback.py:323] - INFO -    1.9% |                                                  | 0.82 samples/s/p  17:17:20 }
2024-01-23 23:49:55,369 - mindformers[mindformers/core/callback/callback.py:559] - INFO - ......Saving ckpt......
