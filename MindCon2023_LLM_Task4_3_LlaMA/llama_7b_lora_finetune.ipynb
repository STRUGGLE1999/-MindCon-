{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e48e1d6-5882-4ff5-836e-9aeff2b90b99",
   "metadata": {},
   "source": [
    "## 环境要求\n",
    "硬件：Ascend 910\n",
    "\n",
    "MindSpore：2.2.0\n",
    "\n",
    "cann7.0rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea053a-f87c-4064-997c-1a12cb48178b",
   "metadata": {},
   "source": [
    "## mindformers安装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2a7d7c-e2d9-404c-b354-852d93d8111c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mindformers'...\n",
      "remote: Enumerating objects: 25530, done.\u001b[K\n",
      "remote: Counting objects: 100% (4223/4223), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2231/2231), done.\u001b[K\n",
      "remote: Total 25530 (delta 2993), reused 2863 (delta 1975), pack-reused 21307\u001b[K\n",
      "Receiving objects: 100% (25530/25530), 39.66 MiB | 4.54 MiB/s, done.\n",
      "Resolving deltas: 100% (18585/18585), done.\n"
     ]
    }
   ],
   "source": [
    "#  下载链接\n",
    "!git clone -b dev https://gitee.com/mindspore/mindformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846eee0b-b48c-461c-8672-37e3f815f196",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- MindFormers: build start ----------------\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build/lib/mindformers\n",
      "copying mindformers/mindformer_book.py -> build/lib/mindformers\n",
      "copying mindformers/version_control.py -> build/lib/mindformers\n",
      "copying mindformers/__init__.py -> build/lib/mindformers\n",
      "copying mindformers/auto_class.py -> build/lib/mindformers\n",
      "creating build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/mask_language_model_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/img_cls_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/labels.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/contrastive_language_image_pretrain_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/text_classification_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/translation_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/reward_model_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/keyword_gen_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/mim_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/zero_shot_image_classification_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/token_classification_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/utils.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/build_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/causal_language_model_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/__init__.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/base_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/multi_turn_dataset.py -> build/lib/mindformers/dataset\n",
      "copying mindformers/dataset/question_answering_dataset.py -> build/lib/mindformers/dataset\n",
      "creating build/lib/mindformers/modules\n",
      "copying mindformers/modules/kvcache_mgr.py -> build/lib/mindformers/modules\n",
      "copying mindformers/modules/local_block_sparse_attention.py -> build/lib/mindformers/modules\n",
      "copying mindformers/modules/layers.py -> build/lib/mindformers/modules\n",
      "copying mindformers/modules/__init__.py -> build/lib/mindformers/modules\n",
      "copying mindformers/modules/activation.py -> build/lib/mindformers/modules\n",
      "creating build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/zero_shot_image_classification_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/image_classification_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/base_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/translation_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/fill_mask_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/token_classification_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/text_classification_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/text_generation_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/masked_image_modeling_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/__init__.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/build_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/question_answering_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/image_to_text_generation_pipeline.py -> build/lib/mindformers/pipeline\n",
      "copying mindformers/pipeline/segment_anything_pipeline.py -> build/lib/mindformers/pipeline\n",
      "creating build/lib/mindformers/tools\n",
      "copying mindformers/tools/logger.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/download_tools_multithread.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/export.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/check_rules.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/merge_hccl.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/hccl_tools.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/moe_token_distribution_tools.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/utils.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/transform_ckpt.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/image_tools.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/download_tools.py -> build/lib/mindformers/tools\n",
      "copying mindformers/tools/__init__.py -> build/lib/mindformers/tools\n",
      "creating build/lib/mindformers/core\n",
      "copying mindformers/core/parallel_config.py -> build/lib/mindformers/core\n",
      "copying mindformers/core/clip_grad.py -> build/lib/mindformers/core\n",
      "copying mindformers/core/__init__.py -> build/lib/mindformers/core\n",
      "creating build/lib/mindformers/wrapper\n",
      "copying mindformers/wrapper/adaptive_loss_scale.py -> build/lib/mindformers/wrapper\n",
      "copying mindformers/wrapper/build_wrapper.py -> build/lib/mindformers/wrapper\n",
      "copying mindformers/wrapper/__init__.py -> build/lib/mindformers/wrapper\n",
      "copying mindformers/wrapper/wrapper.py -> build/lib/mindformers/wrapper\n",
      "creating build/lib/mindformers/inference\n",
      "copying mindformers/inference/infer_task.py -> build/lib/mindformers/inference\n",
      "copying mindformers/inference/postprocess_sampler.py -> build/lib/mindformers/inference\n",
      "copying mindformers/inference/context.py -> build/lib/mindformers/inference\n",
      "copying mindformers/inference/__init__.py -> build/lib/mindformers/inference\n",
      "copying mindformers/inference/pipeline.py -> build/lib/mindformers/inference\n",
      "copying mindformers/inference/infer_config.py -> build/lib/mindformers/inference\n",
      "creating build/lib/mindformers/generation\n",
      "copying mindformers/generation/beam_search.py -> build/lib/mindformers/generation\n",
      "copying mindformers/generation/utils.py -> build/lib/mindformers/generation\n",
      "copying mindformers/generation/text_generator.py -> build/lib/mindformers/generation\n",
      "copying mindformers/generation/streamers.py -> build/lib/mindformers/generation\n",
      "copying mindformers/generation/__init__.py -> build/lib/mindformers/generation\n",
      "copying mindformers/generation/generation_config.py -> build/lib/mindformers/generation\n",
      "copying mindformers/generation/logits_process.py -> build/lib/mindformers/generation\n",
      "creating build/lib/mindformers/models\n",
      "copying mindformers/models/base_tokenizer.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/build_processor.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/base_config.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/utils.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/base_model.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/build_config.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/convert_slow_tokenizer.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/base_processor.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/build_tokenizer.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/base_fast_tokenizer.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/__init__.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/sentencepiece_model_pb2.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/build_model.py -> build/lib/mindformers/models\n",
      "copying mindformers/models/sentencepiece_model_pb2_new.py -> build/lib/mindformers/models\n",
      "creating build/lib/mindformers/pet\n",
      "copying mindformers/pet/pet_config.py -> build/lib/mindformers/pet\n",
      "copying mindformers/pet/constants.py -> build/lib/mindformers/pet\n",
      "copying mindformers/pet/utils.py -> build/lib/mindformers/pet\n",
      "copying mindformers/pet/pet_model.py -> build/lib/mindformers/pet\n",
      "copying mindformers/pet/__init__.py -> build/lib/mindformers/pet\n",
      "creating build/lib/mindformers/trainer\n",
      "copying mindformers/trainer/training_args.py -> build/lib/mindformers/trainer\n",
      "copying mindformers/trainer/trainer.py -> build/lib/mindformers/trainer\n",
      "copying mindformers/trainer/config_args.py -> build/lib/mindformers/trainer\n",
      "copying mindformers/trainer/base_trainer.py -> build/lib/mindformers/trainer\n",
      "copying mindformers/trainer/utils.py -> build/lib/mindformers/trainer\n",
      "copying mindformers/trainer/optimizer_grouped_parameters.py -> build/lib/mindformers/trainer\n",
      "copying mindformers/trainer/__init__.py -> build/lib/mindformers/trainer\n",
      "copying mindformers/trainer/build_trainer.py -> build/lib/mindformers/trainer\n",
      "creating build/lib/mindformers/dataset/transforms\n",
      "copying mindformers/dataset/transforms/random_erasing.py -> build/lib/mindformers/dataset/transforms\n",
      "copying mindformers/dataset/transforms/auto_augment.py -> build/lib/mindformers/dataset/transforms\n",
      "copying mindformers/dataset/transforms/mixup.py -> build/lib/mindformers/dataset/transforms\n",
      "copying mindformers/dataset/transforms/text_transforms.py -> build/lib/mindformers/dataset/transforms\n",
      "copying mindformers/dataset/transforms/vision_transforms.py -> build/lib/mindformers/dataset/transforms\n",
      "copying mindformers/dataset/transforms/build_transforms.py -> build/lib/mindformers/dataset/transforms\n",
      "copying mindformers/dataset/transforms/__init__.py -> build/lib/mindformers/dataset/transforms\n",
      "creating build/lib/mindformers/dataset/mask\n",
      "copying mindformers/dataset/mask/build_mask.py -> build/lib/mindformers/dataset/mask\n",
      "copying mindformers/dataset/mask/__init__.py -> build/lib/mindformers/dataset/mask\n",
      "copying mindformers/dataset/mask/vision_mask.py -> build/lib/mindformers/dataset/mask\n",
      "creating build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/wmt16_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/toolaplaca_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/cluener_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/multi_source_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/sft_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/multi_image_cap_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/sft_map_functions.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/training_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/cifar100_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/datareaders.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/__init__.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/adgen_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/build_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/squad_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "copying mindformers/dataset/dataloader/flickr8k_dataloader.py -> build/lib/mindformers/dataset/dataloader\n",
      "creating build/lib/mindformers/dataset/sampler\n",
      "copying mindformers/dataset/sampler/build_sampler.py -> build/lib/mindformers/dataset/sampler\n",
      "copying mindformers/dataset/sampler/__init__.py -> build/lib/mindformers/dataset/sampler\n",
      "creating build/lib/mindformers/modules/transformer\n",
      "copying mindformers/modules/transformer/transformer.py -> build/lib/mindformers/modules/transformer\n",
      "copying mindformers/modules/transformer/__init__.py -> build/lib/mindformers/modules/transformer\n",
      "copying mindformers/modules/transformer/op_parallel_config.py -> build/lib/mindformers/modules/transformer\n",
      "copying mindformers/modules/transformer/moe.py -> build/lib/mindformers/modules/transformer\n",
      "creating build/lib/mindformers/tools/register\n",
      "copying mindformers/tools/register/register.py -> build/lib/mindformers/tools/register\n",
      "copying mindformers/tools/register/__init__.py -> build/lib/mindformers/tools/register\n",
      "copying mindformers/tools/register/config.py -> build/lib/mindformers/tools/register\n",
      "creating build/lib/mindformers/tools/cloud_adapter\n",
      "copying mindformers/tools/cloud_adapter/cloud_adapter.py -> build/lib/mindformers/tools/cloud_adapter\n",
      "copying mindformers/tools/cloud_adapter/cloud_monitor.py -> build/lib/mindformers/tools/cloud_adapter\n",
      "copying mindformers/tools/cloud_adapter/__init__.py -> build/lib/mindformers/tools/cloud_adapter\n",
      "creating build/lib/mindformers/core/optim\n",
      "copying mindformers/core/optim/build_optim.py -> build/lib/mindformers/core/optim\n",
      "copying mindformers/core/optim/optim.py -> build/lib/mindformers/core/optim\n",
      "copying mindformers/core/optim/came.py -> build/lib/mindformers/core/optim\n",
      "copying mindformers/core/optim/__init__.py -> build/lib/mindformers/core/optim\n",
      "creating build/lib/mindformers/core/context\n",
      "copying mindformers/core/context/build_context.py -> build/lib/mindformers/core/context\n",
      "copying mindformers/core/context/__init__.py -> build/lib/mindformers/core/context\n",
      "creating build/lib/mindformers/core/loss\n",
      "copying mindformers/core/loss/loss.py -> build/lib/mindformers/core/loss\n",
      "copying mindformers/core/loss/__init__.py -> build/lib/mindformers/core/loss\n",
      "copying mindformers/core/loss/build_loss.py -> build/lib/mindformers/core/loss\n",
      "creating build/lib/mindformers/core/callback\n",
      "copying mindformers/core/callback/build_callback.py -> build/lib/mindformers/core/callback\n",
      "copying mindformers/core/callback/callback.py -> build/lib/mindformers/core/callback\n",
      "copying mindformers/core/callback/__init__.py -> build/lib/mindformers/core/callback\n",
      "creating build/lib/mindformers/core/metric\n",
      "copying mindformers/core/metric/build_metric.py -> build/lib/mindformers/core/metric\n",
      "copying mindformers/core/metric/utils.py -> build/lib/mindformers/core/metric\n",
      "copying mindformers/core/metric/metric.py -> build/lib/mindformers/core/metric\n",
      "copying mindformers/core/metric/__init__.py -> build/lib/mindformers/core/metric\n",
      "creating build/lib/mindformers/core/lr\n",
      "copying mindformers/core/lr/__init__.py -> build/lib/mindformers/core/lr\n",
      "copying mindformers/core/lr/build_lr.py -> build/lib/mindformers/core/lr\n",
      "copying mindformers/core/lr/lr_schedule.py -> build/lib/mindformers/core/lr\n",
      "creating build/lib/mindformers/inference/infers\n",
      "copying mindformers/inference/infers/text_generator_infer.py -> build/lib/mindformers/inference/infers\n",
      "copying mindformers/inference/infers/__init__.py -> build/lib/mindformers/inference/infers\n",
      "copying mindformers/inference/infers/base_infer.py -> build/lib/mindformers/inference/infers\n",
      "creating build/lib/mindformers/models/gpt2\n",
      "copying mindformers/models/gpt2/gpt2.py -> build/lib/mindformers/models/gpt2\n",
      "copying mindformers/models/gpt2/gpt_modules.py -> build/lib/mindformers/models/gpt2\n",
      "copying mindformers/models/gpt2/gpt2_config.py -> build/lib/mindformers/models/gpt2\n",
      "copying mindformers/models/gpt2/gpt2_tokenizer_fast.py -> build/lib/mindformers/models/gpt2\n",
      "copying mindformers/models/gpt2/gpt2_tokenizer.py -> build/lib/mindformers/models/gpt2\n",
      "copying mindformers/models/gpt2/__init__.py -> build/lib/mindformers/models/gpt2\n",
      "copying mindformers/models/gpt2/convert_weight.py -> build/lib/mindformers/models/gpt2\n",
      "copying mindformers/models/gpt2/gpt2_processor.py -> build/lib/mindformers/models/gpt2\n",
      "creating build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/llama_config.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/llama.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/llama_layer.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/llama_tokenizer_fast.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/llama_interleave.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/llama_tokenizer.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/__init__.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/convert_weight.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/llama_transformer.py -> build/lib/mindformers/models/llama\n",
      "copying mindformers/models/llama/llama_processor.py -> build/lib/mindformers/models/llama\n",
      "creating build/lib/mindformers/models/clip\n",
      "copying mindformers/models/clip/clip_modules.py -> build/lib/mindformers/models/clip\n",
      "copying mindformers/models/clip/clip.py -> build/lib/mindformers/models/clip\n",
      "copying mindformers/models/clip/clip_tokenizer.py -> build/lib/mindformers/models/clip\n",
      "copying mindformers/models/clip/__init__.py -> build/lib/mindformers/models/clip\n",
      "copying mindformers/models/clip/clip_processor.py -> build/lib/mindformers/models/clip\n",
      "copying mindformers/models/clip/convert_weight.py -> build/lib/mindformers/models/clip\n",
      "copying mindformers/models/clip/clip_config.py -> build/lib/mindformers/models/clip\n",
      "creating build/lib/mindformers/models/glm2\n",
      "copying mindformers/models/glm2/glm2_tokenizer.py -> build/lib/mindformers/models/glm2\n",
      "copying mindformers/models/glm2/glm2_modules.py -> build/lib/mindformers/models/glm2\n",
      "copying mindformers/models/glm2/glm2_transformer.py -> build/lib/mindformers/models/glm2\n",
      "copying mindformers/models/glm2/glm2_config.py -> build/lib/mindformers/models/glm2\n",
      "copying mindformers/models/glm2/__init__.py -> build/lib/mindformers/models/glm2\n",
      "copying mindformers/models/glm2/glm2.py -> build/lib/mindformers/models/glm2\n",
      "creating build/lib/mindformers/models/t5\n",
      "copying mindformers/models/t5/t5_processor.py -> build/lib/mindformers/models/t5\n",
      "copying mindformers/models/t5/mt5.py -> build/lib/mindformers/models/t5\n",
      "copying mindformers/models/t5/t5.py -> build/lib/mindformers/models/t5\n",
      "copying mindformers/models/t5/t5_tokenizer.py -> build/lib/mindformers/models/t5\n",
      "copying mindformers/models/t5/t5_config.py -> build/lib/mindformers/models/t5\n",
      "copying mindformers/models/t5/__init__.py -> build/lib/mindformers/models/t5\n",
      "copying mindformers/models/t5/t5_tokenizer_fast.py -> build/lib/mindformers/models/t5\n",
      "copying mindformers/models/t5/convert_weight.py -> build/lib/mindformers/models/t5\n",
      "creating build/lib/mindformers/models/swin\n",
      "copying mindformers/models/swin/swin_modules.py -> build/lib/mindformers/models/swin\n",
      "copying mindformers/models/swin/swin.py -> build/lib/mindformers/models/swin\n",
      "copying mindformers/models/swin/swin_processor.py -> build/lib/mindformers/models/swin\n",
      "copying mindformers/models/swin/swin_config.py -> build/lib/mindformers/models/swin\n",
      "copying mindformers/models/swin/__init__.py -> build/lib/mindformers/models/swin\n",
      "copying mindformers/models/swin/convert_weight.py -> build/lib/mindformers/models/swin\n",
      "creating build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/blip2_itm_evaluator.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/qformer_config.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/blip2_processor.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/blip2_config.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/qformer.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/layers.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/blip2_vit.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/blip2_llama.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/blip2.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/blip2_qformer.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/__init__.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/convert_weight.py -> build/lib/mindformers/models/blip2\n",
      "copying mindformers/models/blip2/blip2_llm.py -> build/lib/mindformers/models/blip2\n",
      "creating build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/bloom.py -> build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/bloom_reward.py -> build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/layers.py -> build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/bloom_processor.py -> build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/bloom_tokenizer_fast.py -> build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/__init__.py -> build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/convert_weight.py -> build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/bloom_config.py -> build/lib/mindformers/models/bloom\n",
      "copying mindformers/models/bloom/bloom_tokenizer.py -> build/lib/mindformers/models/bloom\n",
      "creating build/lib/mindformers/models/bert\n",
      "copying mindformers/models/bert/bert_processor.py -> build/lib/mindformers/models/bert\n",
      "copying mindformers/models/bert/bert.py -> build/lib/mindformers/models/bert\n",
      "copying mindformers/models/bert/bert_tokenizer_fast.py -> build/lib/mindformers/models/bert\n",
      "copying mindformers/models/bert/bert_tokenizer.py -> build/lib/mindformers/models/bert\n",
      "copying mindformers/models/bert/bert_config.py -> build/lib/mindformers/models/bert\n",
      "copying mindformers/models/bert/__init__.py -> build/lib/mindformers/models/bert\n",
      "copying mindformers/models/bert/convert_weight.py -> build/lib/mindformers/models/bert\n",
      "creating build/lib/mindformers/models/glm3\n",
      "copying mindformers/models/glm3/__init__.py -> build/lib/mindformers/models/glm3\n",
      "copying mindformers/models/glm3/glm3_tokenizer.py -> build/lib/mindformers/models/glm3\n",
      "creating build/lib/mindformers/models/glm\n",
      "copying mindformers/models/glm/glm_processor.py -> build/lib/mindformers/models/glm\n",
      "copying mindformers/models/glm/chatglm_6b_tokenizer.py -> build/lib/mindformers/models/glm\n",
      "copying mindformers/models/glm/glm_config.py -> build/lib/mindformers/models/glm\n",
      "copying mindformers/models/glm/attention.py -> build/lib/mindformers/models/glm\n",
      "copying mindformers/models/glm/layers.py -> build/lib/mindformers/models/glm\n",
      "copying mindformers/models/glm/glm.py -> build/lib/mindformers/models/glm\n",
      "copying mindformers/models/glm/__init__.py -> build/lib/mindformers/models/glm\n",
      "copying mindformers/models/glm/convert_weight.py -> build/lib/mindformers/models/glm\n",
      "creating build/lib/mindformers/models/mae\n",
      "copying mindformers/models/mae/mae.py -> build/lib/mindformers/models/mae\n",
      "copying mindformers/models/mae/mae_config.py -> build/lib/mindformers/models/mae\n",
      "copying mindformers/models/mae/mae_processor.py -> build/lib/mindformers/models/mae\n",
      "copying mindformers/models/mae/mae_modules.py -> build/lib/mindformers/models/mae\n",
      "copying mindformers/models/mae/__init__.py -> build/lib/mindformers/models/mae\n",
      "copying mindformers/models/mae/convert_weight.py -> build/lib/mindformers/models/mae\n",
      "creating build/lib/mindformers/models/pangualpha\n",
      "copying mindformers/models/pangualpha/pangualpha_processor.py -> build/lib/mindformers/models/pangualpha\n",
      "copying mindformers/models/pangualpha/pangualpha_config.py -> build/lib/mindformers/models/pangualpha\n",
      "copying mindformers/models/pangualpha/pangualpha.py -> build/lib/mindformers/models/pangualpha\n",
      "copying mindformers/models/pangualpha/pangualpha_tokenizer.py -> build/lib/mindformers/models/pangualpha\n",
      "copying mindformers/models/pangualpha/__init__.py -> build/lib/mindformers/models/pangualpha\n",
      "copying mindformers/models/pangualpha/convert_weight.py -> build/lib/mindformers/models/pangualpha\n",
      "creating build/lib/mindformers/models/vit\n",
      "copying mindformers/models/vit/vit_config.py -> build/lib/mindformers/models/vit\n",
      "copying mindformers/models/vit/vit_processor.py -> build/lib/mindformers/models/vit\n",
      "copying mindformers/models/vit/__init__.py -> build/lib/mindformers/models/vit\n",
      "copying mindformers/models/vit/vit_modules.py -> build/lib/mindformers/models/vit\n",
      "copying mindformers/models/vit/vit.py -> build/lib/mindformers/models/vit\n",
      "copying mindformers/models/vit/convert_weight.py -> build/lib/mindformers/models/vit\n",
      "creating build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/sam_config.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/conver_weight.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/sam_image_encoder.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/sam.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/sam_layers.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/sam_processor.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/sam_utils.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/__init__.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/sam_prompt_encoder.py -> build/lib/mindformers/models/sam\n",
      "copying mindformers/models/sam/sam_mask_decoder.py -> build/lib/mindformers/models/sam\n",
      "creating build/lib/mindformers/pet/tuners\n",
      "copying mindformers/pet/tuners/prefix_tuning_adapter.py -> build/lib/mindformers/pet/tuners\n",
      "copying mindformers/pet/tuners/ptuning2_adapter.py -> build/lib/mindformers/pet/tuners\n",
      "copying mindformers/pet/tuners/lora_adapter.py -> build/lib/mindformers/pet/tuners\n",
      "copying mindformers/pet/tuners/ada_adapter.py -> build/lib/mindformers/pet/tuners\n",
      "copying mindformers/pet/tuners/adalora_adapter.py -> build/lib/mindformers/pet/tuners\n",
      "copying mindformers/pet/tuners/__init__.py -> build/lib/mindformers/pet/tuners\n",
      "copying mindformers/pet/tuners/pet_adapter.py -> build/lib/mindformers/pet/tuners\n",
      "creating build/lib/mindformers/pet/models\n",
      "copying mindformers/pet/models/lora.py -> build/lib/mindformers/pet/models\n",
      "copying mindformers/pet/models/__init__.py -> build/lib/mindformers/pet/models\n",
      "creating build/lib/mindformers/trainer/causal_language_modeling\n",
      "copying mindformers/trainer/causal_language_modeling/causal_language_modeling.py -> build/lib/mindformers/trainer/causal_language_modeling\n",
      "copying mindformers/trainer/causal_language_modeling/__init__.py -> build/lib/mindformers/trainer/causal_language_modeling\n",
      "creating build/lib/mindformers/trainer/image_to_text_retrieval\n",
      "copying mindformers/trainer/image_to_text_retrieval/eval_utils.py -> build/lib/mindformers/trainer/image_to_text_retrieval\n",
      "copying mindformers/trainer/image_to_text_retrieval/__init__.py -> build/lib/mindformers/trainer/image_to_text_retrieval\n",
      "copying mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py -> build/lib/mindformers/trainer/image_to_text_retrieval\n",
      "creating build/lib/mindformers/trainer/token_classification\n",
      "copying mindformers/trainer/token_classification/token_classification.py -> build/lib/mindformers/trainer/token_classification\n",
      "copying mindformers/trainer/token_classification/__init__.py -> build/lib/mindformers/trainer/token_classification\n",
      "creating build/lib/mindformers/trainer/masked_language_modeling\n",
      "copying mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py -> build/lib/mindformers/trainer/masked_language_modeling\n",
      "copying mindformers/trainer/masked_language_modeling/__init__.py -> build/lib/mindformers/trainer/masked_language_modeling\n",
      "creating build/lib/mindformers/trainer/masked_image_modeling\n",
      "copying mindformers/trainer/masked_image_modeling/group_mim_parameters.py -> build/lib/mindformers/trainer/masked_image_modeling\n",
      "copying mindformers/trainer/masked_image_modeling/__init__.py -> build/lib/mindformers/trainer/masked_image_modeling\n",
      "copying mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py -> build/lib/mindformers/trainer/masked_image_modeling\n",
      "creating build/lib/mindformers/trainer/text_classfication\n",
      "copying mindformers/trainer/text_classfication/text_classification.py -> build/lib/mindformers/trainer/text_classfication\n",
      "copying mindformers/trainer/text_classfication/__init__.py -> build/lib/mindformers/trainer/text_classfication\n",
      "creating build/lib/mindformers/trainer/question_answering\n",
      "copying mindformers/trainer/question_answering/question_answering.py -> build/lib/mindformers/trainer/question_answering\n",
      "copying mindformers/trainer/question_answering/__init__.py -> build/lib/mindformers/trainer/question_answering\n",
      "creating build/lib/mindformers/trainer/translation\n",
      "copying mindformers/trainer/translation/translation_finetune.py -> build/lib/mindformers/trainer/translation\n",
      "copying mindformers/trainer/translation/__init__.py -> build/lib/mindformers/trainer/translation\n",
      "creating build/lib/mindformers/trainer/image_to_text_generation\n",
      "copying mindformers/trainer/image_to_text_generation/image_to_text_generation.py -> build/lib/mindformers/trainer/image_to_text_generation\n",
      "copying mindformers/trainer/image_to_text_generation/__init__.py -> build/lib/mindformers/trainer/image_to_text_generation\n",
      "creating build/lib/mindformers/trainer/contrastive_language_image_pretrain\n",
      "copying mindformers/trainer/contrastive_language_image_pretrain/__init__.py -> build/lib/mindformers/trainer/contrastive_language_image_pretrain\n",
      "copying mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py -> build/lib/mindformers/trainer/contrastive_language_image_pretrain\n",
      "creating build/lib/mindformers/trainer/general_task_trainer\n",
      "copying mindformers/trainer/general_task_trainer/general_task_trainer.py -> build/lib/mindformers/trainer/general_task_trainer\n",
      "copying mindformers/trainer/general_task_trainer/__init__.py -> build/lib/mindformers/trainer/general_task_trainer\n",
      "creating build/lib/mindformers/trainer/image_classification\n",
      "copying mindformers/trainer/image_classification/group_ic_params.py -> build/lib/mindformers/trainer/image_classification\n",
      "copying mindformers/trainer/image_classification/__init__.py -> build/lib/mindformers/trainer/image_classification\n",
      "copying mindformers/trainer/image_classification/zero_shot_image_classification.py -> build/lib/mindformers/trainer/image_classification\n",
      "copying mindformers/trainer/image_classification/image_classification.py -> build/lib/mindformers/trainer/image_classification\n",
      "running egg_info\n",
      "creating mindformers.egg-info\n",
      "writing mindformers.egg-info/PKG-INFO\n",
      "writing dependency_links to mindformers.egg-info/dependency_links.txt\n",
      "writing requirements to mindformers.egg-info/requires.txt\n",
      "writing top-level names to mindformers.egg-info/top_level.txt\n",
      "writing manifest file 'mindformers.egg-info/SOURCES.txt'\n",
      "reading manifest file 'mindformers.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'mindformers.egg-info/SOURCES.txt'\n",
      "copying mindformers/.commit_id -> build/lib/mindformers\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "installing to build/bdist.linux-aarch64/wheel\n",
      "running install\n",
      "running install_lib\n",
      "creating build/bdist.linux-aarch64/wheel\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/mask_language_model_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/img_cls_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/labels.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/contrastive_language_image_pretrain_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/text_classification_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/translation_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/reward_model_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/keyword_gen_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/mim_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/dataset/transforms\n",
      "copying build/lib/mindformers/dataset/transforms/random_erasing.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/transforms\n",
      "copying build/lib/mindformers/dataset/transforms/auto_augment.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/transforms\n",
      "copying build/lib/mindformers/dataset/transforms/mixup.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/transforms\n",
      "copying build/lib/mindformers/dataset/transforms/text_transforms.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/transforms\n",
      "copying build/lib/mindformers/dataset/transforms/vision_transforms.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/transforms\n",
      "copying build/lib/mindformers/dataset/transforms/build_transforms.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/transforms\n",
      "copying build/lib/mindformers/dataset/transforms/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/transforms\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/dataset/mask\n",
      "copying build/lib/mindformers/dataset/mask/build_mask.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/mask\n",
      "copying build/lib/mindformers/dataset/mask/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/mask\n",
      "copying build/lib/mindformers/dataset/mask/vision_mask.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/mask\n",
      "copying build/lib/mindformers/dataset/zero_shot_image_classification_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/token_classification_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/wmt16_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/toolaplaca_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/cluener_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/multi_source_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/sft_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/multi_image_cap_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/sft_map_functions.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/training_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/cifar100_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/datareaders.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/adgen_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/build_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/squad_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "copying build/lib/mindformers/dataset/dataloader/flickr8k_dataloader.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/dataloader\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/dataset/sampler\n",
      "copying build/lib/mindformers/dataset/sampler/build_sampler.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/sampler\n",
      "copying build/lib/mindformers/dataset/sampler/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset/sampler\n",
      "copying build/lib/mindformers/dataset/utils.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/build_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/causal_language_model_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/base_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/multi_turn_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "copying build/lib/mindformers/dataset/question_answering_dataset.py -> build/bdist.linux-aarch64/wheel/mindformers/dataset\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/modules\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/modules/transformer\n",
      "copying build/lib/mindformers/modules/transformer/transformer.py -> build/bdist.linux-aarch64/wheel/mindformers/modules/transformer\n",
      "copying build/lib/mindformers/modules/transformer/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/modules/transformer\n",
      "copying build/lib/mindformers/modules/transformer/op_parallel_config.py -> build/bdist.linux-aarch64/wheel/mindformers/modules/transformer\n",
      "copying build/lib/mindformers/modules/transformer/moe.py -> build/bdist.linux-aarch64/wheel/mindformers/modules/transformer\n",
      "copying build/lib/mindformers/modules/kvcache_mgr.py -> build/bdist.linux-aarch64/wheel/mindformers/modules\n",
      "copying build/lib/mindformers/modules/local_block_sparse_attention.py -> build/bdist.linux-aarch64/wheel/mindformers/modules\n",
      "copying build/lib/mindformers/modules/layers.py -> build/bdist.linux-aarch64/wheel/mindformers/modules\n",
      "copying build/lib/mindformers/modules/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/modules\n",
      "copying build/lib/mindformers/modules/activation.py -> build/bdist.linux-aarch64/wheel/mindformers/modules\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/zero_shot_image_classification_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/image_classification_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/base_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/translation_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/fill_mask_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/token_classification_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/text_classification_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/text_generation_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/masked_image_modeling_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/build_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/question_answering_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/image_to_text_generation_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "copying build/lib/mindformers/pipeline/segment_anything_pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/pipeline\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/logger.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/download_tools_multithread.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/export.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/tools/register\n",
      "copying build/lib/mindformers/tools/register/register.py -> build/bdist.linux-aarch64/wheel/mindformers/tools/register\n",
      "copying build/lib/mindformers/tools/register/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/tools/register\n",
      "copying build/lib/mindformers/tools/register/config.py -> build/bdist.linux-aarch64/wheel/mindformers/tools/register\n",
      "copying build/lib/mindformers/tools/check_rules.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/merge_hccl.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/hccl_tools.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/moe_token_distribution_tools.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/utils.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/transform_ckpt.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/image_tools.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/tools/download_tools.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/tools/cloud_adapter\n",
      "copying build/lib/mindformers/tools/cloud_adapter/cloud_adapter.py -> build/bdist.linux-aarch64/wheel/mindformers/tools/cloud_adapter\n",
      "copying build/lib/mindformers/tools/cloud_adapter/cloud_monitor.py -> build/bdist.linux-aarch64/wheel/mindformers/tools/cloud_adapter\n",
      "copying build/lib/mindformers/tools/cloud_adapter/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/tools/cloud_adapter\n",
      "copying build/lib/mindformers/tools/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/tools\n",
      "copying build/lib/mindformers/mindformer_book.py -> build/bdist.linux-aarch64/wheel/mindformers\n",
      "copying build/lib/mindformers/version_control.py -> build/bdist.linux-aarch64/wheel/mindformers\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/core\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/core/optim\n",
      "copying build/lib/mindformers/core/optim/build_optim.py -> build/bdist.linux-aarch64/wheel/mindformers/core/optim\n",
      "copying build/lib/mindformers/core/optim/optim.py -> build/bdist.linux-aarch64/wheel/mindformers/core/optim\n",
      "copying build/lib/mindformers/core/optim/came.py -> build/bdist.linux-aarch64/wheel/mindformers/core/optim\n",
      "copying build/lib/mindformers/core/optim/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/core/optim\n",
      "copying build/lib/mindformers/core/parallel_config.py -> build/bdist.linux-aarch64/wheel/mindformers/core\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/core/context\n",
      "copying build/lib/mindformers/core/context/build_context.py -> build/bdist.linux-aarch64/wheel/mindformers/core/context\n",
      "copying build/lib/mindformers/core/context/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/core/context\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/core/loss\n",
      "copying build/lib/mindformers/core/loss/loss.py -> build/bdist.linux-aarch64/wheel/mindformers/core/loss\n",
      "copying build/lib/mindformers/core/loss/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/core/loss\n",
      "copying build/lib/mindformers/core/loss/build_loss.py -> build/bdist.linux-aarch64/wheel/mindformers/core/loss\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/core/callback\n",
      "copying build/lib/mindformers/core/callback/build_callback.py -> build/bdist.linux-aarch64/wheel/mindformers/core/callback\n",
      "copying build/lib/mindformers/core/callback/callback.py -> build/bdist.linux-aarch64/wheel/mindformers/core/callback\n",
      "copying build/lib/mindformers/core/callback/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/core/callback\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/core/metric\n",
      "copying build/lib/mindformers/core/metric/build_metric.py -> build/bdist.linux-aarch64/wheel/mindformers/core/metric\n",
      "copying build/lib/mindformers/core/metric/utils.py -> build/bdist.linux-aarch64/wheel/mindformers/core/metric\n",
      "copying build/lib/mindformers/core/metric/metric.py -> build/bdist.linux-aarch64/wheel/mindformers/core/metric\n",
      "copying build/lib/mindformers/core/metric/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/core/metric\n",
      "copying build/lib/mindformers/core/clip_grad.py -> build/bdist.linux-aarch64/wheel/mindformers/core\n",
      "copying build/lib/mindformers/core/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/core\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/core/lr\n",
      "copying build/lib/mindformers/core/lr/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/core/lr\n",
      "copying build/lib/mindformers/core/lr/build_lr.py -> build/bdist.linux-aarch64/wheel/mindformers/core/lr\n",
      "copying build/lib/mindformers/core/lr/lr_schedule.py -> build/bdist.linux-aarch64/wheel/mindformers/core/lr\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/wrapper\n",
      "copying build/lib/mindformers/wrapper/adaptive_loss_scale.py -> build/bdist.linux-aarch64/wheel/mindformers/wrapper\n",
      "copying build/lib/mindformers/wrapper/build_wrapper.py -> build/bdist.linux-aarch64/wheel/mindformers/wrapper\n",
      "copying build/lib/mindformers/wrapper/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/wrapper\n",
      "copying build/lib/mindformers/wrapper/wrapper.py -> build/bdist.linux-aarch64/wheel/mindformers/wrapper\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/inference\n",
      "copying build/lib/mindformers/inference/infer_task.py -> build/bdist.linux-aarch64/wheel/mindformers/inference\n",
      "copying build/lib/mindformers/inference/postprocess_sampler.py -> build/bdist.linux-aarch64/wheel/mindformers/inference\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/inference/infers\n",
      "copying build/lib/mindformers/inference/infers/text_generator_infer.py -> build/bdist.linux-aarch64/wheel/mindformers/inference/infers\n",
      "copying build/lib/mindformers/inference/infers/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/inference/infers\n",
      "copying build/lib/mindformers/inference/infers/base_infer.py -> build/bdist.linux-aarch64/wheel/mindformers/inference/infers\n",
      "copying build/lib/mindformers/inference/context.py -> build/bdist.linux-aarch64/wheel/mindformers/inference\n",
      "copying build/lib/mindformers/inference/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/inference\n",
      "copying build/lib/mindformers/inference/pipeline.py -> build/bdist.linux-aarch64/wheel/mindformers/inference\n",
      "copying build/lib/mindformers/inference/infer_config.py -> build/bdist.linux-aarch64/wheel/mindformers/inference\n",
      "copying build/lib/mindformers/.commit_id -> build/bdist.linux-aarch64/wheel/mindformers\n",
      "copying build/lib/mindformers/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers\n",
      "copying build/lib/mindformers/auto_class.py -> build/bdist.linux-aarch64/wheel/mindformers\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/generation\n",
      "copying build/lib/mindformers/generation/beam_search.py -> build/bdist.linux-aarch64/wheel/mindformers/generation\n",
      "copying build/lib/mindformers/generation/utils.py -> build/bdist.linux-aarch64/wheel/mindformers/generation\n",
      "copying build/lib/mindformers/generation/text_generator.py -> build/bdist.linux-aarch64/wheel/mindformers/generation\n",
      "copying build/lib/mindformers/generation/streamers.py -> build/bdist.linux-aarch64/wheel/mindformers/generation\n",
      "copying build/lib/mindformers/generation/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/generation\n",
      "copying build/lib/mindformers/generation/generation_config.py -> build/bdist.linux-aarch64/wheel/mindformers/generation\n",
      "copying build/lib/mindformers/generation/logits_process.py -> build/bdist.linux-aarch64/wheel/mindformers/generation\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "copying build/lib/mindformers/models/gpt2/gpt2.py -> build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "copying build/lib/mindformers/models/gpt2/gpt_modules.py -> build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "copying build/lib/mindformers/models/gpt2/gpt2_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "copying build/lib/mindformers/models/gpt2/gpt2_tokenizer_fast.py -> build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "copying build/lib/mindformers/models/gpt2/gpt2_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "copying build/lib/mindformers/models/gpt2/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "copying build/lib/mindformers/models/gpt2/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "copying build/lib/mindformers/models/gpt2/gpt2_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/gpt2\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/llama_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/llama.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/llama_layer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/llama_tokenizer_fast.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/llama_interleave.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/llama_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/llama_transformer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/llama/llama_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/llama\n",
      "copying build/lib/mindformers/models/base_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/clip\n",
      "copying build/lib/mindformers/models/clip/clip_modules.py -> build/bdist.linux-aarch64/wheel/mindformers/models/clip\n",
      "copying build/lib/mindformers/models/clip/clip.py -> build/bdist.linux-aarch64/wheel/mindformers/models/clip\n",
      "copying build/lib/mindformers/models/clip/clip_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/clip\n",
      "copying build/lib/mindformers/models/clip/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/clip\n",
      "copying build/lib/mindformers/models/clip/clip_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/clip\n",
      "copying build/lib/mindformers/models/clip/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/clip\n",
      "copying build/lib/mindformers/models/clip/clip_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/clip\n",
      "copying build/lib/mindformers/models/build_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/glm2\n",
      "copying build/lib/mindformers/models/glm2/glm2_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm2\n",
      "copying build/lib/mindformers/models/glm2/glm2_modules.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm2\n",
      "copying build/lib/mindformers/models/glm2/glm2_transformer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm2\n",
      "copying build/lib/mindformers/models/glm2/glm2_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm2\n",
      "copying build/lib/mindformers/models/glm2/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm2\n",
      "copying build/lib/mindformers/models/glm2/glm2.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm2\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "copying build/lib/mindformers/models/t5/t5_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "copying build/lib/mindformers/models/t5/mt5.py -> build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "copying build/lib/mindformers/models/t5/t5.py -> build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "copying build/lib/mindformers/models/t5/t5_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "copying build/lib/mindformers/models/t5/t5_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "copying build/lib/mindformers/models/t5/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "copying build/lib/mindformers/models/t5/t5_tokenizer_fast.py -> build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "copying build/lib/mindformers/models/t5/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/t5\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/swin\n",
      "copying build/lib/mindformers/models/swin/swin_modules.py -> build/bdist.linux-aarch64/wheel/mindformers/models/swin\n",
      "copying build/lib/mindformers/models/swin/swin.py -> build/bdist.linux-aarch64/wheel/mindformers/models/swin\n",
      "copying build/lib/mindformers/models/swin/swin_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/swin\n",
      "copying build/lib/mindformers/models/swin/swin_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/swin\n",
      "copying build/lib/mindformers/models/swin/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/swin\n",
      "copying build/lib/mindformers/models/swin/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/swin\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/blip2_itm_evaluator.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/qformer_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/blip2_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/blip2_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/qformer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/layers.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/blip2_vit.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/blip2_llama.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/blip2.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/blip2_qformer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/blip2/blip2_llm.py -> build/bdist.linux-aarch64/wheel/mindformers/models/blip2\n",
      "copying build/lib/mindformers/models/base_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/bloom.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/bloom_reward.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/layers.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/bloom_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/bloom_tokenizer_fast.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/bloom_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "copying build/lib/mindformers/models/bloom/bloom_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bloom\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/bert\n",
      "copying build/lib/mindformers/models/bert/bert_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bert\n",
      "copying build/lib/mindformers/models/bert/bert.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bert\n",
      "copying build/lib/mindformers/models/bert/bert_tokenizer_fast.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bert\n",
      "copying build/lib/mindformers/models/bert/bert_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bert\n",
      "copying build/lib/mindformers/models/bert/bert_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bert\n",
      "copying build/lib/mindformers/models/bert/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bert\n",
      "copying build/lib/mindformers/models/bert/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/bert\n",
      "copying build/lib/mindformers/models/utils.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "copying build/lib/mindformers/models/base_model.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/glm3\n",
      "copying build/lib/mindformers/models/glm3/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm3\n",
      "copying build/lib/mindformers/models/glm3/glm3_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm3\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "copying build/lib/mindformers/models/glm/glm_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "copying build/lib/mindformers/models/glm/chatglm_6b_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "copying build/lib/mindformers/models/glm/glm_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "copying build/lib/mindformers/models/glm/attention.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "copying build/lib/mindformers/models/glm/layers.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "copying build/lib/mindformers/models/glm/glm.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "copying build/lib/mindformers/models/glm/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "copying build/lib/mindformers/models/glm/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/glm\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/mae\n",
      "copying build/lib/mindformers/models/mae/mae.py -> build/bdist.linux-aarch64/wheel/mindformers/models/mae\n",
      "copying build/lib/mindformers/models/mae/mae_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/mae\n",
      "copying build/lib/mindformers/models/mae/mae_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/mae\n",
      "copying build/lib/mindformers/models/mae/mae_modules.py -> build/bdist.linux-aarch64/wheel/mindformers/models/mae\n",
      "copying build/lib/mindformers/models/mae/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/mae\n",
      "copying build/lib/mindformers/models/mae/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/mae\n",
      "copying build/lib/mindformers/models/build_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "copying build/lib/mindformers/models/convert_slow_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "copying build/lib/mindformers/models/base_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "copying build/lib/mindformers/models/build_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "copying build/lib/mindformers/models/base_fast_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "copying build/lib/mindformers/models/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "copying build/lib/mindformers/models/sentencepiece_model_pb2.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/pangualpha\n",
      "copying build/lib/mindformers/models/pangualpha/pangualpha_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/pangualpha\n",
      "copying build/lib/mindformers/models/pangualpha/pangualpha_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/pangualpha\n",
      "copying build/lib/mindformers/models/pangualpha/pangualpha.py -> build/bdist.linux-aarch64/wheel/mindformers/models/pangualpha\n",
      "copying build/lib/mindformers/models/pangualpha/pangualpha_tokenizer.py -> build/bdist.linux-aarch64/wheel/mindformers/models/pangualpha\n",
      "copying build/lib/mindformers/models/pangualpha/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/pangualpha\n",
      "copying build/lib/mindformers/models/pangualpha/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/pangualpha\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/vit\n",
      "copying build/lib/mindformers/models/vit/vit_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/vit\n",
      "copying build/lib/mindformers/models/vit/vit_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/vit\n",
      "copying build/lib/mindformers/models/vit/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/vit\n",
      "copying build/lib/mindformers/models/vit/vit_modules.py -> build/bdist.linux-aarch64/wheel/mindformers/models/vit\n",
      "copying build/lib/mindformers/models/vit/vit.py -> build/bdist.linux-aarch64/wheel/mindformers/models/vit\n",
      "copying build/lib/mindformers/models/vit/convert_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/vit\n",
      "copying build/lib/mindformers/models/build_model.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/sam_config.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/conver_weight.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/sam_image_encoder.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/sam.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/sam_layers.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/sam_processor.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/sam_utils.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/sam_prompt_encoder.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sam/sam_mask_decoder.py -> build/bdist.linux-aarch64/wheel/mindformers/models/sam\n",
      "copying build/lib/mindformers/models/sentencepiece_model_pb2_new.py -> build/bdist.linux-aarch64/wheel/mindformers/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/pet\n",
      "copying build/lib/mindformers/pet/pet_config.py -> build/bdist.linux-aarch64/wheel/mindformers/pet\n",
      "copying build/lib/mindformers/pet/constants.py -> build/bdist.linux-aarch64/wheel/mindformers/pet\n",
      "copying build/lib/mindformers/pet/utils.py -> build/bdist.linux-aarch64/wheel/mindformers/pet\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/pet/tuners\n",
      "copying build/lib/mindformers/pet/tuners/prefix_tuning_adapter.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/tuners\n",
      "copying build/lib/mindformers/pet/tuners/ptuning2_adapter.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/tuners\n",
      "copying build/lib/mindformers/pet/tuners/lora_adapter.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/tuners\n",
      "copying build/lib/mindformers/pet/tuners/ada_adapter.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/tuners\n",
      "copying build/lib/mindformers/pet/tuners/adalora_adapter.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/tuners\n",
      "copying build/lib/mindformers/pet/tuners/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/tuners\n",
      "copying build/lib/mindformers/pet/tuners/pet_adapter.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/tuners\n",
      "copying build/lib/mindformers/pet/pet_model.py -> build/bdist.linux-aarch64/wheel/mindformers/pet\n",
      "copying build/lib/mindformers/pet/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/pet\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/pet/models\n",
      "copying build/lib/mindformers/pet/models/lora.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/models\n",
      "copying build/lib/mindformers/pet/models/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/pet/models\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/causal_language_modeling\n",
      "copying build/lib/mindformers/trainer/causal_language_modeling/causal_language_modeling.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/causal_language_modeling\n",
      "copying build/lib/mindformers/trainer/causal_language_modeling/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/causal_language_modeling\n",
      "copying build/lib/mindformers/trainer/training_args.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "copying build/lib/mindformers/trainer/trainer.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/image_to_text_retrieval\n",
      "copying build/lib/mindformers/trainer/image_to_text_retrieval/eval_utils.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_to_text_retrieval\n",
      "copying build/lib/mindformers/trainer/image_to_text_retrieval/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_to_text_retrieval\n",
      "copying build/lib/mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_to_text_retrieval\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/token_classification\n",
      "copying build/lib/mindformers/trainer/token_classification/token_classification.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/token_classification\n",
      "copying build/lib/mindformers/trainer/token_classification/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/token_classification\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/masked_language_modeling\n",
      "copying build/lib/mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/masked_language_modeling\n",
      "copying build/lib/mindformers/trainer/masked_language_modeling/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/masked_language_modeling\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/masked_image_modeling\n",
      "copying build/lib/mindformers/trainer/masked_image_modeling/group_mim_parameters.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/masked_image_modeling\n",
      "copying build/lib/mindformers/trainer/masked_image_modeling/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/masked_image_modeling\n",
      "copying build/lib/mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/masked_image_modeling\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/text_classfication\n",
      "copying build/lib/mindformers/trainer/text_classfication/text_classification.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/text_classfication\n",
      "copying build/lib/mindformers/trainer/text_classfication/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/text_classfication\n",
      "copying build/lib/mindformers/trainer/config_args.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/question_answering\n",
      "copying build/lib/mindformers/trainer/question_answering/question_answering.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/question_answering\n",
      "copying build/lib/mindformers/trainer/question_answering/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/question_answering\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/translation\n",
      "copying build/lib/mindformers/trainer/translation/translation_finetune.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/translation\n",
      "copying build/lib/mindformers/trainer/translation/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/translation\n",
      "copying build/lib/mindformers/trainer/base_trainer.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "copying build/lib/mindformers/trainer/utils.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "copying build/lib/mindformers/trainer/optimizer_grouped_parameters.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "copying build/lib/mindformers/trainer/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/image_to_text_generation\n",
      "copying build/lib/mindformers/trainer/image_to_text_generation/image_to_text_generation.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_to_text_generation\n",
      "copying build/lib/mindformers/trainer/image_to_text_generation/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_to_text_generation\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/contrastive_language_image_pretrain\n",
      "copying build/lib/mindformers/trainer/contrastive_language_image_pretrain/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/contrastive_language_image_pretrain\n",
      "copying build/lib/mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/contrastive_language_image_pretrain\n",
      "copying build/lib/mindformers/trainer/build_trainer.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/general_task_trainer\n",
      "copying build/lib/mindformers/trainer/general_task_trainer/general_task_trainer.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/general_task_trainer\n",
      "copying build/lib/mindformers/trainer/general_task_trainer/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/general_task_trainer\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers/trainer/image_classification\n",
      "copying build/lib/mindformers/trainer/image_classification/group_ic_params.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_classification\n",
      "copying build/lib/mindformers/trainer/image_classification/__init__.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_classification\n",
      "copying build/lib/mindformers/trainer/image_classification/zero_shot_image_classification.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_classification\n",
      "copying build/lib/mindformers/trainer/image_classification/image_classification.py -> build/bdist.linux-aarch64/wheel/mindformers/trainer/image_classification\n",
      "creating build/bdist.linux-aarch64/wheel/configs\n",
      "creating build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "copying build/lib/configs/gpt2/run_gpt2_52b.yaml -> build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "copying build/lib/configs/gpt2/run_gpt2_lora.yaml -> build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "copying build/lib/configs/gpt2/run_gpt2.yaml -> build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "copying build/lib/configs/gpt2/run_gpt2_txtcls.yaml -> build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "copying build/lib/configs/gpt2/run_gpt2_xl.yaml -> build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "copying build/lib/configs/gpt2/run_gpt2_13b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "copying build/lib/configs/gpt2/run_gpt2_13b.yaml -> build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "copying build/lib/configs/gpt2/run_gpt2_xl_lora.yaml -> build/bdist.linux-aarch64/wheel/configs/gpt2\n",
      "creating build/bdist.linux-aarch64/wheel/configs/llama\n",
      "copying build/lib/configs/llama/run_llama_13b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama\n",
      "copying build/lib/configs/llama/run_llama_13b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama\n",
      "copying build/lib/configs/llama/run_llama_7b_lora.yaml -> build/bdist.linux-aarch64/wheel/configs/llama\n",
      "copying build/lib/configs/llama/run_llama_7b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama\n",
      "copying build/lib/configs/llama/run_llama_7b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama\n",
      "creating build/bdist.linux-aarch64/wheel/configs/qa\n",
      "copying build/lib/configs/qa/run_qa_bert_base_uncased.yaml -> build/bdist.linux-aarch64/wheel/configs/qa\n",
      "creating build/bdist.linux-aarch64/wheel/configs/clip\n",
      "copying build/lib/configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml -> build/bdist.linux-aarch64/wheel/configs/clip\n",
      "copying build/lib/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml -> build/bdist.linux-aarch64/wheel/configs/clip\n",
      "copying build/lib/configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml -> build/bdist.linux-aarch64/wheel/configs/clip\n",
      "copying build/lib/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml -> build/bdist.linux-aarch64/wheel/configs/clip\n",
      "copying build/lib/configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml -> build/bdist.linux-aarch64/wheel/configs/clip\n",
      "copying build/lib/configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml -> build/bdist.linux-aarch64/wheel/configs/clip\n",
      "copying build/lib/configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml -> build/bdist.linux-aarch64/wheel/configs/clip\n",
      "copying build/lib/configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml -> build/bdist.linux-aarch64/wheel/configs/clip\n",
      "creating build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_finetune_2k_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_ptuning2.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_finetune_2k.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/export_glm2_6b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_lora.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_lora_2k_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_finetune_eval.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_lora_eval.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_lora_2k.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_finetune.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_lora_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b_finetune_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "copying build/lib/configs/glm2/run_glm2_6b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm2\n",
      "creating build/bdist.linux-aarch64/wheel/configs/t5\n",
      "copying build/lib/configs/t5/run_t5_tiny_on_wmt16.yaml -> build/bdist.linux-aarch64/wheel/configs/t5\n",
      "copying build/lib/configs/t5/run_t5_small_on_wmt16.yaml -> build/bdist.linux-aarch64/wheel/configs/t5\n",
      "creating build/bdist.linux-aarch64/wheel/configs/general\n",
      "copying build/lib/configs/general/run_general_task.yaml -> build/bdist.linux-aarch64/wheel/configs/general\n",
      "creating build/bdist.linux-aarch64/wheel/configs/swin\n",
      "copying build/lib/configs/swin/run_swin_base_p4w7_224_100ep.yaml -> build/bdist.linux-aarch64/wheel/configs/swin\n",
      "creating build/bdist.linux-aarch64/wheel/configs/tokcls\n",
      "copying build/lib/configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml -> build/bdist.linux-aarch64/wheel/configs/tokcls\n",
      "copying build/lib/configs/tokcls/run_tokcls_bert_base_chinese.yaml -> build/bdist.linux-aarch64/wheel/configs/tokcls\n",
      "creating build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "copying build/lib/configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "copying build/lib/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml -> build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "copying build/lib/configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml -> build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "copying build/lib/configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml -> build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "copying build/lib/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml -> build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "copying build/lib/configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml -> build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "copying build/lib/configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml -> build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "copying build/lib/configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml -> build/bdist.linux-aarch64/wheel/configs/blip2\n",
      "creating build/bdist.linux-aarch64/wheel/configs/txtcls\n",
      "copying build/lib/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml -> build/bdist.linux-aarch64/wheel/configs/txtcls\n",
      "copying build/lib/configs/txtcls/run_txtcls_bert_base_uncased.yaml -> build/bdist.linux-aarch64/wheel/configs/txtcls\n",
      "creating build/bdist.linux-aarch64/wheel/configs/bloom\n",
      "copying build/lib/configs/bloom/run_bloom_7.1b_910b_fa.yaml -> build/bdist.linux-aarch64/wheel/configs/bloom\n",
      "copying build/lib/configs/bloom/run_bloom_560m.yaml -> build/bdist.linux-aarch64/wheel/configs/bloom\n",
      "copying build/lib/configs/bloom/run_bloom_7.1b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/bloom\n",
      "copying build/lib/configs/bloom/run_bloom_7.1b.yaml -> build/bdist.linux-aarch64/wheel/configs/bloom\n",
      "creating build/bdist.linux-aarch64/wheel/configs/bert\n",
      "copying build/lib/configs/bert/run_bert_base_uncased.yaml -> build/bdist.linux-aarch64/wheel/configs/bert\n",
      "copying build/lib/configs/bert/run_bert_tiny_uncased.yaml -> build/bdist.linux-aarch64/wheel/configs/bert\n",
      "creating build/bdist.linux-aarch64/wheel/configs/codegeex2\n",
      "copying build/lib/configs/codegeex2/run_codegeex2_6b_eval.yaml -> build/bdist.linux-aarch64/wheel/configs/codegeex2\n",
      "copying build/lib/configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml -> build/bdist.linux-aarch64/wheel/configs/codegeex2\n",
      "copying build/lib/configs/codegeex2/run_codegeex2_6b.yaml -> build/bdist.linux-aarch64/wheel/configs/codegeex2\n",
      "copying build/lib/configs/codegeex2/run_codegeex2_6b_finetune.yaml -> build/bdist.linux-aarch64/wheel/configs/codegeex2\n",
      "creating build/bdist.linux-aarch64/wheel/configs/codellama\n",
      "copying build/lib/configs/codellama/run_codellama_34b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/codellama\n",
      "copying build/lib/configs/codellama/predict_codellama_34b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/codellama\n",
      "creating build/bdist.linux-aarch64/wheel/configs/glm3\n",
      "copying build/lib/configs/glm3/run_glm3_6b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm3\n",
      "copying build/lib/configs/glm3/export_glm3_6b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm3\n",
      "copying build/lib/configs/glm3/run_glm3_6b_multiturn_finetune_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm3\n",
      "copying build/lib/configs/glm3/run_glm3_6b_finetune_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm3\n",
      "copying build/lib/configs/glm3/run_glm3_6b_finetune_2k_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/glm3\n",
      "creating build/bdist.linux-aarch64/wheel/configs/glm\n",
      "copying build/lib/configs/glm/run_glm_6b_lora.yaml -> build/bdist.linux-aarch64/wheel/configs/glm\n",
      "copying build/lib/configs/glm/run_glm_6b_finetune.yaml -> build/bdist.linux-aarch64/wheel/configs/glm\n",
      "copying build/lib/configs/glm/run_glm_6b_infer.yaml -> build/bdist.linux-aarch64/wheel/configs/glm\n",
      "copying build/lib/configs/glm/run_glm_6b_lora_infer.yaml -> build/bdist.linux-aarch64/wheel/configs/glm\n",
      "creating build/bdist.linux-aarch64/wheel/configs/mae\n",
      "copying build/lib/configs/mae/run_mae_vit_base_p16_224_800ep.yaml -> build/bdist.linux-aarch64/wheel/configs/mae\n",
      "copying build/lib/configs/README.md -> build/bdist.linux-aarch64/wheel/configs\n",
      "creating build/bdist.linux-aarch64/wheel/configs/pangualpha\n",
      "copying build/lib/configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml -> build/bdist.linux-aarch64/wheel/configs/pangualpha\n",
      "copying build/lib/configs/pangualpha/run_pangualpha_2_6b.yaml -> build/bdist.linux-aarch64/wheel/configs/pangualpha\n",
      "copying build/lib/configs/pangualpha/run_pangualpha_13b.yaml -> build/bdist.linux-aarch64/wheel/configs/pangualpha\n",
      "copying build/lib/configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml -> build/bdist.linux-aarch64/wheel/configs/pangualpha\n",
      "creating build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_7b_lora_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_7b_910b_auto_parallel.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_13b_lora_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_70b_910b_auto_parallel.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_7b_910b_finetune.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_13b_910b_finetune.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_13b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_13b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_70b_910b_finetune.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_70b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_7b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_7b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_13b_910b_auto_parallel.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/predict_llama2_70b_910b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/export_llama2_7b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/run_llama2_70b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "copying build/lib/configs/llama2/export_llama2_13b.yaml -> build/bdist.linux-aarch64/wheel/configs/llama2\n",
      "creating build/bdist.linux-aarch64/wheel/configs/vit\n",
      "copying build/lib/configs/vit/run_vit_base_p16_224_100ep.yaml -> build/bdist.linux-aarch64/wheel/configs/vit\n",
      "creating build/bdist.linux-aarch64/wheel/configs/sam\n",
      "copying build/lib/configs/sam/run_sam_vit-l.yaml -> build/bdist.linux-aarch64/wheel/configs/sam\n",
      "copying build/lib/configs/sam/run_sam_vit-h.yaml -> build/bdist.linux-aarch64/wheel/configs/sam\n",
      "copying build/lib/configs/sam/run_sam_vit-b.yaml -> build/bdist.linux-aarch64/wheel/configs/sam\n",
      "running install_egg_info\n",
      "Copying mindformers.egg-info to build/bdist.linux-aarch64/wheel/mindformers-0.8.0-py3.9.egg-info\n",
      "running install_scripts\n",
      "creating build/bdist.linux-aarch64/wheel/mindformers-0.8.0.dist-info/WHEEL\n",
      "creating '/home/ma-user/work/mindformers/output/mindformers-0.8.0-py3-none-any.whl' and adding 'build/bdist.linux-aarch64/wheel' to it\n",
      "adding 'configs/README.md'\n",
      "adding 'configs/bert/run_bert_base_uncased.yaml'\n",
      "adding 'configs/bert/run_bert_tiny_uncased.yaml'\n",
      "adding 'configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml'\n",
      "adding 'configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml'\n",
      "adding 'configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml'\n",
      "adding 'configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml'\n",
      "adding 'configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml'\n",
      "adding 'configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml'\n",
      "adding 'configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml'\n",
      "adding 'configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml'\n",
      "adding 'configs/bloom/run_bloom_560m.yaml'\n",
      "adding 'configs/bloom/run_bloom_7.1b.yaml'\n",
      "adding 'configs/bloom/run_bloom_7.1b_910b.yaml'\n",
      "adding 'configs/bloom/run_bloom_7.1b_910b_fa.yaml'\n",
      "adding 'configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml'\n",
      "adding 'configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml'\n",
      "adding 'configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'\n",
      "adding 'configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'\n",
      "adding 'configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml'\n",
      "adding 'configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml'\n",
      "adding 'configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml'\n",
      "adding 'configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml'\n",
      "adding 'configs/codegeex2/run_codegeex2_6b.yaml'\n",
      "adding 'configs/codegeex2/run_codegeex2_6b_eval.yaml'\n",
      "adding 'configs/codegeex2/run_codegeex2_6b_finetune.yaml'\n",
      "adding 'configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml'\n",
      "adding 'configs/codellama/predict_codellama_34b_910b.yaml'\n",
      "adding 'configs/codellama/run_codellama_34b_910b.yaml'\n",
      "adding 'configs/general/run_general_task.yaml'\n",
      "adding 'configs/glm/run_glm_6b_finetune.yaml'\n",
      "adding 'configs/glm/run_glm_6b_infer.yaml'\n",
      "adding 'configs/glm/run_glm_6b_lora.yaml'\n",
      "adding 'configs/glm/run_glm_6b_lora_infer.yaml'\n",
      "adding 'configs/glm2/export_glm2_6b.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_finetune.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_finetune_2k.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_finetune_2k_910b.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_finetune_910b.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_finetune_eval.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_lora.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_lora_2k.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_lora_2k_910b.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_lora_910b.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_lora_eval.yaml'\n",
      "adding 'configs/glm2/run_glm2_6b_ptuning2.yaml'\n",
      "adding 'configs/glm3/export_glm3_6b.yaml'\n",
      "adding 'configs/glm3/run_glm3_6b.yaml'\n",
      "adding 'configs/glm3/run_glm3_6b_finetune_2k_910b.yaml'\n",
      "adding 'configs/glm3/run_glm3_6b_finetune_910b.yaml'\n",
      "adding 'configs/glm3/run_glm3_6b_multiturn_finetune_910b.yaml'\n",
      "adding 'configs/gpt2/run_gpt2.yaml'\n",
      "adding 'configs/gpt2/run_gpt2_13b.yaml'\n",
      "adding 'configs/gpt2/run_gpt2_13b_910b.yaml'\n",
      "adding 'configs/gpt2/run_gpt2_52b.yaml'\n",
      "adding 'configs/gpt2/run_gpt2_lora.yaml'\n",
      "adding 'configs/gpt2/run_gpt2_txtcls.yaml'\n",
      "adding 'configs/gpt2/run_gpt2_xl.yaml'\n",
      "adding 'configs/gpt2/run_gpt2_xl_lora.yaml'\n",
      "adding 'configs/llama/run_llama_13b.yaml'\n",
      "adding 'configs/llama/run_llama_13b_910b.yaml'\n",
      "adding 'configs/llama/run_llama_7b.yaml'\n",
      "adding 'configs/llama/run_llama_7b_910b.yaml'\n",
      "adding 'configs/llama/run_llama_7b_lora.yaml'\n",
      "adding 'configs/llama2/export_llama2_13b.yaml'\n",
      "adding 'configs/llama2/export_llama2_7b.yaml'\n",
      "adding 'configs/llama2/predict_llama2_70b_910b.yaml'\n",
      "adding 'configs/llama2/run_llama2_13b.yaml'\n",
      "adding 'configs/llama2/run_llama2_13b_910b.yaml'\n",
      "adding 'configs/llama2/run_llama2_13b_910b_auto_parallel.yaml'\n",
      "adding 'configs/llama2/run_llama2_13b_910b_finetune.yaml'\n",
      "adding 'configs/llama2/run_llama2_13b_lora_910b.yaml'\n",
      "adding 'configs/llama2/run_llama2_70b.yaml'\n",
      "adding 'configs/llama2/run_llama2_70b_910b.yaml'\n",
      "adding 'configs/llama2/run_llama2_70b_910b_auto_parallel.yaml'\n",
      "adding 'configs/llama2/run_llama2_70b_910b_finetune.yaml'\n",
      "adding 'configs/llama2/run_llama2_7b.yaml'\n",
      "adding 'configs/llama2/run_llama2_7b_910b.yaml'\n",
      "adding 'configs/llama2/run_llama2_7b_910b_auto_parallel.yaml'\n",
      "adding 'configs/llama2/run_llama2_7b_910b_finetune.yaml'\n",
      "adding 'configs/llama2/run_llama2_7b_lora_910b.yaml'\n",
      "adding 'configs/mae/run_mae_vit_base_p16_224_800ep.yaml'\n",
      "adding 'configs/pangualpha/run_pangualpha_13b.yaml'\n",
      "adding 'configs/pangualpha/run_pangualpha_2_6b.yaml'\n",
      "adding 'configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml'\n",
      "adding 'configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml'\n",
      "adding 'configs/qa/run_qa_bert_base_uncased.yaml'\n",
      "adding 'configs/sam/run_sam_vit-b.yaml'\n",
      "adding 'configs/sam/run_sam_vit-h.yaml'\n",
      "adding 'configs/sam/run_sam_vit-l.yaml'\n",
      "adding 'configs/swin/run_swin_base_p4w7_224_100ep.yaml'\n",
      "adding 'configs/t5/run_t5_small_on_wmt16.yaml'\n",
      "adding 'configs/t5/run_t5_tiny_on_wmt16.yaml'\n",
      "adding 'configs/tokcls/run_tokcls_bert_base_chinese.yaml'\n",
      "adding 'configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml'\n",
      "adding 'configs/txtcls/run_txtcls_bert_base_uncased.yaml'\n",
      "adding 'configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'\n",
      "adding 'configs/vit/run_vit_base_p16_224_100ep.yaml'\n",
      "adding 'mindformers/.commit_id'\n",
      "adding 'mindformers/__init__.py'\n",
      "adding 'mindformers/auto_class.py'\n",
      "adding 'mindformers/mindformer_book.py'\n",
      "adding 'mindformers/version_control.py'\n",
      "adding 'mindformers/core/__init__.py'\n",
      "adding 'mindformers/core/clip_grad.py'\n",
      "adding 'mindformers/core/parallel_config.py'\n",
      "adding 'mindformers/core/callback/__init__.py'\n",
      "adding 'mindformers/core/callback/build_callback.py'\n",
      "adding 'mindformers/core/callback/callback.py'\n",
      "adding 'mindformers/core/context/__init__.py'\n",
      "adding 'mindformers/core/context/build_context.py'\n",
      "adding 'mindformers/core/loss/__init__.py'\n",
      "adding 'mindformers/core/loss/build_loss.py'\n",
      "adding 'mindformers/core/loss/loss.py'\n",
      "adding 'mindformers/core/lr/__init__.py'\n",
      "adding 'mindformers/core/lr/build_lr.py'\n",
      "adding 'mindformers/core/lr/lr_schedule.py'\n",
      "adding 'mindformers/core/metric/__init__.py'\n",
      "adding 'mindformers/core/metric/build_metric.py'\n",
      "adding 'mindformers/core/metric/metric.py'\n",
      "adding 'mindformers/core/metric/utils.py'\n",
      "adding 'mindformers/core/optim/__init__.py'\n",
      "adding 'mindformers/core/optim/build_optim.py'\n",
      "adding 'mindformers/core/optim/came.py'\n",
      "adding 'mindformers/core/optim/optim.py'\n",
      "adding 'mindformers/dataset/__init__.py'\n",
      "adding 'mindformers/dataset/base_dataset.py'\n",
      "adding 'mindformers/dataset/build_dataset.py'\n",
      "adding 'mindformers/dataset/causal_language_model_dataset.py'\n",
      "adding 'mindformers/dataset/contrastive_language_image_pretrain_dataset.py'\n",
      "adding 'mindformers/dataset/img_cls_dataset.py'\n",
      "adding 'mindformers/dataset/keyword_gen_dataset.py'\n",
      "adding 'mindformers/dataset/labels.py'\n",
      "adding 'mindformers/dataset/mask_language_model_dataset.py'\n",
      "adding 'mindformers/dataset/mim_dataset.py'\n",
      "adding 'mindformers/dataset/multi_turn_dataset.py'\n",
      "adding 'mindformers/dataset/question_answering_dataset.py'\n",
      "adding 'mindformers/dataset/reward_model_dataset.py'\n",
      "adding 'mindformers/dataset/text_classification_dataset.py'\n",
      "adding 'mindformers/dataset/token_classification_dataset.py'\n",
      "adding 'mindformers/dataset/translation_dataset.py'\n",
      "adding 'mindformers/dataset/utils.py'\n",
      "adding 'mindformers/dataset/zero_shot_image_classification_dataset.py'\n",
      "adding 'mindformers/dataset/dataloader/__init__.py'\n",
      "adding 'mindformers/dataset/dataloader/adgen_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/build_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/cifar100_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/cluener_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/datareaders.py'\n",
      "adding 'mindformers/dataset/dataloader/flickr8k_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/multi_image_cap_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/multi_source_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/sft_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/sft_map_functions.py'\n",
      "adding 'mindformers/dataset/dataloader/squad_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/toolaplaca_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/training_dataloader.py'\n",
      "adding 'mindformers/dataset/dataloader/wmt16_dataloader.py'\n",
      "adding 'mindformers/dataset/mask/__init__.py'\n",
      "adding 'mindformers/dataset/mask/build_mask.py'\n",
      "adding 'mindformers/dataset/mask/vision_mask.py'\n",
      "adding 'mindformers/dataset/sampler/__init__.py'\n",
      "adding 'mindformers/dataset/sampler/build_sampler.py'\n",
      "adding 'mindformers/dataset/transforms/__init__.py'\n",
      "adding 'mindformers/dataset/transforms/auto_augment.py'\n",
      "adding 'mindformers/dataset/transforms/build_transforms.py'\n",
      "adding 'mindformers/dataset/transforms/mixup.py'\n",
      "adding 'mindformers/dataset/transforms/random_erasing.py'\n",
      "adding 'mindformers/dataset/transforms/text_transforms.py'\n",
      "adding 'mindformers/dataset/transforms/vision_transforms.py'\n",
      "adding 'mindformers/generation/__init__.py'\n",
      "adding 'mindformers/generation/beam_search.py'\n",
      "adding 'mindformers/generation/generation_config.py'\n",
      "adding 'mindformers/generation/logits_process.py'\n",
      "adding 'mindformers/generation/streamers.py'\n",
      "adding 'mindformers/generation/text_generator.py'\n",
      "adding 'mindformers/generation/utils.py'\n",
      "adding 'mindformers/inference/__init__.py'\n",
      "adding 'mindformers/inference/context.py'\n",
      "adding 'mindformers/inference/infer_config.py'\n",
      "adding 'mindformers/inference/infer_task.py'\n",
      "adding 'mindformers/inference/pipeline.py'\n",
      "adding 'mindformers/inference/postprocess_sampler.py'\n",
      "adding 'mindformers/inference/infers/__init__.py'\n",
      "adding 'mindformers/inference/infers/base_infer.py'\n",
      "adding 'mindformers/inference/infers/text_generator_infer.py'\n",
      "adding 'mindformers/models/__init__.py'\n",
      "adding 'mindformers/models/base_config.py'\n",
      "adding 'mindformers/models/base_fast_tokenizer.py'\n",
      "adding 'mindformers/models/base_model.py'\n",
      "adding 'mindformers/models/base_processor.py'\n",
      "adding 'mindformers/models/base_tokenizer.py'\n",
      "adding 'mindformers/models/build_config.py'\n",
      "adding 'mindformers/models/build_model.py'\n",
      "adding 'mindformers/models/build_processor.py'\n",
      "adding 'mindformers/models/build_tokenizer.py'\n",
      "adding 'mindformers/models/convert_slow_tokenizer.py'\n",
      "adding 'mindformers/models/sentencepiece_model_pb2.py'\n",
      "adding 'mindformers/models/sentencepiece_model_pb2_new.py'\n",
      "adding 'mindformers/models/utils.py'\n",
      "adding 'mindformers/models/bert/__init__.py'\n",
      "adding 'mindformers/models/bert/bert.py'\n",
      "adding 'mindformers/models/bert/bert_config.py'\n",
      "adding 'mindformers/models/bert/bert_processor.py'\n",
      "adding 'mindformers/models/bert/bert_tokenizer.py'\n",
      "adding 'mindformers/models/bert/bert_tokenizer_fast.py'\n",
      "adding 'mindformers/models/bert/convert_weight.py'\n",
      "adding 'mindformers/models/blip2/__init__.py'\n",
      "adding 'mindformers/models/blip2/blip2.py'\n",
      "adding 'mindformers/models/blip2/blip2_config.py'\n",
      "adding 'mindformers/models/blip2/blip2_itm_evaluator.py'\n",
      "adding 'mindformers/models/blip2/blip2_llama.py'\n",
      "adding 'mindformers/models/blip2/blip2_llm.py'\n",
      "adding 'mindformers/models/blip2/blip2_processor.py'\n",
      "adding 'mindformers/models/blip2/blip2_qformer.py'\n",
      "adding 'mindformers/models/blip2/blip2_vit.py'\n",
      "adding 'mindformers/models/blip2/convert_weight.py'\n",
      "adding 'mindformers/models/blip2/layers.py'\n",
      "adding 'mindformers/models/blip2/qformer.py'\n",
      "adding 'mindformers/models/blip2/qformer_config.py'\n",
      "adding 'mindformers/models/bloom/__init__.py'\n",
      "adding 'mindformers/models/bloom/bloom.py'\n",
      "adding 'mindformers/models/bloom/bloom_config.py'\n",
      "adding 'mindformers/models/bloom/bloom_processor.py'\n",
      "adding 'mindformers/models/bloom/bloom_reward.py'\n",
      "adding 'mindformers/models/bloom/bloom_tokenizer.py'\n",
      "adding 'mindformers/models/bloom/bloom_tokenizer_fast.py'\n",
      "adding 'mindformers/models/bloom/convert_weight.py'\n",
      "adding 'mindformers/models/bloom/layers.py'\n",
      "adding 'mindformers/models/clip/__init__.py'\n",
      "adding 'mindformers/models/clip/clip.py'\n",
      "adding 'mindformers/models/clip/clip_config.py'\n",
      "adding 'mindformers/models/clip/clip_modules.py'\n",
      "adding 'mindformers/models/clip/clip_processor.py'\n",
      "adding 'mindformers/models/clip/clip_tokenizer.py'\n",
      "adding 'mindformers/models/clip/convert_weight.py'\n",
      "adding 'mindformers/models/glm/__init__.py'\n",
      "adding 'mindformers/models/glm/attention.py'\n",
      "adding 'mindformers/models/glm/chatglm_6b_tokenizer.py'\n",
      "adding 'mindformers/models/glm/convert_weight.py'\n",
      "adding 'mindformers/models/glm/glm.py'\n",
      "adding 'mindformers/models/glm/glm_config.py'\n",
      "adding 'mindformers/models/glm/glm_processor.py'\n",
      "adding 'mindformers/models/glm/layers.py'\n",
      "adding 'mindformers/models/glm2/__init__.py'\n",
      "adding 'mindformers/models/glm2/glm2.py'\n",
      "adding 'mindformers/models/glm2/glm2_config.py'\n",
      "adding 'mindformers/models/glm2/glm2_modules.py'\n",
      "adding 'mindformers/models/glm2/glm2_tokenizer.py'\n",
      "adding 'mindformers/models/glm2/glm2_transformer.py'\n",
      "adding 'mindformers/models/glm3/__init__.py'\n",
      "adding 'mindformers/models/glm3/glm3_tokenizer.py'\n",
      "adding 'mindformers/models/gpt2/__init__.py'\n",
      "adding 'mindformers/models/gpt2/convert_weight.py'\n",
      "adding 'mindformers/models/gpt2/gpt2.py'\n",
      "adding 'mindformers/models/gpt2/gpt2_config.py'\n",
      "adding 'mindformers/models/gpt2/gpt2_processor.py'\n",
      "adding 'mindformers/models/gpt2/gpt2_tokenizer.py'\n",
      "adding 'mindformers/models/gpt2/gpt2_tokenizer_fast.py'\n",
      "adding 'mindformers/models/gpt2/gpt_modules.py'\n",
      "adding 'mindformers/models/llama/__init__.py'\n",
      "adding 'mindformers/models/llama/convert_weight.py'\n",
      "adding 'mindformers/models/llama/llama.py'\n",
      "adding 'mindformers/models/llama/llama_config.py'\n",
      "adding 'mindformers/models/llama/llama_interleave.py'\n",
      "adding 'mindformers/models/llama/llama_layer.py'\n",
      "adding 'mindformers/models/llama/llama_processor.py'\n",
      "adding 'mindformers/models/llama/llama_tokenizer.py'\n",
      "adding 'mindformers/models/llama/llama_tokenizer_fast.py'\n",
      "adding 'mindformers/models/llama/llama_transformer.py'\n",
      "adding 'mindformers/models/mae/__init__.py'\n",
      "adding 'mindformers/models/mae/convert_weight.py'\n",
      "adding 'mindformers/models/mae/mae.py'\n",
      "adding 'mindformers/models/mae/mae_config.py'\n",
      "adding 'mindformers/models/mae/mae_modules.py'\n",
      "adding 'mindformers/models/mae/mae_processor.py'\n",
      "adding 'mindformers/models/pangualpha/__init__.py'\n",
      "adding 'mindformers/models/pangualpha/convert_weight.py'\n",
      "adding 'mindformers/models/pangualpha/pangualpha.py'\n",
      "adding 'mindformers/models/pangualpha/pangualpha_config.py'\n",
      "adding 'mindformers/models/pangualpha/pangualpha_processor.py'\n",
      "adding 'mindformers/models/pangualpha/pangualpha_tokenizer.py'\n",
      "adding 'mindformers/models/sam/__init__.py'\n",
      "adding 'mindformers/models/sam/conver_weight.py'\n",
      "adding 'mindformers/models/sam/sam.py'\n",
      "adding 'mindformers/models/sam/sam_config.py'\n",
      "adding 'mindformers/models/sam/sam_image_encoder.py'\n",
      "adding 'mindformers/models/sam/sam_layers.py'\n",
      "adding 'mindformers/models/sam/sam_mask_decoder.py'\n",
      "adding 'mindformers/models/sam/sam_processor.py'\n",
      "adding 'mindformers/models/sam/sam_prompt_encoder.py'\n",
      "adding 'mindformers/models/sam/sam_utils.py'\n",
      "adding 'mindformers/models/swin/__init__.py'\n",
      "adding 'mindformers/models/swin/convert_weight.py'\n",
      "adding 'mindformers/models/swin/swin.py'\n",
      "adding 'mindformers/models/swin/swin_config.py'\n",
      "adding 'mindformers/models/swin/swin_modules.py'\n",
      "adding 'mindformers/models/swin/swin_processor.py'\n",
      "adding 'mindformers/models/t5/__init__.py'\n",
      "adding 'mindformers/models/t5/convert_weight.py'\n",
      "adding 'mindformers/models/t5/mt5.py'\n",
      "adding 'mindformers/models/t5/t5.py'\n",
      "adding 'mindformers/models/t5/t5_config.py'\n",
      "adding 'mindformers/models/t5/t5_processor.py'\n",
      "adding 'mindformers/models/t5/t5_tokenizer.py'\n",
      "adding 'mindformers/models/t5/t5_tokenizer_fast.py'\n",
      "adding 'mindformers/models/vit/__init__.py'\n",
      "adding 'mindformers/models/vit/convert_weight.py'\n",
      "adding 'mindformers/models/vit/vit.py'\n",
      "adding 'mindformers/models/vit/vit_config.py'\n",
      "adding 'mindformers/models/vit/vit_modules.py'\n",
      "adding 'mindformers/models/vit/vit_processor.py'\n",
      "adding 'mindformers/modules/__init__.py'\n",
      "adding 'mindformers/modules/activation.py'\n",
      "adding 'mindformers/modules/kvcache_mgr.py'\n",
      "adding 'mindformers/modules/layers.py'\n",
      "adding 'mindformers/modules/local_block_sparse_attention.py'\n",
      "adding 'mindformers/modules/transformer/__init__.py'\n",
      "adding 'mindformers/modules/transformer/moe.py'\n",
      "adding 'mindformers/modules/transformer/op_parallel_config.py'\n",
      "adding 'mindformers/modules/transformer/transformer.py'\n",
      "adding 'mindformers/pet/__init__.py'\n",
      "adding 'mindformers/pet/constants.py'\n",
      "adding 'mindformers/pet/pet_config.py'\n",
      "adding 'mindformers/pet/pet_model.py'\n",
      "adding 'mindformers/pet/utils.py'\n",
      "adding 'mindformers/pet/models/__init__.py'\n",
      "adding 'mindformers/pet/models/lora.py'\n",
      "adding 'mindformers/pet/tuners/__init__.py'\n",
      "adding 'mindformers/pet/tuners/ada_adapter.py'\n",
      "adding 'mindformers/pet/tuners/adalora_adapter.py'\n",
      "adding 'mindformers/pet/tuners/lora_adapter.py'\n",
      "adding 'mindformers/pet/tuners/pet_adapter.py'\n",
      "adding 'mindformers/pet/tuners/prefix_tuning_adapter.py'\n",
      "adding 'mindformers/pet/tuners/ptuning2_adapter.py'\n",
      "adding 'mindformers/pipeline/__init__.py'\n",
      "adding 'mindformers/pipeline/base_pipeline.py'\n",
      "adding 'mindformers/pipeline/build_pipeline.py'\n",
      "adding 'mindformers/pipeline/fill_mask_pipeline.py'\n",
      "adding 'mindformers/pipeline/image_classification_pipeline.py'\n",
      "adding 'mindformers/pipeline/image_to_text_generation_pipeline.py'\n",
      "adding 'mindformers/pipeline/masked_image_modeling_pipeline.py'\n",
      "adding 'mindformers/pipeline/pipeline.py'\n",
      "adding 'mindformers/pipeline/question_answering_pipeline.py'\n",
      "adding 'mindformers/pipeline/segment_anything_pipeline.py'\n",
      "adding 'mindformers/pipeline/text_classification_pipeline.py'\n",
      "adding 'mindformers/pipeline/text_generation_pipeline.py'\n",
      "adding 'mindformers/pipeline/token_classification_pipeline.py'\n",
      "adding 'mindformers/pipeline/translation_pipeline.py'\n",
      "adding 'mindformers/pipeline/zero_shot_image_classification_pipeline.py'\n",
      "adding 'mindformers/tools/__init__.py'\n",
      "adding 'mindformers/tools/check_rules.py'\n",
      "adding 'mindformers/tools/download_tools.py'\n",
      "adding 'mindformers/tools/download_tools_multithread.py'\n",
      "adding 'mindformers/tools/export.py'\n",
      "adding 'mindformers/tools/hccl_tools.py'\n",
      "adding 'mindformers/tools/image_tools.py'\n",
      "adding 'mindformers/tools/logger.py'\n",
      "adding 'mindformers/tools/merge_hccl.py'\n",
      "adding 'mindformers/tools/moe_token_distribution_tools.py'\n",
      "adding 'mindformers/tools/transform_ckpt.py'\n",
      "adding 'mindformers/tools/utils.py'\n",
      "adding 'mindformers/tools/cloud_adapter/__init__.py'\n",
      "adding 'mindformers/tools/cloud_adapter/cloud_adapter.py'\n",
      "adding 'mindformers/tools/cloud_adapter/cloud_monitor.py'\n",
      "adding 'mindformers/tools/register/__init__.py'\n",
      "adding 'mindformers/tools/register/config.py'\n",
      "adding 'mindformers/tools/register/register.py'\n",
      "adding 'mindformers/trainer/__init__.py'\n",
      "adding 'mindformers/trainer/base_trainer.py'\n",
      "adding 'mindformers/trainer/build_trainer.py'\n",
      "adding 'mindformers/trainer/config_args.py'\n",
      "adding 'mindformers/trainer/optimizer_grouped_parameters.py'\n",
      "adding 'mindformers/trainer/trainer.py'\n",
      "adding 'mindformers/trainer/training_args.py'\n",
      "adding 'mindformers/trainer/utils.py'\n",
      "adding 'mindformers/trainer/causal_language_modeling/__init__.py'\n",
      "adding 'mindformers/trainer/causal_language_modeling/causal_language_modeling.py'\n",
      "adding 'mindformers/trainer/contrastive_language_image_pretrain/__init__.py'\n",
      "adding 'mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py'\n",
      "adding 'mindformers/trainer/general_task_trainer/__init__.py'\n",
      "adding 'mindformers/trainer/general_task_trainer/general_task_trainer.py'\n",
      "adding 'mindformers/trainer/image_classification/__init__.py'\n",
      "adding 'mindformers/trainer/image_classification/group_ic_params.py'\n",
      "adding 'mindformers/trainer/image_classification/image_classification.py'\n",
      "adding 'mindformers/trainer/image_classification/zero_shot_image_classification.py'\n",
      "adding 'mindformers/trainer/image_to_text_generation/__init__.py'\n",
      "adding 'mindformers/trainer/image_to_text_generation/image_to_text_generation.py'\n",
      "adding 'mindformers/trainer/image_to_text_retrieval/__init__.py'\n",
      "adding 'mindformers/trainer/image_to_text_retrieval/eval_utils.py'\n",
      "adding 'mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py'\n",
      "adding 'mindformers/trainer/masked_image_modeling/__init__.py'\n",
      "adding 'mindformers/trainer/masked_image_modeling/group_mim_parameters.py'\n",
      "adding 'mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py'\n",
      "adding 'mindformers/trainer/masked_language_modeling/__init__.py'\n",
      "adding 'mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py'\n",
      "adding 'mindformers/trainer/question_answering/__init__.py'\n",
      "adding 'mindformers/trainer/question_answering/question_answering.py'\n",
      "adding 'mindformers/trainer/text_classfication/__init__.py'\n",
      "adding 'mindformers/trainer/text_classfication/text_classification.py'\n",
      "adding 'mindformers/trainer/token_classification/__init__.py'\n",
      "adding 'mindformers/trainer/token_classification/token_classification.py'\n",
      "adding 'mindformers/trainer/translation/__init__.py'\n",
      "adding 'mindformers/trainer/translation/translation_finetune.py'\n",
      "adding 'mindformers/wrapper/__init__.py'\n",
      "adding 'mindformers/wrapper/adaptive_loss_scale.py'\n",
      "adding 'mindformers/wrapper/build_wrapper.py'\n",
      "adding 'mindformers/wrapper/wrapper.py'\n",
      "adding 'mindformers-0.8.0.dist-info/LICENSE'\n",
      "adding 'mindformers-0.8.0.dist-info/METADATA'\n",
      "adding 'mindformers-0.8.0.dist-info/WHEEL'\n",
      "adding 'mindformers-0.8.0.dist-info/top_level.txt'\n",
      "adding 'mindformers-0.8.0.dist-info/RECORD'\n",
      "removing build/bdist.linux-aarch64/wheel\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Processing ./mindformers-0.8.0-py3-none-any.whl\n",
      "Requirement already satisfied: ftfy>=6.1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (6.1.1)\n",
      "Requirement already satisfied: tokenizers==0.14.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (0.14.0)\n",
      "Requirement already satisfied: mdtex2html in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (1.2.0)\n",
      "Requirement already satisfied: numpy in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (1.22.0)\n",
      "Requirement already satisfied: nltk>=2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.97 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (0.1.99)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (6.0.1)\n",
      "Requirement already satisfied: rouge-chinese>=1.0.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (1.0.3)\n",
      "Requirement already satisfied: pyarrow==12.0.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (12.0.1)\n",
      "Requirement already satisfied: opencv-python-headless in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (4.8.1.78)\n",
      "Requirement already satisfied: setuptools in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (65.5.1)\n",
      "Requirement already satisfied: regex>=2022.10.31 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (2023.10.3)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (4.66.1)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (1.6.3)\n",
      "Requirement already satisfied: jieba>=0.42.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (0.42.1)\n",
      "Requirement already satisfied: mindpet==1.0.2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindformers==0.8.0) (1.0.2)\n",
      "Requirement already satisfied: click in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindpet==1.0.2->mindformers==0.8.0) (8.1.7)\n",
      "Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from tokenizers==0.14.0->mindformers==0.8.0) (0.16.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from astunparse>=1.6.3->mindformers==0.8.0) (0.38.4)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from astunparse>=1.6.3->mindformers==0.8.0) (1.16.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from ftfy>=6.1.1->mindformers==0.8.0) (0.2.6)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (23.2)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (3.12.4)\n",
      "Requirement already satisfied: fsspec in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (2023.9.2)\n",
      "Requirement already satisfied: joblib in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from nltk>=2.0->mindformers==0.8.0) (1.3.2)\n",
      "Requirement already satisfied: markdown in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mdtex2html->mindformers==0.8.0) (3.5)\n",
      "Requirement already satisfied: latex2mathml in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mdtex2html->mindformers==0.8.0) (3.76.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from markdown->mdtex2html->mindformers==0.8.0) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->mdtex2html->mindformers==0.8.0) (3.17.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14.0->mindformers==0.8.0) (2.10)\n",
      "mindformers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "---------------- MindFormers: build and install end   ----------------\n"
     ]
    }
   ],
   "source": [
    "#进入mindformers/setup.py文件中修改版本为0.8.0,修改requirements.txt文件的mindpet==1.0.2,tokenizers==0.14.0\n",
    "#开始安装\n",
    "# %cd mindformers\n",
    "!bash build.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4685cb44-ea43-4525-8daf-d045a369ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb4fe8-e28d-46c7-836b-2d47c7278a5c",
   "metadata": {},
   "source": [
    "## 获取MindFormers提供的已转换权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8a2068-12f3-4e9e-a9af-28fa92d66d76",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-23 16:46:45--  https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/llama/open_llama_7b.ckpt\n",
      "Resolving proxy-notebook.modelarts.com (proxy-notebook.modelarts.com)... 192.168.0.106\n",
      "Connecting to proxy-notebook.modelarts.com (proxy-notebook.modelarts.com)|192.168.0.106|:8083... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 13476850247 (13G) [binary/octet-stream]\n",
      "Saving to: ‘open_llama_7b.ckpt’\n",
      "\n",
      "open_llama_7b.ckpt  100%[===================>]  12.55G  83.9MB/s    in 2m 17s  \n",
      "\n",
      "2024-01-23 16:49:03 (93.6 MB/s) - ‘open_llama_7b.ckpt’ saved [13476850247/13476850247]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/llama/open_llama_7b.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee30037a-349a-4479-a194-02f4b94d72a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-23 16:49:39--  https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/llama/tokenizer.model\n",
      "Resolving proxy-notebook.modelarts.com (proxy-notebook.modelarts.com)... 192.168.0.106\n",
      "Connecting to proxy-notebook.modelarts.com (proxy-notebook.modelarts.com)|192.168.0.106|:8083... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 534194 (522K) [binary/octet-stream]\n",
      "Saving to: ‘tokenizer.model’\n",
      "\n",
      "tokenizer.model     100%[===================>] 521.67K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-01-23 16:49:39 (3.74 MB/s) - ‘tokenizer.model’ saved [534194/534194]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer文件\n",
    "!wget https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/llama/tokenizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36253e5-b162-4f8b-8c0f-d38aec91ed9d",
   "metadata": {},
   "source": [
    "## 微调\n",
    "数据集准备-微调\n",
    "目前提供alpaca数据集的预处理脚本用于全参微调/lora微调任务。\n",
    "\n",
    "数据集下载链接如下：\n",
    "https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json\n",
    "\n",
    "alpaca数据集原始格式样例：\n",
    "\n",
    "```json\n",
    "# alpaca examples:\n",
    "    {\n",
    "        \"instruction\": \"Describe a time when you had to make a difficult decision.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client\\u2019s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team\\u2019s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client\\u2019s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Identify the odd one out.\",\n",
    "        \"input\": \"Twitter, Instagram, Telegram\",\n",
    "        \"output\": \"Telegram\"\n",
    "    },\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ff22866-9e4d-4af7-8432-352f96c3f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1. 执行alpaca_converter.py，使用fastchat工具添加prompts模板，将原始数据集转换为多轮对话格式。\n",
    "# 脚本路径：tools/dataset_preprocess/llama/alpaca_converter.py\n",
    "# 执行转换脚本\n",
    "!python mindformers/mindformers/tools/dataset_preprocess/llama/alpaca_converter.py --data_path alpaca_data.json --output_path alpaca-data-conversation.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c869ffff-abc6-41bd-8aab-949e63bc959a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Collecting fschat==0.2.13\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/fschat/0.2.13/fschat-0.2.13-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nh3\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/nh3/0.2.15/nh3-0.2.15-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/httpx/0.26.0/httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from fschat==0.2.13) (1.26.1)\n",
      "Requirement already satisfied: requests in /home/ma-user/modelarts-dev/modelarts-sdk (from fschat==0.2.13) (2.28.1)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.0 in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from fschat==0.2.13) (3.0.39)\n",
      "Collecting sentencepiece\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/sentencepiece/0.1.99/sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting shortuuid\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/shortuuid/1.0.11/shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting pydantic\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/pydantic/2.5.3/pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/uvicorn/0.27.0/uvicorn-0.27.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/wandb/0.16.2/wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers<4.29.0,>=4.28.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/transformers/4.28.1/transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown2[all]\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/markdown2/2.4.12/markdown2-2.4.12-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/fastapi/0.109.0/fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/accelerate/0.26.1/accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich>=10.0.0 in /home/ma-user/modelarts-dev/ma-cli (from fschat==0.2.13) (13.5.2)\n",
      "Collecting tokenizers>=0.12.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/tokenizers/0.15.1/tokenizers-0.15.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/torch/2.1.2/torch-2.1.2-cp39-cp39-manylinux2014_aarch64.whl (84.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gradio==3.23\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/gradio/3.23.0/gradio-3.23.0-py3-none-any.whl (15.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/tiktoken/0.5.2/tiktoken-0.5.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/fsspec/2023.12.2/fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.13.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/huggingface-hub/0.20.3/huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.13) (4.8.0)\n",
      "Requirement already satisfied: pyyaml in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.13) (6.0.1)\n",
      "Requirement already satisfied: jinja2 in /home/ma-user/modelarts-dev/ma-cli (from gradio==3.23->fschat==0.2.13) (3.1.2)\n",
      "Requirement already satisfied: markupsafe in /home/ma-user/modelarts-dev/ma-cli (from gradio==3.23->fschat==0.2.13) (2.1.3)\n",
      "Requirement already satisfied: pandas in /home/ma-user/modelarts-dev/modelarts-sdk (from gradio==3.23->fschat==0.2.13) (1.3.5)\n",
      "Collecting pydub\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/pydub/0.25.1/pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting aiofiles\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/aiofiles/23.2.1/aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting python-multipart\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/python-multipart/0.0.6/python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/websockets/12.0/websockets-12.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/orjson/3.9.12/orjson-3.9.12-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.4/142.4 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/aiohttp/3.9.1/aiohttp-3.9.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdit-py-plugins<=0.3.3\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/mdit-py-plugins/0.3.3/mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /home/ma-user/modelarts-dev/ma-cli (from gradio==3.23->fschat==0.2.13) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/ma-user/modelarts-dev/modelarts-sdk (from gradio==3.23->fschat==0.2.13) (3.5.2)\n",
      "Requirement already satisfied: semantic-version in /home/ma-user/modelarts-dev/modelarts-sdk (from gradio==3.23->fschat==0.2.13) (2.10.0)\n",
      "Requirement already satisfied: pillow in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from gradio==3.23->fschat==0.2.13) (10.1.0)\n",
      "Collecting ffmpy\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/ffmpy/0.3.1/ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting altair>=4.2.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/altair/5.2.0/altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.13) (0.2.6)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.13) (2.16.1)\n",
      "Collecting filelock\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/filelock/3.13.1/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/regex/2023.12.25/regex-2023.12.25-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.6/773.6 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ma-user/modelarts-dev/modelarts-sdk (from transformers<4.29.0,>=4.28.0->fschat==0.2.13) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ma-user/modelarts-dev/modelarts-sdk (from transformers<4.29.0,>=4.28.0->fschat==0.2.13) (4.64.1)\n",
      "Collecting tokenizers>=0.12.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/tokenizers/0.13.3/tokenizers-0.13.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/safetensors/0.4.1/safetensors-0.4.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from accelerate->fschat==0.2.13) (5.9.6)\n",
      "Requirement already satisfied: networkx in /home/ma-user/modelarts-dev/modelarts-sdk (from torch->fschat==0.2.13) (2.6.3)\n",
      "Collecting sympy\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/sympy/1.12/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.36.0,>=0.35.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/starlette/0.35.1/starlette-0.35.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.14.6\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/pydantic-core/2.14.6/pydantic_core-2.14.6-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/annotated-types/0.6.0/annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/httpcore/1.0.2/httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /home/ma-user/modelarts-dev/modelarts-sdk (from httpx->fschat==0.2.13) (2.10)\n",
      "Collecting sniffio\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/sniffio/1.3.0/sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting anyio\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/anyio/4.2.0/anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from httpx->fschat==0.2.13) (2023.7.22)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/h11/0.14.0/h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wavedrom\n",
      "  Using cached wavedrom-2.0.3.post3-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/urllib3/1.26.18/urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /home/ma-user/modelarts-dev/modelarts-sdk (from requests->fschat==0.2.13) (2.0.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/ma-user/modelarts-dev/common-algo-toolkit (from uvicorn->fschat==0.2.13) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from wandb->fschat==0.2.13) (3.20.3)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/gitpython/3.1.41/GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/sentry-sdk/1.39.2/sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/setproctitle/1.3.3/setproctitle-1.3.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/docker-pycreds/0.4.0/docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/appdirs/1.4.4/appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: setuptools in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from wandb->fschat==0.2.13) (65.5.1)\n",
      "Collecting jsonschema>=3.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/jsonschema/4.21.1/jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toolz\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/toolz/0.12.0/toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb->fschat==0.2.13) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/gitdb/4.0.11/gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /home/ma-user/modelarts-dev/ma-cli (from markdown-it-py[linkify]>=2.0.0->gradio==3.23->fschat==0.2.13) (0.1.2)\n",
      "Collecting linkify-it-py<3,>=1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/linkify-it-py/2.0.2/linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ma-user/modelarts-dev/modelarts-sdk (from pandas->gradio==3.23->fschat==0.2.13) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from pandas->gradio==3.23->fschat==0.2.13) (2.8.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ma-user/anaconda3/envs/python-3.9.10/lib/python3.9/site-packages (from anyio->httpx->fschat==0.2.13) (1.1.3)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/yarl/1.9.4/yarl-1.9.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.9/300.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/async-timeout/4.0.3/async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/frozenlist/1.4.1/frozenlist-1.4.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/ma-user/modelarts-dev/modelarts-sdk (from aiohttp->gradio==3.23->fschat==0.2.13) (21.4.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/multidict/6.0.4/multidict-6.0.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/aiosignal/1.3.1/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ma-user/modelarts-dev/modelarts-sdk (from matplotlib->gradio==3.23->fschat==0.2.13) (1.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ma-user/modelarts-dev/modelarts-sdk (from matplotlib->gradio==3.23->fschat==0.2.13) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ma-user/modelarts-dev/modelarts-sdk (from matplotlib->gradio==3.23->fschat==0.2.13) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ma-user/modelarts-dev/modelarts-sdk (from matplotlib->gradio==3.23->fschat==0.2.13) (3.0.9)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/mpmath/1.3.0/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting svgwrite\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/svgwrite/1.4.3/svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/smmap/5.0.1/smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/jsonschema-specifications/2023.12.1/jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/attrs/23.2.0/attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting referencing>=0.28.4\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/referencing/0.32.1/referencing-0.32.1-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/rpds-py/0.17.1/rpds_py-0.17.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uc-micro-py\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/uc-micro-py/1.0.2/uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5580 sha256=f1f0fa235fc4225c73fa62fb72569ffac6d0d53927f02a04142939489a767535\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/64/a7/7d/8c845cffda3981dc0e11c1fbd200e410c5e6802a1071577605\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: tokenizers, sentencepiece, pydub, nh3, mpmath, ffmpy, appdirs, websockets, urllib3, uc-micro-py, toolz, sympy, svgwrite, sniffio, smmap, shortuuid, setproctitle, safetensors, rpds-py, regex, python-multipart, pydantic-core, orjson, multidict, markdown2, h11, fsspec, frozenlist, filelock, docker-pycreds, attrs, async-timeout, annotated-types, aiofiles, yarl, wavedrom, uvicorn, torch, sentry-sdk, referencing, pydantic, mdit-py-plugins, linkify-it-py, httpcore, gitdb, anyio, aiosignal, tiktoken, starlette, jsonschema-specifications, huggingface-hub, httpx, GitPython, aiohttp, wandb, transformers, jsonschema, fastapi, accelerate, altair, gradio, fschat\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "te 0.4.0 requires cloudpickle, which is not installed.\n",
      "te 0.4.0 requires synr==0.5.0, which is not installed.\n",
      "modelarts 1.4.18 requires attrs==21.4.0, but you have attrs 23.2.0 which is incompatible.\n",
      "modelarts 1.4.18 requires lxml<=4.9.1, but you have lxml 4.9.3 which is incompatible.\n",
      "modelarts 1.4.18 requires Pillow<=9.3.0, but you have pillow 10.1.0 which is incompatible.\n",
      "modelarts 1.4.18 requires psutil<=5.9.2, but you have psutil 5.9.6 which is incompatible.\n",
      "modelarts 1.4.18 requires pyyaml<=6.0, but you have pyyaml 6.0.1 which is incompatible.\n",
      "modelarts 1.4.18 requires typing-extensions<=4.0.1, but you have typing-extensions 4.8.0 which is incompatible.\n",
      "modelarts 1.4.18 requires urllib3<=1.26.13, but you have urllib3 1.26.18 which is incompatible.\n",
      "ma-cau 1.1.4.9 requires matplotlib==3.5.1, but you have matplotlib 3.5.2 which is incompatible.\n",
      "ma-cau 1.1.4.9 requires numpy<=1.22.1, but you have numpy 1.26.1 which is incompatible.\n",
      "ma-cau 1.1.4.9 requires Pillow==9.5.0, but you have pillow 10.1.0 which is incompatible.\n",
      "ma-cau 1.1.4.9 requires tqdm==4.65.0, but you have tqdm 4.64.1 which is incompatible.\n",
      "ma-cau 1.1.4.9 requires typing-extensions<=4.0.1, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed GitPython-3.1.41 accelerate-0.26.1 aiofiles-23.2.1 aiohttp-3.9.1 aiosignal-1.3.1 altair-5.2.0 annotated-types-0.6.0 anyio-4.2.0 appdirs-1.4.4 async-timeout-4.0.3 attrs-23.2.0 docker-pycreds-0.4.0 fastapi-0.109.0 ffmpy-0.3.1 filelock-3.13.1 frozenlist-1.4.1 fschat-0.2.13 fsspec-2023.12.2 gitdb-4.0.11 gradio-3.23.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.20.3 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 linkify-it-py-2.0.2 markdown2-2.4.12 mdit-py-plugins-0.3.3 mpmath-1.3.0 multidict-6.0.4 nh3-0.2.15 orjson-3.9.12 pydantic-2.5.3 pydantic-core-2.14.6 pydub-0.25.1 python-multipart-0.0.6 referencing-0.32.1 regex-2023.12.25 rpds-py-0.17.1 safetensors-0.4.1 sentencepiece-0.1.99 sentry-sdk-1.39.2 setproctitle-1.3.3 shortuuid-1.0.11 smmap-5.0.1 sniffio-1.3.0 starlette-0.35.1 svgwrite-1.4.3 sympy-1.12 tiktoken-0.5.2 tokenizers-0.13.3 toolz-0.12.0 torch-2.1.2 transformers-4.28.1 uc-micro-py-1.0.2 urllib3-1.26.18 uvicorn-0.27.0 wandb-0.16.2 wavedrom-2.0.3.post3 websockets-12.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Collecting tk\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/tk/0.1.0/tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
      "Installing collected packages: tk\n",
      "Successfully installed tk-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fschat==0.2.13\n",
    "%pip install tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "142a4670-7bcb-4989-ac8c-493b50d1cb94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ma-user/work/mindformers/mindformers/tools/dataset_preprocess/llama/llama_preprocess.py\", line 25, in <module>\n",
      "    from mindspore.mindrecord import FileWriter\n",
      "ModuleNotFoundError: No module named 'mindspore'\n"
     ]
    }
   ],
   "source": [
    "#step 2. 执行llama_preprocess.py，进行数据预处理、Mindrecord数据生成，将带有prompt模板的数据转换为mindrecord格式。\n",
    "\n",
    "#注：由于此工具依赖fschat工具包解析prompt模板，请提前安装fschat >= 0.2.13 python = 3.9\n",
    "\n",
    "# 脚本路径：tools/dataset_preprocess/llama/llama_preprocess.py\n",
    "!python mindformers/mindformers/tools/dataset_preprocess/llama/llama_preprocess.py  --dataset_type qa --input_glob alpaca-data-conversation.json  --model_file tokenizer.model --seq_length 2048 --output_file alpaca-fastchat2048.mindrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a1e53-ae3e-42e7-9f3d-4716f57b6e7a",
   "metadata": {},
   "source": [
    "## lora微调\n",
    "\n",
    "step 1. 修改config/llama/run_llama_7b.yaml中训练数据集路径为微调数据集路径,并在input_columns中添加labels\n",
    "\n",
    "``` yaml\n",
    "train_dataset: &train_dataset\n",
    "  data_loader:\n",
    "    type: MindDataset\n",
    "    dataset_dir: \"/{path}/alpaca-fastchat2048.mindrecord\"\n",
    "    shuffle: True\n",
    "  input_columns: [\"input_ids\", \"labels\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99e76c-5f8c-4b87-b518-b396b978269d",
   "metadata": {},
   "source": [
    "step 2. 启动lora微调任务。\n",
    "\n",
    "注：llama_7b_lora模型支持单卡启动，需将配置文件中的use_parallel参数置为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8d8efe-6239-45c5-83c3-8349502c38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir is /home/ma-user/work/mindformers/output\n",
      "start training for device 0\n",
      "log saved in /home/ma-user/work/mindformers/output/log/rank_0\n"
     ]
    }
   ],
   "source": [
    "# %cd mindformers/scripts\n",
    "# 单卡启动\n",
    "!bash run_standalone.sh ../configs/llama/run_llama_7b_lora.yaml 0 finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4630279-4a58-4911-ba9c-76efb91acf45",
   "metadata": {},
   "source": [
    "## 评测\n",
    "文本生成\n",
    "\n",
    "step 1. 获取数据集\n",
    "\n",
    "WikiText2数据集是从维基百科上经过验证的优质文章集中提取的超过1亿个token的集合。(https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623f9f4-b522-4677-b9ae-0a2c0b5fa394",
   "metadata": {},
   "source": [
    "step 2. 处理数据成mindrecord格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fce6ee2-69b1-4b1d-9603-fc7a68c7f9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/mindformers\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7caf2eb9-7264-4a22-8515-4764f96b31e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \\L\n",
      "  if not dirpath.find(\"AppData\\Local\\Temp\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \\B\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(__version__) >= LooseVersion(\"1.17.0\"):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ma-user/work/mindformers/mindformers/tools/dataset_preprocess/llama/llama_preprocess.py\", line 27, in <module>\n",
      "    from mindformers.models.llama.llama_tokenizer import LlamaTokenizer\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/__init__.py\", line 17, in <module>\n",
      "    from mindformers import core, auto_class, dataset, \\\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/core/__init__.py\", line 19, in <module>\n",
      "    from .metric import build_metric\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/core/metric/__init__.py\", line 17, in <module>\n",
      "    from .metric import *\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/core/metric/metric.py\", line 37, in <module>\n",
      "    from mindformers.models import BasicTokenizer\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/models/__init__.py\", line 21, in <module>\n",
      "    from .blip2 import *\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/models/blip2/__init__.py\", line 17, in <module>\n",
      "    from .blip2_config import Blip2Config\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/models/blip2/blip2_config.py\", line 23, in <module>\n",
      "    from mindformers.models.llama import LlamaConfig\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/models/llama/__init__.py\", line 18, in <module>\n",
      "    from .llama import LlamaForCausalLM, LlamaForCausalLMWithLora, LlamaModel\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/models/llama/llama.py\", line 41, in <module>\n",
      "    from mindformers.pet.tuners.pet_adapter import PetAdapter\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/pet/__init__.py\", line 18, in <module>\n",
      "    from .tuners import *\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/pet/tuners/__init__.py\", line 17, in <module>\n",
      "    from .adalora_adapter import *\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/pet/tuners/adalora_adapter.py\", line 21, in <module>\n",
      "    from .pet_adapter import PetAdapter\n",
      "  File \"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/pet/tuners/pet_adapter.py\", line 22, in <module>\n",
      "    from tk.graph.freeze_utils import freeze_delta\n",
      "ModuleNotFoundError: No module named 'tk.graph'\n"
     ]
    }
   ],
   "source": [
    "# 使用tools/dataset_preprocess/llama/llama_preprocess.py进行数据预处理+Mindrecord数据生成\n",
    "!python /home/ma-user/work/mindformers/mindformers/tools/dataset_preprocess/llama/llama_preprocess.py \\\n",
    "--dataset_type wiki \\\n",
    "--input_glob  /home/ma-user/work/wikitext-2/wiki.valid.tokens \\\n",
    "--model_file /home/ma-user/work/tokenizer.model \\\n",
    "--seq_length 2047 \\\n",
    "--output_file /home/ma-user/work/wiki2048.mindrecord\n",
    "\n",
    "#因为我这里已经安装过mindformers以及安装了一些冲突的依赖，所以我在启智社区重新创建了一个云脑任务，在云脑中，只安装mindformers，\n",
    "#可以成功转换,重新创建一个云脑，依次运行以下代码，就不会依赖冲突。\n",
    "#git clone -b dev https://gitee.com/mindspore/mindformers.git\n",
    "# cd mindformers\n",
    "# bash build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa6df4-92bc-4f5b-918a-b720bdfca09e",
   "metadata": {},
   "source": [
    "step 3. 开启评测，指标为PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b39a927-59a7-4f41-bdf0-51f7be5f8901",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Collecting pyarrow\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/pyarrow/15.0.0/pyarrow-15.0.0-cp39-cp39-manylinux_2_28_aarch64.whl (35.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 35.6 MB 69.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16.6 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pyarrow) (1.22.0)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-15.0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/MindSpore/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f5309-538c-4560-9f7a-37e24262f34c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \\L\n",
      "  if not dirpath.find(\"AppData\\Local\\Temp\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \\B\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(__version__) >= LooseVersion(\"1.17.0\"):\n",
      "2024-01-23 23:52:08,691 - mindformers[mindformers/tools/utils.py:153] - INFO - set output path to '/home/ma-user/work/output'\n",
      "2024-01-23 23:52:08,693 - mindformers[mindformers/run_mindformer.py:109] - INFO - .........Build context config..........\n",
      "2024-01-23 23:52:08,693 - mindformers[mindformers/core/parallel_config.py:45] - INFO - initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}\n",
      "2024-01-23 23:52:08,693 - mindformers[mindformers/core/parallel_config.py:51] - INFO - initial parallel_config from dict: {'data_parallel': 2, 'model_parallel': 1, 'pipeline_stage': 4, 'use_seq_parallel': False, 'micro_batch_num': 16, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}\n",
      "2024-01-23 23:52:08,693 - mindformers[mindformers/core/parallel_config.py:53] - INFO - pipeline_stage = 4 > 1, vocab_emd_dp will be reset to False.\n",
      "2024-01-23 23:52:08,695 - mindformers[mindformers/run_mindformer.py:111] - INFO - context config is: [ParallelConfig]\n",
      "_recompute:[ParallelConfig]\n",
      "_recompute:True\n",
      "_select_recompute:False\n",
      "_parallel_optimizer_comm_recompute:False\n",
      "_mp_comm_recompute:True\n",
      "_recompute_slice_activation:True\n",
      "\n",
      "select_recompute:False\n",
      "use_seq_parallel:False\n",
      "_gradient_aggregation_group:4\n",
      "_embed_dp_mp_config:[ParallelConfig]\n",
      "_dp_mp_config:[ParallelConfig]\n",
      "_data_parallel:2\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_vocab_emb_dp:False\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_pp_config:[ParallelConfig]\n",
      "_pipeline_stage:4\n",
      "_micro_batch_num:16\n",
      "\n",
      "_moe_config:[ParallelConfig]\n",
      "_dpmp:[ParallelConfig]\n",
      "_data_parallel:2\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_expert_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "\n",
      "2024-01-23 23:52:08,695 - mindformers[mindformers/run_mindformer.py:112] - INFO - moe config is: <mindformers.modules.transformer.moe.MoEConfig object at 0xffff2068dd60>\n",
      "2024-01-23 23:52:08,696 - mindformers[mindformers/trainer/base_trainer.py:85] - INFO - Now Running Task is: text_generation, Model is: llama_7b\n",
      "2024-01-23 23:52:08,696 - mindformers[mindformers/trainer/base_trainer.py:228] - INFO - The current parallel mode is stand_alone, batch size per card will not be changed: batch_size_per_card = 4\n",
      "2024-01-23 23:52:08,696 - mindformers[mindformers/trainer/base_trainer.py:232] - INFO - global_batch_size = batch_size_per_card * device_num * gradient_accumulation_steps = 4 = 4 * 1 * 1\n",
      "2024-01-23 23:52:08,697 - mindformers[mindformers/trainer/base_trainer.py:241] - INFO - parallel_config will be change to default config: [ParallelConfig]\n",
      "_recompute:[ParallelConfig]\n",
      "_recompute:True\n",
      "_select_recompute:False\n",
      "_parallel_optimizer_comm_recompute:False\n",
      "_mp_comm_recompute:True\n",
      "_recompute_slice_activation:True\n",
      "\n",
      "select_recompute:False\n",
      "use_seq_parallel:False\n",
      "_gradient_aggregation_group:4\n",
      "_embed_dp_mp_config:[ParallelConfig]\n",
      "_dp_mp_config:[ParallelConfig]\n",
      "_data_parallel:1\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_vocab_emb_dp:False\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_pp_config:[ParallelConfig]\n",
      "_pipeline_stage:1\n",
      "_micro_batch_num:1\n",
      "\n",
      "_moe_config:[ParallelConfig]\n",
      "_dpmp:[ParallelConfig]\n",
      "_data_parallel:1\n",
      "_model_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      "_expert_parallel:1\n",
      "use_seq_parallel:False\n",
      "select_recompute:False\n",
      "\n",
      ".\n",
      "2024-01-23 23:52:08,697 - mindformers[mindformers/trainer/base_trainer.py:755] - INFO - .........Build Dataset For Evaluate..........\n",
      "2024-01-23 23:52:08,697 - mindformers[mindformers/trainer/base_trainer.py:355] - INFO - .........Build Dataset From Config..........\n",
      "2024-01-23 23:52:08,697 - mindformers[mindformers/trainer/base_trainer.py:375] - INFO - For evaluate phase, batch size for eval dataset is 4, different from training, not multiplied by micro_batch_num, micro_batch_interleave_num and gradient_accumulation_steps\n",
      "2024-01-23 23:52:08,698 - mindformers[mindformers/dataset/causal_language_model_dataset.py:166] - INFO - Now Create Causal Language Model Dataset.\n",
      "2024-01-23 23:52:08,705 - mindformers[mindformers/trainer/base_trainer.py:759] - INFO - Create evaluate dataset finish, dataset size:29\n",
      "2024-01-23 23:52:08,708 - mindformers[mindformers/trainer/base_trainer.py:383] - INFO - .........Build Network From Config..........\n",
      "2024-01-23 23:52:08,709 - mindformers[mindformers/models/llama/llama_config.py:174] - WARNING - Argument `pretrain_seqlen` is deprecated. Use `scaling_factor` instead.\n",
      "2024-01-23 23:52:08,709 - mindformers[mindformers/models/llama/llama_config.py:177] - WARNING - Argument `compute_in_2d` is deprecated.\n",
      "2024-01-23 23:52:08,709 - mindformers[mindformers/models/llama/llama_config.py:180] - WARNING - Argument `use_past_shard` is deprecated.\n",
      "2024-01-23 23:52:08,709 - mindformers[mindformers/version_control.py:60] - INFO - The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1\n",
      "2024-01-23 23:52:08,709 - mindformers[mindformers/version_control.py:64] - INFO - \n",
      "The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: \n",
      "export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.\n",
      "2024-01-23 23:52:08,709 - mindformers[mindformers/version_control.py:70] - INFO - The Cell Reuse compilation acceleration feature does not support single-card mode.This feature is disabled by default. ENABLE_CELL_REUSE=1 does not take effect.\n",
      "2024-01-23 23:52:08,710 - mindformers[mindformers/version_control.py:73] - INFO - The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-23-23:52:09.274.620 [mindspore/ops/primitive.py:228] The in_strategy of the operator in your network will not take effect in stand_alone mode. This means the the shard function called in the network is ignored. \n",
      "If you want to enable it, please use semi auto or auto parallel mode by context.set_auto_parallel_context(parallel_mode=ParallelMode.SEMI_AUTO_PARALLEL or context.set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL)\n",
      "2024-01-23 23:52:10,898 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:10,901 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:12,410 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:13,791 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-23-23:52:13.793.494 [mindspore/common/parameter.py:786] This interface may be deleted in the future.\n",
      "2024-01-23 23:52:13,797 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:13,800 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:15,437 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:16,821 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:16,828 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:16,831 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:18,326 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:19,487 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:19,495 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:19,498 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:21,014 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:22,192 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:22,198 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:22,201 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:23,728 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:24,897 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:24,903 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:24,905 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:26,531 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:27,780 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:27,786 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:27,789 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:29,385 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:30,716 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:30,721 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:30,724 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:32,412 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:33,713 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:33,719 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:33,721 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:35,319 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:36,620 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:36,625 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:36,628 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:38,330 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:39,563 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:39,572 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:39,575 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:41,315 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:42,692 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:42,698 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:42,701 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:44,309 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:45,719 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:45,725 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:45,727 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:47,428 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:48,889 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:48,894 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:48,900 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:50,560 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:51,825 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:51,830 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:51,833 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:53,490 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:54,913 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:54,920 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:54,923 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:56,567 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:57,998 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:52:58,005 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:58,007 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:52:59,703 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:01,068 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:01,074 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:01,076 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:02,705 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:03,977 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:03,982 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:03,985 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:05,555 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:06,965 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:06,970 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:06,973 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:08,643 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:10,011 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:10,018 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:10,020 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:11,672 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:13,046 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:13,051 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:13,054 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:14,621 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:15,991 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:15,997 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:16,000 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:17,723 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:19,004 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:19,010 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:19,012 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:20,600 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:22,013 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:22,019 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:22,021 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:23,736 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:24,926 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:24,932 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:24,934 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:26,577 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:27,957 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:27,963 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:27,965 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:29,531 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:30,702 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:30,708 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:30,710 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:32,428 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:33,637 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:33,643 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:33,645 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:35,219 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:36,481 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:36,487 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:36,489 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:38,148 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:39,456 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:39,462 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:39,464 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:41,030 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:42,232 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:42,238 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:42,240 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:43,891 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:45,282 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.\n",
      "2024-01-23 23:53:45,287 - mindformers[mindformers/version_control.py:212] - WARNING - Current MindSpore do not support big kernel SiLU and RMSNorm, please upgrade to 2.2.10 or later version.\n",
      "2024-01-23 23:53:47,782 - mindformers[mindformers/tools/download_tools.py:95] - INFO - Start download ./checkpoint_download/llama/llama_7b.ckpt\n",
      "Downloading: 13.5GB [05:24, 41.6MB/s]                                                               \n",
      "2024-01-23 23:59:12,097 - mindformers[mindformers/tools/download_tools.py:108] - INFO - Download completed!, times: 324.93s\n",
      "2024-01-23 23:59:12,098 - mindformers[mindformers/models/base_model.py:110] - INFO - start to read the ckpt file: 13476850247\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:23.992.817 [mindspore/train/serialization.py:172] The type of model.layers.0.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:23.994.104 [mindspore/train/serialization.py:172] The type of model.layers.0.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:25.202.424 [mindspore/train/serialization.py:172] The type of model.layers.1.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:25.203.554 [mindspore/train/serialization.py:172] The type of model.layers.1.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:26.403.891 [mindspore/train/serialization.py:172] The type of model.layers.2.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:26.405.016 [mindspore/train/serialization.py:172] The type of model.layers.2.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:27.610.749 [mindspore/train/serialization.py:172] The type of model.layers.3.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:27.611.815 [mindspore/train/serialization.py:172] The type of model.layers.3.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:28.841.752 [mindspore/train/serialization.py:172] The type of model.layers.4.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:28.842.819 [mindspore/train/serialization.py:172] The type of model.layers.4.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:30.657.09 [mindspore/train/serialization.py:172] The type of model.layers.5.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:30.667.89 [mindspore/train/serialization.py:172] The type of model.layers.5.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:31.221.651 [mindspore/train/serialization.py:172] The type of model.layers.6.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:31.222.728 [mindspore/train/serialization.py:172] The type of model.layers.6.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:32.407.539 [mindspore/train/serialization.py:172] The type of model.layers.7.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:32.408.646 [mindspore/train/serialization.py:172] The type of model.layers.7.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:33.609.263 [mindspore/train/serialization.py:172] The type of model.layers.8.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:33.610.320 [mindspore/train/serialization.py:172] The type of model.layers.8.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:34.803.125 [mindspore/train/serialization.py:172] The type of model.layers.9.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:34.804.247 [mindspore/train/serialization.py:172] The type of model.layers.9.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:35.984.753 [mindspore/train/serialization.py:172] The type of model.layers.10.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:35.985.842 [mindspore/train/serialization.py:172] The type of model.layers.10.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:37.164.767 [mindspore/train/serialization.py:172] The type of model.layers.11.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:37.165.817 [mindspore/train/serialization.py:172] The type of model.layers.11.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:38.392.099 [mindspore/train/serialization.py:172] The type of model.layers.12.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:38.393.213 [mindspore/train/serialization.py:172] The type of model.layers.12.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:39.626.615 [mindspore/train/serialization.py:172] The type of model.layers.13.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:39.627.723 [mindspore/train/serialization.py:172] The type of model.layers.13.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:40.196.447 [mindspore/train/serialization.py:172] The type of model.layers.14.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:40.197.550 [mindspore/train/serialization.py:172] The type of model.layers.14.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:40.656.706 [mindspore/train/serialization.py:172] The type of model.layers.15.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:40.657.669 [mindspore/train/serialization.py:172] The type of model.layers.15.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:41.115.330 [mindspore/train/serialization.py:172] The type of model.layers.16.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:41.116.347 [mindspore/train/serialization.py:172] The type of model.layers.16.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:41.572.988 [mindspore/train/serialization.py:172] The type of model.layers.17.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:41.573.941 [mindspore/train/serialization.py:172] The type of model.layers.17.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:42.326.99 [mindspore/train/serialization.py:172] The type of model.layers.18.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:42.337.08 [mindspore/train/serialization.py:172] The type of model.layers.18.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:42.493.604 [mindspore/train/serialization.py:172] The type of model.layers.19.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:42.494.612 [mindspore/train/serialization.py:172] The type of model.layers.19.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:42.957.078 [mindspore/train/serialization.py:172] The type of model.layers.20.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:42.958.031 [mindspore/train/serialization.py:172] The type of model.layers.20.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:43.479.408 [mindspore/train/serialization.py:172] The type of model.layers.21.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:43.480.385 [mindspore/train/serialization.py:172] The type of model.layers.21.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:43.932.059 [mindspore/train/serialization.py:172] The type of model.layers.22.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:43.933.043 [mindspore/train/serialization.py:172] The type of model.layers.22.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:44.391.337 [mindspore/train/serialization.py:172] The type of model.layers.23.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:44.392.315 [mindspore/train/serialization.py:172] The type of model.layers.23.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:44.888.559 [mindspore/train/serialization.py:172] The type of model.layers.24.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:44.889.576 [mindspore/train/serialization.py:172] The type of model.layers.24.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:45.535.880 [mindspore/train/serialization.py:172] The type of model.layers.25.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:45.536.890 [mindspore/train/serialization.py:172] The type of model.layers.25.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:46.202.582 [mindspore/train/serialization.py:172] The type of model.layers.26.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:46.203.525 [mindspore/train/serialization.py:172] The type of model.layers.26.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:46.867.289 [mindspore/train/serialization.py:172] The type of model.layers.27.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:46.868.285 [mindspore/train/serialization.py:172] The type of model.layers.27.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:47.530.127 [mindspore/train/serialization.py:172] The type of model.layers.28.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:47.531.103 [mindspore/train/serialization.py:172] The type of model.layers.28.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:48.194.917 [mindspore/train/serialization.py:172] The type of model.layers.29.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:48.195.887 [mindspore/train/serialization.py:172] The type of model.layers.29.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:48.859.946 [mindspore/train/serialization.py:172] The type of model.layers.30.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:48.860.988 [mindspore/train/serialization.py:172] The type of model.layers.30.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:49.531.772 [mindspore/train/serialization.py:172] The type of model.layers.31.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:49.532.746 [mindspore/train/serialization.py:172] The type of model.layers.31.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:50.195.532 [mindspore/train/serialization.py:172] The type of model.norm_out.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.\n",
      "2024-01-24 00:00:50,617 - mindformers[mindformers/models/base_model.py:115] - INFO - weights in ./checkpoint_download/llama/llama_7b.ckpt are loaded\n",
      "2024-01-24 00:00:50,643 - mindformers[mindformers/trainer/base_trainer.py:527] - INFO - Network Parameters: 6738 M.\n",
      "2024-01-24 00:00:50,643 - mindformers[mindformers/trainer/base_trainer.py:777] - INFO - .........Build Compute Metrics For Evaluate..........\n",
      "2024-01-24 00:00:50,656 - mindformers[mindformers/trainer/base_trainer.py:782] - INFO - .........Build Callbacks For Evaluate..........\n",
      "2024-01-24 00:00:50,656 - mindformers[mindformers/trainer/base_trainer.py:511] - INFO - .........Build Callbacks for Evaluate From Config..........\n",
      "2024-01-24 00:00:50,657 - mindformers[mindformers/trainer/base_trainer.py:791] - INFO - .........Starting Init Evaluate Model..........\n",
      "2024-01-24 00:00:50,658 - mindformers[mindformers/trainer/base_trainer.py:801] - INFO - .........Starting Evaluate Model..........\n",
      "{'auto_trans_ckpt': False,\n",
      " 'auto_tune': False,\n",
      " 'autotune_per_step': 10,\n",
      " 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),\n",
      "               OrderedDict([('type', 'CheckpointMointor'),\n",
      "                            ('prefix', 'llama_7b'),\n",
      "                            ('save_checkpoint_steps', 100),\n",
      "                            ('integrated_save', False),\n",
      "                            ('async_save', False)]),\n",
      "               OrderedDict([('type', 'ObsMonitor')])],\n",
      " 'context': {'device_id': 0,\n",
      "             'device_target': 'Ascend',\n",
      "             'enable_graph_kernel': False,\n",
      "             'graph_kernel_flags': '--disable_expand_ops=Softmax,Dropout '\n",
      "                                   '--enable_parallel_fusion=true '\n",
      "                                   '--reduce_fuse_depth=8 '\n",
      "                                   '--enable_auto_tensor_inplace=true',\n",
      "             'max_call_depth': 10000,\n",
      "             'save_graphs': False,\n",
      "             'save_graphs_path': './graph'},\n",
      " 'device_num': 1,\n",
      " 'do_eval': False,\n",
      " 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor')])],\n",
      " 'eval_dataset': {'auto_tune': False,\n",
      "                  'autotune_per_step': 10,\n",
      "                  'batch_size': 4,\n",
      "                  'data_loader': {'dataset_dir': '/home/ma-user/work/wiki2048.mindrecord',\n",
      "                                  'shuffle': False,\n",
      "                                  'type': 'MindDataset'},\n",
      "                  'do_eval': True,\n",
      "                  'drop_remainder': False,\n",
      "                  'filepath_prefix': './autotune',\n",
      "                  'input_columns': ['input_ids'],\n",
      "                  'num_parallel_workers': 8,\n",
      "                  'numa_enable': False,\n",
      "                  'prefetch_size': 1,\n",
      "                  'profile': False,\n",
      "                  'python_multiprocessing': False,\n",
      "                  'repeat': 1,\n",
      "                  'seed': 0},\n",
      " 'eval_dataset_task': {'dataset_config': {'auto_tune': False,\n",
      "                                          'autotune_per_step': 10,\n",
      "                                          'batch_size': 4,\n",
      "                                          'data_loader': {'dataset_dir': '/home/ma-user/work/wiki2048.mindrecord',\n",
      "                                                          'shuffle': False,\n",
      "                                                          'type': 'MindDataset'},\n",
      "                                          'do_eval': True,\n",
      "                                          'drop_remainder': False,\n",
      "                                          'filepath_prefix': './autotune',\n",
      "                                          'input_columns': ['input_ids'],\n",
      "                                          'num_parallel_workers': 8,\n",
      "                                          'numa_enable': False,\n",
      "                                          'prefetch_size': 1,\n",
      "                                          'profile': False,\n",
      "                                          'python_multiprocessing': False,\n",
      "                                          'repeat': 1,\n",
      "                                          'seed': 0},\n",
      "                       'type': 'CausalLanguageModelDataset'},\n",
      " 'filepath_prefix': './autotune',\n",
      " 'init_start_profile': False,\n",
      " 'layer_decay': 0.65,\n",
      " 'layer_scale': False,\n",
      " 'load_checkpoint': None,\n",
      " 'local_rank': 0,\n",
      " 'lr_scale_factor': 256,\n",
      " 'lr_schedule': {'learning_rate': 0.0003,\n",
      "                 'lr_end': 3e-05,\n",
      "                 'total_steps': -1,\n",
      "                 'type': 'CosineWithWarmUpLR',\n",
      "                 'warmup_ratio': 0.03},\n",
      " 'metric': {'type': 'PerplexityMetric'},\n",
      " 'micro_batch_interleave_num': 1,\n",
      " 'model': {'arch': {'type': 'LlamaForCausalLM'},\n",
      "           'model_config': {'batch_size': 1,\n",
      "                            'bos_token_id': 1,\n",
      "                            'checkpoint_name_or_path': 'llama_7b',\n",
      "                            'compute_dtype': 'float16',\n",
      "                            'compute_in_2d': False,\n",
      "                            'do_sample': False,\n",
      "                            'eos_token_id': 2,\n",
      "                            'extend_method': 'None',\n",
      "                            'hidden_size': 4096,\n",
      "                            'ignore_token_id': -100,\n",
      "                            'layernorm_compute_type': 'float32',\n",
      "                            'max_decode_length': 512,\n",
      "                            'max_position_embedding': 2048,\n",
      "                            'multiple_of': 256,\n",
      "                            'num_heads': 32,\n",
      "                            'num_layers': 32,\n",
      "                            'offset': 0,\n",
      "                            'pad_token_id': 0,\n",
      "                            'param_init_type': 'float16',\n",
      "                            'pretrain_seqlen': 2048,\n",
      "                            'repetition_penalty': 1,\n",
      "                            'rms_norm_eps': 1e-06,\n",
      "                            'rotary_dtype': 'float16',\n",
      "                            'seq_length': 2048,\n",
      "                            'softmax_compute_type': 'float16',\n",
      "                            'top_k': 3,\n",
      "                            'top_p': 1,\n",
      "                            'type': 'LlamaConfig',\n",
      "                            'use_flash_attention': False,\n",
      "                            'use_past': False,\n",
      "                            'use_past_shard': False,\n",
      "                            'vocab_size': 32000}},\n",
      " 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0xffff2068dd60>,\n",
      " 'only_save_strategy': False,\n",
      " 'optimizer': {'beta1': 0.9,\n",
      "               'beta2': 0.95,\n",
      "               'eps': 1e-08,\n",
      "               'learning_rate': 0.0003,\n",
      "               'type': 'FP32StateAdamWeightDecay'},\n",
      " 'output_dir': './output',\n",
      " 'parallel': {'enable_alltoall': False,\n",
      "              'enable_parallel_optimizer': True,\n",
      "              'full_batch': True,\n",
      "              'gradients_mean': False,\n",
      "              'parallel_mode': 1,\n",
      "              'parallel_optimizer_config': {'gradient_accumulation_shard': False,\n",
      "                                            'parallel_optimizer_threshold': 64},\n",
      "              'search_mode': 'sharding_propagation',\n",
      "              'strategy_ckpt_save_file': './ckpt_strategy.ckpt'},\n",
      " 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0xfffeaa9c4df0>,\n",
      " 'processor': {'return_tensors': 'ms',\n",
      "               'tokenizer': {'bos_token': '<s>',\n",
      "                             'eos_token': '</s>',\n",
      "                             'pad_token': '<unk>',\n",
      "                             'type': 'LlamaTokenizer',\n",
      "                             'unk_token': '<unk>'},\n",
      "               'type': 'LlamaProcessor'},\n",
      " 'profile': False,\n",
      " 'profile_communication': False,\n",
      " 'profile_memory': True,\n",
      " 'profile_start_step': 1,\n",
      " 'profile_stop_step': 10,\n",
      " 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0xfffeaa92ed90>,\n",
      " 'remote_save_url': 'Please input obs url on AICC platform.',\n",
      " 'resume_training': False,\n",
      " 'run_mode': 'eval',\n",
      " 'runner_config': {'batch_size': 4,\n",
      "                   'epochs': 1,\n",
      "                   'gradient_accumulation_steps': 1,\n",
      "                   'sink_mode': True,\n",
      "                   'sink_size': 2},\n",
      " 'runner_wrapper': {'scale_sense': {'loss_scale_value': 4294967296,\n",
      "                                    'scale_factor': 2,\n",
      "                                    'scale_window': 1000,\n",
      "                                    'type': 'DynamicLossScaleUpdateCell'},\n",
      "                    'type': 'MFTrainOneStepCell',\n",
      "                    'use_clip_grad': True},\n",
      " 'seed': 0,\n",
      " 'src_strategy_path_or_dir': '',\n",
      " 'train_dataset': {'auto_tune': False,\n",
      "                   'autotune_per_step': 10,\n",
      "                   'batch_size': 4,\n",
      "                   'data_loader': {'dataset_dir': '',\n",
      "                                   'shuffle': True,\n",
      "                                   'type': 'MindDataset'},\n",
      "                   'do_eval': False,\n",
      "                   'drop_remainder': True,\n",
      "                   'filepath_prefix': './autotune',\n",
      "                   'input_columns': ['input_ids'],\n",
      "                   'num_parallel_workers': 8,\n",
      "                   'numa_enable': False,\n",
      "                   'prefetch_size': 1,\n",
      "                   'profile': False,\n",
      "                   'python_multiprocessing': False,\n",
      "                   'repeat': 1,\n",
      "                   'seed': 0},\n",
      " 'train_dataset_task': {'dataset_config': {'auto_tune': False,\n",
      "                                           'autotune_per_step': 10,\n",
      "                                           'batch_size': 4,\n",
      "                                           'data_loader': {'dataset_dir': '',\n",
      "                                                           'shuffle': True,\n",
      "                                                           'type': 'MindDataset'},\n",
      "                                           'do_eval': False,\n",
      "                                           'drop_remainder': True,\n",
      "                                           'filepath_prefix': './autotune',\n",
      "                                           'input_columns': ['input_ids'],\n",
      "                                           'num_parallel_workers': 8,\n",
      "                                           'numa_enable': False,\n",
      "                                           'prefetch_size': 1,\n",
      "                                           'profile': False,\n",
      "                                           'python_multiprocessing': False,\n",
      "                                           'repeat': 1,\n",
      "                                           'seed': 0},\n",
      "                        'type': 'CausalLanguageModelDataset'},\n",
      " 'trainer': {'model_name': 'llama_7b', 'type': 'CausalLanguageModelingTrainer'},\n",
      " 'use_parallel': False}\n",
      "[WARNING] ME(15704:281473457270960,MainProcess):2024-01-24-00:00:50.665.495 [mindspore/train/model.py:1106] For Local2ObsMonitor callback, {'step_end', 'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] DEVICE(15704,ffffa56f30b0,python):2024-01-24-00:00:50.795.097 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_memory_adapter.cc:103] Initialize] Reserved memory size for other components(1073741824) is less than recommend size(2145736704), It may lead to Out Of Memory in HCCL or other components, Please double check context key 'variable_memory_max_size'/'max_device_memory'\n",
      "/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \\L\n",
      "  if not dirpath.find(\"AppData\\Local\\Temp\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \\B\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/7.0.RC1/python/site-packages/tbe/tvm/contrib/ccec.py:766: DeprecationWarning: invalid escape sequence \\L\n",
      "  if not dirpath.find(\"AppData\\Local\\Temp\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \\B\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:140: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(__version__) >= LooseVersion(\"1.17.0\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/extract_image_patches_without_cbuf_schedule.py:317: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if _ is not 1:\n"
     ]
    }
   ],
   "source": [
    "!python /home/ma-user/work/mindformers/run_mindformer.py \\\n",
    "--config /home/ma-user/work/configs/llama/run_llama_7b.yaml \\\n",
    "--eval_dataset_dir /home/ma-user/work/wiki2048.mindrecord \\\n",
    "--run_mode eval \\\n",
    "--load_checkpoint llama_7b \\\n",
    "--epochs 1 \\\n",
    "--use_parallel False \\\n",
    "--device_id 0\n",
    "\n",
    "# PerplexityMetric = {'PerplexityMetric': {'loss': 2.1142693907022476, 'PPL': 8.283531529594038}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7d2a3-1b40-47eb-8d6b-ac2b83b8696e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
