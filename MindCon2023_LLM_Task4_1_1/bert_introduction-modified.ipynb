{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f94bef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"./assets/cover.png\" alt=\"bert-cover\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42b02f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"./assets/contents_1_2.png\" alt=\"table-of-contents\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25e880",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/qr_code.jpeg\" alt=\"table-of-contents\" width=\"1300\"></div>\n",
    "\n",
    "https://github.com/mindspore-courses/step_into_chatgpt\n",
    "\n",
    "https://github.com/mindspore-lab/mindnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3db765",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3202年了，为什么还要学BERT\n",
    "\n",
    "- ### Decoder only模型当道： GPT3、Bloom、LLAMA、GLM\n",
    "\n",
    "    - #### Transformer Encoder结构在生成式任务上的缺陷\n",
    "\n",
    "    - #### BERT模型规模小\n",
    "\n",
    "    - #### Pretrain-Fintune范式的落寞\n",
    "\n",
    "- ### 2022年以前，学术界还是在倒腾BERT\n",
    "\n",
    "    - #### Finetune更容易针对单领域任务训练\n",
    "    \n",
    "    - #### BERT是首个大规模并行预训练的模型，也是当前的performance baseline\n",
    "    \n",
    "    - #### 由BERT入手学大模型训练、微调、Prompt最简单"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a2ad5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# NLP中的预训练模型\n",
    "\n",
    "语言模型的演变经历了以下几个阶段：\n",
    "\n",
    "<div align=center><img src=\"./assets/language_models.svg\" alt=\"language-models\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040570eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. `word2vec`/`Glove`将离散的文本数据转换为固定长度的**静态词向量**，后根据下游任务训练不同的**语言模型**；\n",
    "\n",
    "2. `ELMo`预训练模型将文本数据*结合上下文信息*，转换为**动态词向量**，后根据下游任务训练不同的**语言模型**；\n",
    "\n",
    "3. `BERT`同样将文本数据转换为**动态词向量**，能够更好地捕捉*句子级别的信息与语境信息*，后续只需finetune最后的**输出层**即可适配下游任务；\n",
    "\n",
    "4. `GPT`等预训练语言模型主要用于*文本生成类任务*，需要通过**prompt方法**来应用于下游任务，指导模型生成特定的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c77795",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BERT\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-transfer-learning.png\"></div>\n",
    "\n",
    "BERT模型本质上是结合了`ELMo`模型与`GPT`模型的优势。\n",
    "\n",
    "- 相比于ELMo，BERT仅需改动最后的输出层，而非模型架构，便可以在下游任务中达到很好的效果；\n",
    "- 相比于GPT，BERT在处理词元表示时考虑到了双向上下文的信息；\n",
    "\n",
    "BERT通过两种无监督任务（Masked Language Modelling 和 Next Sentence Prediction）进行预训练，其次，在下游任务中对预训练Transformer编码器的所有参数进行微调，额外的输出层将从头开始训练。\n",
    "\n",
    "> Reference: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2780a0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BERT 结构\n",
    "\n",
    "BERT（Bidirectional Encoder Representation from Transformers）是由Transformer的Encoder层堆叠而成，BERT的模型大小有如下两种：\n",
    "\n",
    "- BERT BASE：与Transformer参数量齐平，用于比较模型效果（110M parameters）\n",
    "- BERT LARGE：在BERT BASE基础上扩大参数量，达到了当时各任务最好的结果（340M parameters）\n",
    "\n",
    "| model | blocks | hidden size | attention heads |\n",
    "| :-----:| :----: | :----: | :----: |\n",
    "| Transformer | 6 | 512 | 8 |\n",
    "| BERT BASE | 12 | 768 | 12 |\n",
    "| BERT LARGE | 24 | 1024 | 16 |\n",
    "\n",
    "> Reference: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b9c13",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"./assets/bert-base-bert-large-encoders.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933f74e",
   "metadata": {},
   "source": [
    "接受输入序列后，BERT会输出每个位置对应的向量（长度等于hidden size），在后续下游任务中，我们会选取与任务相关的位置的向量，输入到最终输出层中得到结果。\n",
    "\n",
    "如在诈骗邮件分类任务中，我们会将表示句子级别信息的`[CLS]` token所对应的vector，放入classfier中，得到对spam/not spam分类的预测。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-classifier.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098e4b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 输入\n",
    "\n",
    "- 针对句子对相关任务，将两个句子合并为一个句子对输入到Encoder中，`[CLS]` + 第一个句子 + `[SEP]` + 第二个句子 + `[SEP]`;\n",
    "- 针对单个文本相关任务，`[CLS]` + 句子 + \n",
    "`[SEP]`。\n",
    "\n",
    "在诈骗邮件分类中，输入为单个句子，在拆分为tokens后，在序列首尾分别添加`[CLS]`与`[SEP]`即可。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-input.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63648b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting git+https://openi.pcl.ac.cn/lvyufeng/mindnlp\n",
      "  Cloning https://openi.pcl.ac.cn/lvyufeng/mindnlp to /tmp/pip-req-build-lu1ghwyc\n",
      "  Running command git clone --filter=blob:none --quiet https://openi.pcl.ac.cn/lvyufeng/mindnlp /tmp/pip-req-build-lu1ghwyc\n",
      "  warning: filtering not recognized by server, ignoring\n",
      "  Resolved https://openi.pcl.ac.cn/lvyufeng/mindnlp to commit 6d6698f9c0e36255f16413f25832a59acea806bd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: mindspore in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (2.2.1)\n",
      "Requirement already satisfied: tqdm in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (4.66.1)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (2.31.0)\n",
      "Requirement already satisfied: datasets in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (2.16.1)\n",
      "Requirement already satisfied: tokenizers in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (0.15.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (0.1.99)\n",
      "Requirement already satisfied: regex in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (2023.10.3)\n",
      "Requirement already satisfied: easydict in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (1.11)\n",
      "Requirement already satisfied: safetensors in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (0.4.1)\n",
      "Requirement already satisfied: ml_dtypes in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindnlp==0.2.0) (0.3.2)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (2.1.2)\n",
      "Requirement already satisfied: xxhash in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->mindnlp==0.2.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (0.20.2)\n",
      "Requirement already satisfied: packaging in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->mindnlp==0.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->mindnlp==0.2.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->mindnlp==0.2.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from requests->mindnlp==0.2.0) (2023.7.22)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (3.20.3)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (2.4.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (9.0.1)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (1.11.3)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (5.9.5)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from asttokens>=2.0.4->mindspore->mindnlp==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore->mindnlp==0.2.0) (0.41.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets->mindnlp==0.2.0) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.2.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.2.0) (2023.3)\n",
      "\u001b[33mDEPRECATION: moxing-framework 2.1.16.2ae09d45 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of moxing-framework or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 安装 mindnlp\n",
    "!pip install git+https://openi.pcl.ac.cn/lvyufeng/mindnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec97b8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 226k/226k [00:00<00:00, 1.19MB/s]\n",
      "100%|██████████| 455k/455k [00:00<00:00, 1.41MB/s]\n",
      "100%|██████████| 570/570 [00:00<00:00, 353kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2393, 3159, 2089, 6968, 2080, 4651, 4121, 12839, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'help', 'prince', 'may', '##uk', '##o', 'transfer', 'huge', 'inheritance', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import BertTokenizer\n",
    "\n",
    "# 从预训练的'bert-base-uncased'模型加载BertTokenizer，设置待处理的文本序列\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sequence = 'help prince mayuko transfer huge inheritance'\n",
    "\n",
    "# 使用BertTokenizer对文本进行处理，得到模型输入并输出\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs)\n",
    "\n",
    "# 将模型输入中的input_ids转换回对应的tokens，打印转换后的tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(model_inputs['input_ids'])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356f9c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT Embedding\n",
    "\n",
    "输入到BERT模型的信息由三部分内容组成：\n",
    "\n",
    "- 表示内容的token ids\n",
    "- 表示位置的position ids\n",
    "- 用于区分不同句子的token type ids\n",
    "\n",
    "三种信息分别进入Embedding层，得到token embeddings、position embeddings与segment embeddings；与Transformer不同，以上三种均为**可学习**的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfad58",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/bert-embedding-modified.png\" alt=\"bert-embedding\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbfd31d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.common.initializer import initializer, TruncatedNormal\n",
    "\n",
    "class BertEmbeddings(nn.Cell):\n",
    "    \"\"\"\n",
    "    BERT的嵌入层，包括词、位置和标记类型的嵌入\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(1 - config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        # 获取输入序列长度\n",
    "        seq_len = input_ids.shape[1]\n",
    "\n",
    "        # 如果未提供位置信息，则默认使用序列长度生成位置信息\n",
    "        if position_ids is None:\n",
    "            position_ids = mindspore.numpy.arange(seq_len)\n",
    "            position_ids = position_ids.expand_dims(0).expand_as(input_ids)\n",
    "\n",
    "        # 如果未提供标记类型信息，则默认使用全零向量\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = mindspore.ops.zeros_like(input_ids)\n",
    "\n",
    "        # 分别获取单词、位置和标记类型的嵌入\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # 将三种嵌入相加得到最终嵌入，并应用LayerNormalization和dropout\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # 返回最终嵌入结果\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d04a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 模型构建\n",
    "\n",
    "BERT模型的构建与上一节课程的Transformer Encoder构建类似。\n",
    "\n",
    "分别构建multi-head attention层，feed-forward network，并在中间用add&norm连接，最后通过线性层与softmax层进行输出。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-model-architecture.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613850b",
   "metadata": {},
   "source": [
    "### BERT self-attention 层\n",
    "\n",
    "<div align=center><img src=\"./assets/transformer_multi-headed_self-attention-recap.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cdbb3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Cell):\n",
    "    \"\"\"\n",
    "    BERT的自注意力层。\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 检查隐藏层大小是否能够被注意力头数整除\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"隐藏层大小 {config.hidden_size} 不能被注意力头数 {config.num_attention_heads} 整除\"\n",
    "            )\n",
    "\n",
    "        # 是否输出注意力分数\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        # 设置注意力头数、每个头的大小以及总的头大小\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Query、Key、Value的全连接层\n",
    "        self.query = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.key = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.value = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        # 丢弃层和Softmax层\n",
    "        self.dropout = nn.Dropout(1 - config.attention_probs_dropout_prob)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "        # 矩阵乘法操作\n",
    "        self.matmul = Matmul()\n",
    "\n",
    "    def transpose_for_scores(self, input_x):\n",
    "        \"\"\"\n",
    "        将形状从 [batch_size, seq_len, num_heads, head_size] 转换为 [batch_size, num_heads, seq_len, head_size]\n",
    "        \"\"\"\n",
    "        new_x_shape = input_x.shape[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        input_x = input_x.view(*new_x_shape)\n",
    "        return input_x.transpose(0, 2, 1, 3)\n",
    "\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        # 分别通过全连接层获取Query、Key、Value\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # 将获取的Query、Key、Value进行形状变换\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attention_scores = self.matmul(query_layer, key_layer.swapaxes(-1, -2))\n",
    "        attention_scores = attention_scores / ops.sqrt(Tensor(self.attention_head_size, mstype.float32))\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # 计算注意力概率\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        # 计算上下文层\n",
    "        context_layer = self.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(0, 2, 1, 3)\n",
    "\n",
    "        # 将上下文层的形状还原为 [batch_size, seq_len, hidden_size]\n",
    "        new_context_layer_shape = context_layer.shape[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        # 如果需要输出注意力分数，则返回注意力分数\n",
    "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
    "\n",
    "        # 返回最终输出\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a410d57",
   "metadata": {},
   "source": [
    "### BERT self-attention 输出层 \n",
    "\n",
    "- BERTSelfOutput：residual connection + layer normalization\n",
    "- BERTAttention: self-attention + add&norm\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-attention-code.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2d4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Self Output\n",
    "    自注意力输出 + 残差连接 + 层归一化\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全连接层\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        # LayerNormalization层\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=1e-12)\n",
    "\n",
    "        # 丢弃层\n",
    "        self.dropout = Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, hidden_states, input_tensor):\n",
    "        # 通过全连接层获取输出\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "\n",
    "        # 应用丢弃层\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # 残差连接并应用层归一化\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "\n",
    "        # 返回最终输出\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f090d1",
   "metadata": {},
   "source": [
    "### BERT feed-forward 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4e8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Cell):\n",
    "    r\"\"\"\n",
    "    BERT中间层\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全连接层\n",
    "        self.dense = nn.Dense(config.hidden_size, config.intermediate_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        # 中间层激活函数\n",
    "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def construct(self, hidden_states):\n",
    "        # 通过全连接层获取输出\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "\n",
    "        # 应用中间层激活函数\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        # 返回中间层输出\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc326d8",
   "metadata": {},
   "source": [
    "### BERT 最后的Add&Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3bddd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Cell):\n",
    "    r\"\"\"\n",
    "    BERT输出层\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全连接层\n",
    "        self.dense = nn.Dense(config.intermediate_size, config.hidden_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        # LayerNormalization层\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=1e-12)\n",
    "\n",
    "        # 丢弃层\n",
    "        self.dropout = Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, hidden_states, input_tensor):\n",
    "        # 通过全连接层获取输出\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "\n",
    "        # 应用丢弃层\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # 残差连接并应用LayerNormalization\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "\n",
    "        # 返回最终输出\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de1ce5",
   "metadata": {},
   "source": [
    "### BERT Encoder\n",
    "\n",
    "- BertLayer：Encoder Layer，集合了self-attention, feed-forward并通过add&norm连接\n",
    "- BertEnocoder：通过Encoder Layer堆叠起来的Encoder结构\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-model-code.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7626fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Cell):\n",
    "    r\"\"\"\n",
    "    BERT层\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 自注意力层\n",
    "        self.attention = BertAttention(config)\n",
    "\n",
    "        # 中间层\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "\n",
    "        # 输出层\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        # 获取自注意力层的输出\n",
    "        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        attention_output = attention_outputs[0]\n",
    "\n",
    "        # 中间层的输出\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "\n",
    "        # 输出层的输出\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "\n",
    "        # 构造输出结果\n",
    "        outputs = (layer_output,) + attention_outputs[1:]\n",
    "\n",
    "        # 返回输出结果\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Cell):\n",
    "    r\"\"\"\n",
    "    BERT编码器\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 是否输出隐藏状态\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "\n",
    "        # 是否输出注意力分数\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        # 多层BERT层\n",
    "        self.layer = nn.CellList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        # 存储所有隐藏状态和注意力分数\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "\n",
    "        # 遍历BERT层\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            # 如果需要输出隐藏状态，则将当前隐藏状态存储\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            # 获取当前BERT层的输出\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            # 如果需要输出注意力分数，则将当前层的注意力分数存储\n",
    "            if self.output_attentions:\n",
    "                all_attentions += (layer_outputs[1],)\n",
    "\n",
    "        # 如果需要输出隐藏状态，则将最后一层的隐藏状态存储\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        # 构造最终的输出结果\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs += (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs += (all_attentions,)\n",
    "\n",
    "        # 返回最终输出结果\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acb651",
   "metadata": {},
   "source": [
    "## BERT 输出\n",
    "\n",
    "BERT会针对每一个位置输出大小为hidden size的向量，在下游任务中，会根据任务内容的不同，选取不同的向量放入输出层。\n",
    "- 我们一般称`[CLS]`经过线性层+激活函数tanh的输出为**pooler output**，用于句子级别的分类/回归任务;\n",
    "- 我们一般称BERT输出的每个位置对应的vector为**sequence output**,用于词语级别的分类任务；\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-output.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015638d2",
   "metadata": {},
   "source": [
    "### BERT Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14666bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Cell):\n",
    "    r\"\"\"\n",
    "    BERT池化层\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全连接层\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, \\\n",
    "            activation='tanh', weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "    def construct(self, hidden_states):\n",
    "        # 获取第一个标记的隐藏状态\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "\n",
    "        # 通过全连接层获取池化输出\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        # 返回池化输出\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0c3e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 预训练\n",
    "\n",
    "BERT通过Masked LM（masked language model）与NSP（next sentence prediction）获取词语和句子级别的特征。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert_pretrain_finetune.jpg\" alt=\"bert-pretrain\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源：Devlin, J.; Chang, M. W.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2d014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Masked Language Model (Masked LM)\n",
    "\n",
    "BERT模型通过Masked LM捕捉**词语层面**的信息。\n",
    "\n",
    "我们随机将每个句子中15%的词语进行遮盖，替换成掩码\\<mask\\>。在训练过程中，模型会对句子进行“完形填空”，预测这些被遮盖的词语是什么，通过减小被mask词语的损失值来对模型进行优化。\n",
    "\n",
    "<div align=center><img src=\"./assets/masked_lm.png\" alt=\"masked-lm\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e05e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "由于\\<mask\\>仅在预训练中出现，为了让预训练和微调中的数据处理尽可能接近，我们在随机mask的时候进行如下操作：\n",
    "- 80%的概率替换为\\<mask\\>\n",
    "- 10%的概率替换为文本中的随机词\n",
    "- 10%的概率不进行替换，保持原有的词元\n",
    "\n",
    "<div align=center><img src=\"./assets/masked_lm_2.png\" alt=\"masked-lm-2\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf54b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "我们通过`BERTPredictionHeadTranform`实现单层感知机，对被遮盖的词元进行预测。在前向网络中，我们需要输入BERT模型的编码结果`hidden_states`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1abd0eed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'gelu': nn.GELU(False),\n",
    "    'gelu_approximate': nn.GELU(),\n",
    "    'swish': nn.SiLU()\n",
    "}\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全连接层\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        # 激活函数\n",
    "        self.transform_act_fn = activation_map.get(config.hidden_act, nn.GELU(False))\n",
    "\n",
    "        # LayerNormalization层\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps)\n",
    "    \n",
    "    def construct(self, hidden_states):\n",
    "        # 通过全连接层获取输出\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "\n",
    "        # 应用激活函数\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "\n",
    "        # 应用LayerNormalization\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "\n",
    "        # 返回最终输出\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137522c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "根据被遮盖的词元位置`masked_lm_positions`，获得这些词元的预测输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c11726",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.ops as ops\n",
    "import mindspore.numpy as mnp\n",
    "from mindspore import Parameter, Tensor\n",
    "\n",
    "class BertLMPredictionHead(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertLMPredictionHead, self).__init__()\n",
    "\n",
    "        # 预测头的转换层\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # 输出层\n",
    "        self.decoder = nn.Dense(config.hidden_size, config.vocab_size, has_bias=False, weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        # 偏置项\n",
    "        self.bias = Parameter(initializer('zeros', config.vocab_size), 'bias')\n",
    "\n",
    "    def construct(self, hidden_states, masked_lm_positions):\n",
    "        # 获取输入张量的形状信息\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # 如果提供了掩码位置，则将预测仅关注这些位置的隐藏状态\n",
    "        if masked_lm_positions is not None:\n",
    "            flat_offsets = mnp.arange(batch_size) * seq_len\n",
    "            flat_position = (masked_lm_positions + flat_offsets.reshape(-1, 1)).reshape(-1)\n",
    "            flat_sequence_tensor = hidden_states.reshape(-1, hidden_size)\n",
    "            hidden_states = ops.gather(flat_sequence_tensor, flat_position, 0)\n",
    "\n",
    "        # 通过转换层获取输出\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "\n",
    "        # 通过全连接层获取预测值，并加上偏置项\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "\n",
    "        # 返回最终输出\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d7abd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT通过NSP捕捉**句子级别**的信息，使其可以理解句子与句子之间的联系，从而能够应用于问答或者推理任务。\n",
    "\n",
    "NSP本质上是一个**二分类任务**，通过输入一个句子对，判断两句话是否为连续句子。输入的两个句子A和B中，B有50%的概率是A的下一句。\n",
    "\n",
    "<div align=center><img src=\"./assets/nsp.png\" alt=\"nsp\" width=\"500\"></div>\n",
    "\n",
    "> <font size=2>图片来源: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert)</font>\n",
    "\n",
    "另外，输入的内容最好是document-level的语料，而非sentence-level的语料，这样训练出的模型可以具备抓取长序列特征的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f8c52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "在这里，我们使用一个单隐藏层的多层感知机`BERTPooler`进行二分类预测。因为特殊占位符\\<cls\\>在预训练中对应了句子级别的特征信息，所以多层感知机分类器只需要输出\\<cls\\>对应的隐藏层输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185aabd9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertPooler(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "\n",
    "        # 全连接层\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, activation='tanh', weight_init=TruncatedNormal(config.initializer_range))\n",
    "    \n",
    "    def construct(self, hidden_states):\n",
    "        # 获取第一个标记的隐藏状态\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "\n",
    "        # 通过全连接层获取池化输出\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        # 返回池化输出\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfdfefe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "最后，多层感知机分类器的输出通过一个线性层`self.seq_relationship`，输出对nsp的预测。\n",
    "\n",
    "在`BERTPreTrainingHeads`中，我们对以上提到的两种方式进行整合。最终输出Maked LM（`prediction scores`）和NSP（`seq_realtionship_score`）的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa9d85a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "\n",
    "        # 语言模型预测头\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "        # 序列关系预测头\n",
    "        self.seq_relationship = nn.Dense(config.hidden_size, 2, weight_init=TruncatedNormal(config.initializer_range))\n",
    "    \n",
    "    def construct(self, sequence_output, pooled_output, masked_lm_positions):\n",
    "        # 获取语言模型预测的分数\n",
    "        prediction_scores = self.predictions(sequence_output, masked_lm_positions)\n",
    "\n",
    "        # 获取序列关系预测的分数\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "\n",
    "        # 返回最终的预测分数\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2536e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### BERT预训练代码整合\n",
    "\n",
    "我们将上述的类进行实例化，并借此回顾一下BERT预训练的整体流程。\n",
    "\n",
    "1. `BertModel`构建BERT模型；\n",
    "2. `BertPretrainingHeads`整合了Masked LM与NSP两个训练任务， 输出预测结果；\n",
    "    - `BertLMPredictionHead`：输入BERT编码与\\<mask\\>的位置，输出对应位置词元的预测；\n",
    "    - `BERTPooler`：输入BERT编码，输出对\\<cls\\>的隐藏状态，并在`BertPretrainingHeads`中通过线性层输出预测结果；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c885d522",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertForPretraining(nn.Cell):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "\n",
    "        # BERT模型\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # 预训练头\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        # 将预训练头的预测层权重绑定到BERT模型的嵌入层上\n",
    "        self.cls.predictions.decoder.weight = self.bert.embeddings.word_embeddings.embedding_table\n",
    "\n",
    "    def construct(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_positions=None):\n",
    "        # 获取BERT模型的输出\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "\n",
    "        # 获取BERT模型的序列输出和池化输出\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "\n",
    "        # 获取预训练头的预测分数和序列关系分数\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output, masked_lm_positions)\n",
    "\n",
    "        # 构造最终输出\n",
    "        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]\n",
    "\n",
    "        # 返回最终输出\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9587a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
